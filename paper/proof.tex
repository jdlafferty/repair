% !TEX root = ./repair.tex



\section{Proofs}
\label{sec:proof}

\subsection{Technical Lemmas}

We present a few technical lemmas that will be used in the proofs. The first lemma is Hoeffding's inequality
\begin{lemma}[\cite{hoeffding1963probability}]\label{lem:hoeffding}
Consider independent random variables $X_1,...,X_n$ that satisfy $X_i\in[a_i,b_i]$ for all $i\in[n]$. Then, for any $t>0$,
$$\mathbb{P}\left(\left|\sum_{i=1}^n(X_i-\mathbb{E}X_i)\right|>t\right) \leq 2\exp\left(-\frac{2t^2}{\sum_{i=1}^n(b_i-a_i)^2}\right).$$
\end{lemma}

Next, we need a central limit theorem with an explicit third moment bound. The following lemma is Theorem 2.20 of \cite{ross2007second}.
\begin{lemma}\label{lem:stein}
If $Z\sim N(0,1)$ and $W=\sum_{i=1}^nX_i$ where $X_i$ are independent mean $0$ and $\Var(W)=1$, then
$$\sup_z\left|\mathbb{P}(W\leq z)-\mathbb{P}(Z\leq z)\right|\leq 2\sqrt{3\sum_{i=1}^n\mathbb{E}|X_i|^3}.$$
\end{lemma}


\begin{lemma}[\cite{cirel1976norms}]\label{lem:talagrand}
Let $f:\mathbb{R}^k\rightarrow\mathbb{R}$ be a Lipschitz function with constant $L>0$. That is, $|f(x)-f(y)|\leq L\|x-y\|$ for all $x,y\in\mathbb{R}^k$. Then, for any $t>0$,
$$\mathbb{P}\left(|f(Z)-\mathbb{E}f(Z)|>t\right)\leq 2\exp\left(-\frac{t^2}{2L^2}\right),$$
where $Z\sim N(0,I_k)$.
\end{lemma}

\begin{lemma}[\cite{laurent2000adaptive}]\label{lem:chi-squared}
For any $t>0$, we have
\begin{eqnarray*}
\mathbb{P}\left(\chi_k^2\geq k+2\sqrt{tk}+2t\right) &\leq& e^{-t}, \\
\mathbb{P}\left(\chi_k^2\leq k-2\sqrt{tk}\right) &\leq& e^{-t}.
\end{eqnarray*}
\end{lemma}

\begin{lemma}\label{lem:inner-prod}
Consider independent $Y_1,Y_2\sim N(0,I_k)$. For any $t>0$, we have
\begin{eqnarray*}
\mathbb{P}\left(|\|Y_1\|\|Y_2\|-k|\geq 2\sqrt{tk}+2t\right) &\leq& 4e^{-t}, \\
\mathbb{P}\left(|Y_1^TY_2| \geq \sqrt{2kt}+2t\right) &\leq& 2e^{-t}.
\end{eqnarray*}
\end{lemma}
\begin{proof}
By Lemma \ref{lem:chi-squared}, we have
\begin{eqnarray*}
&& \mathbb{P}\left(\|Y_1\|\|Y_2\| -k \geq 2\sqrt{tk}+2t\right) \\
&\leq& \mathbb{P}\left(\|Y_1\|^2 \geq k +2\sqrt{tk}+2t\right) + \mathbb{P}\left(\|Y_2\|^2 \geq k +2\sqrt{tk}+2t\right) \\
&\leq& 2e^{-t},
\end{eqnarray*}
and
\begin{eqnarray*}
&& \mathbb{P}\left(\|Y_1\|\|Y_2\| -k \leq -2\sqrt{tk}-2t\right) \\
&\leq& \mathbb{P}\left(\|Y_1\|^2 \leq k -2\sqrt{tk}\right) + \mathbb{P}\left(\|Y_2\|^2 \leq k -2\sqrt{tk}\right) \\
&\leq& 2e^{-t}.
\end{eqnarray*}
Summing up the two bounds above, we obtain the first conclusion. For the second conclusion, note that
$$\mathbb{P}\left(Y_1^TY_2 \geq x\right)\leq e^{-\lambda x}\mathbb{E}e^{\lambda Y_1^TY_2}=\exp\left(-\lambda x-\frac{k}{2}\log(1-\lambda^2)\right)\leq \exp\left(-\lambda x+\frac{k}{2}\lambda^2\right),$$
for any $x>0$ and $\lambda\in (0,1)$. Optimize over $\lambda\in (0,1)$, and we obtain $\mathbb{P}\left(Y_1^TY_2>x\right)\leq e^{-\frac{1}{2}\left(\frac{x^2}{k}\wedge x\right)}$. Take $x=\sqrt{2kt}+2t$, and then we obtain the bound
$$\mathbb{P}\left(Y_1^TY_2 \geq \sqrt{2kt}+2t\right)\leq e^{-t},$$
which immediately implies the second conclusion.
\end{proof}



\subsection{Proof of Theorem \ref{thm:robust-reg}}

In order to prove Theorem \ref{thm:robust-reg}, we establish a more general result. Consider $\eta=b+Au^*+z\in\mathbb{R}^m$, where the noise vector $z$ satisfies (\ref{eq:noise-add-con}), and $b\in\mathbb{R}^m$ is any bias vector. Then, the estimator $\wh{u}=\argmin_{u\in\mathbb{R}^k}\|\eta-Au\|_1$ satisfies the following theoretical guarantee.
\begin{thm}\label{thm:robust-reg-b}
Assume the design matrix $A$ satisfies Condition A and Condition B. Then, as long as $\frac{\overline{\lambda}\sqrt{\frac{k}{m}\log\left(\frac{em}{k}\right)}}{\underline{\lambda}(1-\epsilon)}$ is sufficiently small and $\frac{8\frac{1}{m}\sum_{i=1}^m|b_i|}{\underline{\lambda}(1-\epsilon)}<1$, we have
$$\|\wh{u}-u^*\|\leq \frac{4\frac{1}{m}\sum_{i=1}^m|b_i|}{\underline{\lambda}(1-\epsilon)},$$
with high probability.
\end{thm}
It is easy to see that Theorem \ref{thm:robust-reg} is a special case when $b=0$. To prove Theorem \ref{thm:robust-reg-b}, we need the following empirical process result.

\begin{lemma}\label{lem:EP}
Consider independent random variables $z_1,...,z_m$. Assume $k/m\leq 1$. Then, for any $t\in (0,1/2)$ and any fixed $A^T=(a_1,...,a_m)^T$ such that (\ref{eq:l2-upper-A}) holds, we have
$$\sup_{\|\Delta\|\leq t}\left|\frac{1}{m}\sum_{i=1}^m[(|a_i^T\Delta-z_i|-|z_i|)-\mathbb{E}(|a_i^T\Delta-z_i|-|z_i|)]\right|| \lesssim t\overline{\lambda}\sqrt{\frac{k}{m}\log\left(\frac{em}{k}\right)},$$
with high probability.
\end{lemma}
\begin{proof}
We use the notation $G_m(\Delta)=\frac{1}{m}\sum_{i=1}^m[(|a_i^T\Delta-z_i|-|z_i|)-\mathbb{E}(|a_i^T\Delta-z_i|-|z_i|)]$, and we apply a discretization argument. For the Euclidian ball $B_k(t)=\{\Delta\in\mathbb{R}^k:\|\Delta\|\leq t\}$, there exists a subset $\mathcal{N}_{t,\zeta}\subset B_k(t)$, such that for any $\Delta\in B_k(t)$, there exists a $\Delta'\in\mathcal{N}_{t,\zeta}$ that satisfies $\|\Delta-\Delta'\|\leq \zeta$, and we also have the bound $\log|\mathcal{N}_{t,\zeta}|\leq k\log(1+2t/\zeta)$ according to Lemma 5.2 of \cite{vershynin2010introduction}. For any $\Delta\in B_k(t)$ and the corresponding $\Delta'\in\mathcal{N}_{t,\zeta}$ that satisfies $\|\Delta-\Delta'\|\leq \zeta$, we have
\begin{eqnarray*}
|G_m(\Delta)-G_m(\Delta')| &\leq& 2\frac{1}{m}\sum_{i=1}^m|a_i^T(\Delta-\Delta')| \\
&\leq& 2\sqrt{\frac{1}{m}\sum_{i=1}^m|a_i^T(\Delta-\Delta')|^2} \leq 2\overline{\lambda}\zeta,
\end{eqnarray*}
where the last line is due to the condition (\ref{eq:l2-upper-A}). Thus,
$$\left|G_m(\Delta)\right| \leq \left|G_m(\Delta')\right| + 2\overline{\lambda}\zeta.$$
Taking supremum over both sides of the inequality, we obtain
\begin{equation}
\sup_{\|\Delta\|\leq t}|G_m(\Delta)| \leq \max_{\Delta\in\mathcal{N}_{t,\zeta}}\left|G_m(\Delta)\right| + 2\overline{\lambda}\zeta. \label{eq:disc-not-good}
\end{equation}
For any $\Delta\in B_k(t)$, we have
$$\frac{1}{m}\sum_{i=1}^m\left(|a_i^T\Delta-z_i|-|z_i|\right)^2\leq \frac{1}{m}\sum_{i=1}^m|a_i^T\Delta|^2\leq \overline{\lambda}^2t^2.$$
By Lemma \ref{lem:hoeffding}, we have
$$\mathbb{P}\left(\left|G_m(\Delta)\right| > x\right) \leq 2\exp\left(-\frac{2mx^2}{\overline{\lambda}^2t^2}\right).$$
A union bound argument leads to
\begin{equation}
\mathbb{P}\left(\max_{\Delta\in\mathcal{N}_{t,\zeta}}\left|G_m(\Delta)\right| > x\right) \leq 2\exp\left(-\frac{2mx^2}{\overline{\lambda}^2t^2}+k\log\left(1+\frac{2t}{\zeta}\right)\right).\label{eq:double-ub}
\end{equation}
By choosing $x^2\asymp \frac{t^2\bar{\lambda}^2k\log(1+2t/\zeta)}{m}$, we have
$$\max_{\Delta\in\mathcal{N}_{t,\zeta}}\left|G_m(\Delta)\right| \lesssim t\bar{\lambda}\sqrt{\frac{k\log(1+2t/\zeta)}{m}},$$
with high probability. Together with the bound (\ref{eq:disc-not-good}), we have
$$\sup_{\|\Delta\|\leq t}|G_m(\Delta)| \lesssim t\bar{\lambda}\sqrt{\frac{k\log(1+2t/\zeta)}{m}} + \bar{\lambda}\zeta,$$
with high probability.
The choice $\zeta=t\sqrt{k/m}$ leads to the desired result.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:robust-reg-b}]
Define
\begin{eqnarray*}
L_m(u) &=& \frac{1}{m}\sum_{i=1}^m\left(|\gamma_i-a_i^Tu|-|z_i|\right) \\
&=& \frac{1}{m}\sum_{i=1}^m\left(|a_i^T(u^*-u)+z_i|-|z_i|\right),
\end{eqnarray*}
and $K_m(u)=\frac{1}{m}\sum_{i=1}^m\left(|b_i+a_i^T(u^*-u)+z_i|-|z_i|\right)$. It is easy to see that
\begin{equation}
\sup_u|L_m(u)-K_m(u)|\leq \frac{1}{m}\sum_{i=1}^m|b_i|.\label{eq:K-L-b}
\end{equation}
We introduce i.i.d. Rademacher random variables $\delta_1,...,\delta_m$. With the notation $\wt{a}_i=\delta_ia_i$, $\wt{b}_i=\delta_ib_i$ and $\wt{z}_i=\delta_iz_i$, we can write
\begin{eqnarray*}
K_m(u) &=& \frac{1}{m}\sum_{i=1}^m\left(|\wt{b}_i+\wt{a}_i^T(u^*-u)+\wt{z}_i|-|\wt{z}_i|\right), \\
L_m(u) &=& \frac{1}{m}\sum_{i=1}^m\left(|\wt{a}_i^T(u^*-u)+\wt{z}_i|-|\wt{z}_i|\right).
\end{eqnarray*}
Let $\wt{A}\in\mathbb{R}^{m\times k}$ be the matrix whose $i$th row is $\wt{a}_i$. By the symmetry of $A$, we have $\mathbb{P}(\wt{A}\in U|\delta)=\mathbb{P}(\wt{A}\in U)=\mathbb{P}(A\in U)$ for any measurable set $U$. Therefore, for any measurable sets $U$ and $V$, we have
\begin{eqnarray*}
\mathbb{P}(\wt{A}\in U, \wt{z}\in V) &=& \mathbb{E}\mathbb{P}(\wt{A}\in U, \wt{z}\in V|\delta) \\
&=& \mathbb{E}\mathbb{P}(\wt{A}\in U|\delta)\mathbb{P}(\wt{z}\in V|\delta) \\
&=& \mathbb{E}\mathbb{P}(\wt{A}\in U)\mathbb{P}(\wt{z}\in V|\delta) \\
&=& \mathbb{P}(\wt{A}\in U)\mathbb{P}(\wt{z}\in V),
\end{eqnarray*}
and thus $\wt{A}$ ad $\wt{z}$ are independent. Define $L(u)=\mathbb{E}(L_m(u)|\wt{A})$. Suppose $\|\wh{u}-u^*\|\geq t$, we must have
$$\inf_{\|u-u^*\|\geq t}K_m(u) \leq K_m(u^*).$$
By the convexity of $K_m(u)$, we can replace $\|u-u^*\|\geq t$ by $\|u-u^*\| = t$ and the above inequality still holds. By (\ref{eq:K-L-b}), we have $K_m(u^*)\leq \frac{1}{m}\sum_{i=1}^m|b_i|$, and therefore $\inf_{\|u-u^*\|= t}K_m(u)\leq \frac{1}{m}\sum_{i=1}^m|b_i|$. Since
\begin{eqnarray*}
\inf_{\|u-u^*\|= t}K_m(u) &\geq& \inf_{\|u-u^*\|= t}L_m(u) - \frac{1}{m}\sum_{i=1}^m|b_i| \\
&\geq& \inf_{\|u-u^*\|=t}L(u) + \inf_{\|u-u^*\|= t}(L_m(u)-L(u)) - \frac{1}{m}\sum_{i=1}^m|b_i|,
\end{eqnarray*}
we then have
\begin{equation}
\inf_{\|u-u^*\|=t}L(u) \leq \sup_{\|u-u^*\|= t}|L_m(u)-L(u)| + 2\frac{1}{m}\sum_{i=1}^m|b_i| . \label{eq:basic-L}
\end{equation}
Now we study $L(u)$. Introduce the function $f_i(x)=\mathbb{E}(|x+\wt{z}_i|-|\wt{z}_i|)$ so that we can write $L(u)=\frac{1}{m}\sum_{i=1}^mf_i(\wt{a}_i^T(u^*-u))$. For any $x\geq 0$,
\begin{eqnarray*}
f_i(x) &=& \mathbb{E}(|x+\wt{z}_i|-|\wt{z}_i|)\mathbb{I}\{\wt{z}_i<-x\} + \mathbb{E}(|x+\wt{z}_i|-|\wt{z}_i|)\mathbb{I}\{\wt{z}_i> 0\} \\
&& + \mathbb{E}(|x+\wt{z}_i|-|\wt{z}_i|)\mathbb{I}\{-x\leq \wt{z}_i< 0\} + x\mathbb{P}(\wt{z}_i=0) \\
&=& -x\mathbb{P}(\wt{z}_i<-x) + x\mathbb{P}(\wt{z}_i> 0) + \mathbb{E}(x+2\wt{z}_i)\mathbb{I}\{-x\leq \wt{z}_i<0\}   + x\mathbb{P}(\wt{z}_i=0) \\
&\geq& -x\mathbb{P}(\wt{z}_i<-x) + x\mathbb{P}(\wt{z}_i> 0) - x\mathbb{P}(-x\leq \wt{z}_i< 0)   + x\mathbb{P}(\wt{z}_i=0) \\
&=& -x\mathbb{P}(\wt{z}_i<-x) + x\mathbb{P}(\wt{z}_i< 0) - x\mathbb{P}(-x\leq \wt{z}_i< 0)  + x\mathbb{P}(\wt{z}_i=0) \\
&\geq& x\mathbb{P}(\wt{z}_i=0) \\
&\geq& (1-\epsilon)x.
\end{eqnarray*}
By the symmetry of $\wt{z}_i$, we also have
$$f_i(-x)=\mathbb{E}(|-x+\wt{z}_i|-|\wt{z}_i|)=\mathbb{E}(|x-\wt{z}_i|-|\wt{z}_i|)=\mathbb{E}(|x+\wt{z}_i|-|\wt{z}_i|)=f_i(x),$$
which implies $f_i(x)\geq (1-\epsilon)|x|$. Therefore, for any $u$ such that $\|u-u^*\|=t$, we have
\begin{eqnarray*}
L(u) &=& \frac{1}{m}\sum_{i=1}^mf_i(\wt{a}_i^T(u^*-u)) \\
&\geq& (1-\epsilon)\frac{1}{m}\sum_{i=1}^m|\wt{a}_i^T(u^*-u)| \\
&=& (1-\epsilon)\frac{1}{m}\sum_{i=1}^m|a_i^T(u^*-u)| \\
&\geq& \underline{\lambda}(1-\epsilon)t,
\end{eqnarray*}
where the last inequality is by (\ref{eq:l1-upper-A}). Together with (\ref{eq:basic-L}), we have
$$\underline{\lambda}(1-\epsilon)t \leq \sup_{\|u-u^*\|= t}|L_m(u)-L(u)| + 2\frac{1}{m}\sum_{i=1}^m|b_i|.$$
Set $t=\frac{4\frac{1}{m}\sum_{i=1}^m|b_i|}{\underline{\lambda}(1-\epsilon)}$, and we then have
\begin{equation}
\mathbb{P}\left(\|\wh{u}-u\|\geq t\right) \leq \mathbb{P}\left(\sup_{\|u-u^*\|= t}|L_m(u)-L(u)| \geq \underline{\lambda}(1-\epsilon)t/2\right). \label{eq:larger-t-prob}
\end{equation}
Since the condition (\ref{eq:l2-upper-A}) continues to hold with $A$ replaced by $\wt{A}$, we can apply Lemma \ref{lem:EP} and obtain that
$$\sup_{\|u-u^*\|= t}|L_m(u)-L(u)| \lesssim t\overline{\lambda}\sqrt{\frac{k}{m}\log\left(\frac{em}{k}\right)},$$
with high probability. Under the conditions of the theorem, we know that $\frac{t\overline{\lambda}\sqrt{\frac{k}{m}\log\left(\frac{em}{k}\right)}}{\underline{\lambda}(1-\epsilon)t}$ is sufficiently small, and thus by (\ref{eq:larger-t-prob}), $\|\wh{u}-u^*\|<t$ with high probability.
\end{proof}


\subsection{Proofs of Lemma \ref{lem:design-linear}, Corollary \ref{cor:repair-linear}, Lemma \ref{lem:design-rf} and Corollary \ref{cor:repair-rf}}


\begin{proof}[Proof of Lemma \ref{lem:design-linear}]
Condition A is obvious. For Condition B, we have
$$
\inf_{\|\Delta\|=1}\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta| \geq \sqrt{\frac{2}{\pi}} - \sup_{\|\Delta\|=1}\left|\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta| - \sqrt{\frac{2}{\pi}} \right|,
$$
and we will analyze the second term on the right hand side of the inequality above via a discretization argument. There exists a subset $\mathcal{N}_{\zeta}\subset S^{n-1}$, such that for any $\Delta\in S^{n-1}$, there exists a $\Delta'\in\mathcal{N}_{\zeta}$ that satisfies $\|\Delta-\Delta'\|\leq\zeta$, and we also have the bound $\log|\mathcal{N}_{\zeta}|\leq n\log\left(1+2/\zeta\right)$ according to Lemma 5.2 of \cite{vershynin2010introduction}. For any $\Delta\in S^{n-1}$ and the corresponding $\Delta'\in\mathcal{N}_{\zeta}$ that satisfies $\|\Delta-\Delta'\|\leq \zeta$, we have
\begin{eqnarray*}
\left|\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta| - \sqrt{\frac{2}{\pi}} \right| &\leq& \left|\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta'| - \sqrt{\frac{2}{\pi}} \right|  + \zeta\sup_{\|\Delta\|=1}\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta| \\
&\leq& \left|\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta'| - \sqrt{\frac{2}{\pi}} \right| + \zeta\sup_{\|\Delta\|=1}\left|\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta| - \sqrt{\frac{2}{\pi}} \right| + \zeta\sqrt{\frac{2}{\pi}}.
\end{eqnarray*}
Taking supremum on both sides of the inequality, with some arrangements, we obtain
$$\sup_{\|\Delta\|=1}\left|\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta| - \sqrt{\frac{2}{\pi}} \right|\leq (1-\zeta)^{-1}\max_{\Delta\in\mathcal{N}_{\zeta}}\left|\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta| - \sqrt{\frac{2}{\pi}} \right| + \frac{\zeta}{1-\zeta}\sqrt{\frac{2}{\pi}}.$$
Set $\zeta=1/3$, and we then have
$$\inf_{\|\Delta\|=1}\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta| \geq (2\pi)^{-1} - \frac{3}{2}\max_{\Delta\in\mathcal{N}_{1/3}}\left|\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta| - \sqrt{\frac{2}{\pi}} \right|.$$
Lemma \ref{lem:talagrand} together with a union bound argument leads to
$$\mathbb{P}\left(\max_{\Delta\in\mathcal{N}_{1/3}}\left|\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta| - \sqrt{\frac{2}{\pi}} \right|>t\right)\leq 2\exp\left(n\log(7)-\frac{pt^2}{2}\right),$$
which implies $\max_{\Delta\in\mathcal{N}_{1/3}}\left|\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta| - \sqrt{\frac{2}{\pi}} \right|\lesssim \sqrt{\frac{n}{p}}$ with high probability. Since $n/p$ is sufficiently small, we have $\inf_{\|\Delta\|=1}\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta|\gtrsim 1$ with high probability as desired. The high probability bound $\sup_{\|\Delta\|=1}\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta|^2=\opnorm{A}^2/p\lesssim 1+n/p$ is by \cite{davidson2001local}, and the proof is complete.
\end{proof}

\begin{proof}[Proof of Corollary \ref{cor:repair-linear}]
Since $\wh{\theta}$ belongs to the row space of $X$, there exists some $u^*\in\mathbb{R}^n$ such that $\wh{\theta}=X^Tu^*$.
By Theorem \ref{thm:robust-reg} and Lemma \ref{lem:design-linear}, we know that $\wt{u}=u^*$ with high probability, and therefore $\wt{\theta}=X^T\wt{u}=X^Tu^*=\wh{\theta}$.
\end{proof}

Now we state the proof of Lemma \ref{lem:design-rf}. Note that Condition A is obvious, and we only need to prove Condition B. We present the proofs of (\ref{eq:l1-upper-A}) and (\ref{eq:l2-upper-A}) separately.
\begin{proof}[Proof of (\ref{eq:l1-upper-A}) of Lemma \ref{lem:design-rf}]
Let us adopt the notation that
$$f(W,X,\Delta)=\frac{1}{p}\sum_{j=1}^p\left|\sum_{i=1}^n\psi(W_j^Tx_i)\Delta_i\right|.$$
Define $g(X,\Delta)=\mathbb{E}(f(W,X,\Delta)|X)$.
We then have
\begin{eqnarray}
\nonumber \inf_{\|\Delta\|=1}f(W,X,\Delta) &\geq& \inf_{\|\Delta\|=1}\mathbb{E}f(W,X,\Delta) - \sup_{\|\Delta\|=1}\left|f(W,X,\Delta)-\mathbb{E}f(W,X,\Delta)\right| \\
\label{eq:exp-f-inf} &\geq& \inf_{\|\Delta\|=1}\mathbb{E}f(W,X,\Delta) \\
\label{eq:ep-f} && - \sup_{\|\Delta\|=1}\left|f(W,X,\Delta)-\mathbb{E}(f(W,X,\Delta)|X)\right| \\
\label{eq:ep-g} && - \sup_{\|\Delta\|=1}\left|g(X,\Delta)-\mathbb{\mathbb{E}}g(X,\Delta)\right|.
\end{eqnarray}
We will analyze the three terms above separately.

\paragraph{Analysis of (\ref{eq:exp-f-inf}).} For any $\Delta$ such that $\|\Delta\|=1$, we have
\begin{eqnarray}
\nonumber \mathbb{E}f(W,X,\Delta) &=& \mathbb{E}\left|\sum_{i=1}^n\psi(W^Tx_i)\Delta_i\right| \\
\nonumber &\geq& \mathbb{E}\left(\left|\sum_{i=1}^n\psi(W^Tx_i)\Delta_i\right|\mathbb{I}\left\{\left|\sum_{i=1}^n\psi(W^Tx_i)\Delta_i\right|\geq 1, 1/2\leq \|W\|^2\leq 2\right\}\right) \\
\nonumber &\geq& \mathbb{P}\left(\left|\sum_{i=1}^n\psi(W^Tx_i)\Delta_i\right|\geq 1, 1/2\leq \|W\|^2\leq 2\right) \\
\nonumber &=& \mathbb{P}\left(\left|\sum_{i=1}^n\psi(W^Tx_i)\Delta_i\right|\geq 1\Big|1/2\leq \|W\|^2\leq 2\right)\mathbb{P}\left(1/2\leq \|W\|^2\leq 2\right) \\
\nonumber &\geq& \mathbb{P}\left(\left|\sum_{i=1}^n\psi(W^Tx_i)\Delta_i\right|\geq 1\Big|1/2\leq \|W\|^2\leq 2\right)\left(1-2\exp(-d/16)\right),
\end{eqnarray}
where the last inequality is by Lemma \ref{lem:chi-squared}. It is easy to see that $\Var\left(\psi(W^Tx)|W\right)\leq \mathbb{E}(|\psi(W^Tx)|^2|W)\leq 1$. Moreover, for any $W$ such that $1/2\leq \|W\|^2\leq 2$, 
$$\Var\left(\psi(W^Tx)|W\right)\leq \mathbb{E}(|\psi(W^Tx)|^2|W) \geq \frac{1}{5}\mathbb{P}\left(|W^Tx|>1/2|W\right)\geq \frac{1}{5}\mathbb{P}(|N(0,1)|\geq 1/\sqrt{2}),$$
which is at least $1/20$. In summary, we have
$$1/20 \leq \Var\left(\psi(W^Tx)|W\right) \leq 1,$$
for any $W$ such that $1/2\leq \|W\|^2\leq 2$.
By Lemma \ref{lem:stein}, we have
\begin{eqnarray}
\nonumber && \mathbb{P}\left(\left|\sum_{i=1}^n\psi(W^Tx_i)\Delta_i\right|\geq 1\Big|1/2\leq \|W\|^2\leq 2\right) \\
\nonumber &\geq& \mathbb{P}\left(\frac{\left|\sum_{i=1}^n\psi(W^Tx_i)\Delta_i\right|}{\sqrt{\Var\left(\psi(W^Tx)|W\right)}}\geq \sqrt{20}\Bigg|1/2\leq \|W\|^2\leq 2\right) \\
\nonumber &\geq& \mathbb{P}\left(N(0,1)>\sqrt{20}\right) - \sup_{1/2\leq \|W\|^2\leq 2} 2\sqrt{3\sum_{i=1}^n|\Delta_i|^3\frac{\mathbb{E}\left(|\psi(W^Tx_i)|^3|W\right)}{\left(\Var\left(\psi(W^Tx)|W\right)\right)^{3/2}}} \\
\nonumber &\geq& \mathbb{P}\left(N(0,1)>\sqrt{20}\right) - 35\sqrt{\sum_{i=1}^n|\Delta_i|^3} \\
\nonumber &\geq& \mathbb{P}\left(N(0,1)>\sqrt{20}\right) - 35\max_{1\leq i\leq n}|\Delta_i|^{3/2}.
\end{eqnarray}
Hence, when $\max_{1\leq i\leq n}|\Delta_i|^{3/2}\leq \delta_0^{3/2}:=\mathbb{P}\left(N(0,1)>\sqrt{20}\right)/70$, we can lower bound $\mathbb{E}f(W,X,\Delta)$ by an absolute constant, and we conclude that
\begin{equation}
\inf_{\|\Delta\|=1, \max_{1\leq i\leq n}|\Delta_i|\leq\delta_0}\mathbb{E}f(W,X,\Delta) \gtrsim 1.\label{eq:l1-1-2}
\end{equation}

We also need to consider the case when $\max_{1\leq i\leq n}|\Delta_i|> \delta_0$. Without loss of generality, we can assume $\Delta_1>\delta_0$.
We then lower bound $\mathbb{E}f(W,X,\Delta)$ by
\begin{eqnarray*}
&& \mathbb{E}\left(\left|\sum_{i=1}^n\psi(W^Tx_i)\Delta_i\right|\mathbb{I}\left\{\sum_{i=1}^n\psi(W^Tx_i)\Delta_i \geq \delta_0/2, 1/2\leq \|W\|^2\leq 2\right\}\right) \\
&\geq& \frac{\delta_0}{2} \mathbb{P}\left(\sum_{i=1}^n\psi(W^Tx_i)\Delta_i \geq \delta_0/2\Big| 1/2\leq \|W\|^2\leq 2\right)\mathbb{P}\left(1/2\leq \|W\|^2\leq 2\right) \\
&\geq& \frac{\delta_0}{2} \mathbb{P}\left(\psi(W^Tx_1)\Delta_1 \geq \delta_0/2\Big|1/2\leq \|W\|^2\leq 2\right) \\
&& \times \mathbb{P}\left(\sum_{i=2}^n\psi(W^Tx_i)\Delta_i \geq 0\Big|1/2\leq \|W\|^2\leq 2\right)\left(1-2\exp(-d/16)\right) \\
&=& \frac{\delta_0}{4} \mathbb{P}\left(\psi(W^Tx_1)\Delta_1 \geq \delta_0/2\Big|1/2\leq \|W\|^2\leq 2\right)\left(1-2\exp(-d/16)\right).
\end{eqnarray*}
For any $W$ that satisfies $1/2\leq \|W\|^2\leq 2$, we have
\begin{eqnarray*}
\mathbb{P}\left(\psi(W^Tx_1)\Delta_1 \geq \delta_0/2\Big|W\right) &\geq& \mathbb{P}\left(\psi(W^Tx_1)\geq 1/2\Big|W\right) \\
&\geq&  \mathbb{P}\left(W^Tx_1\geq 1\Big|W\right) \\
&\geq& \mathbb{P}\left(N(0,1)\geq \sqrt{2}\right),
\end{eqnarray*}
which is a constant.
Therefore, we have
$$\mathbb{E}f(W,X,\Delta)\geq \frac{\delta_0}{4}\left(1-2\exp(-d/16)\right)\mathbb{P}\left(N(0,1)\geq \sqrt{2}\right)\gtrsim 1,$$
and we can conclude that
\begin{equation}
\inf_{\|\Delta\|=1, \max_{1\leq i\leq n}|\Delta_i|\geq\delta_0}\mathbb{E}f(W,X,\Delta) \gtrsim 1.\label{eq:l1-1-3}
\end{equation}

In the end, we combine the two cases (\ref{eq:l1-1-2}) and (\ref{eq:l1-1-3}),  and we obtain the conclusion that $\inf_{\|\Delta\|=1}\mathbb{E}f(W,X,\Delta)\gtrsim 1$.


\paragraph{Analysis of (\ref{eq:ep-f}).} We shorthand the conditional expectation operator $\mathbb{E}(\cdot|X)$ by $\mathbb{E}^X$. Let $\wt{W}$ be an independent copy of $W$, and we first bound the moment generating function via a standard symmetrization argument. For any $\lambda>0$,
\begin{eqnarray}
\nonumber && \mathbb{E}^X\exp\left(\lambda \sup_{\|\Delta\|=1}\left|f(W,X,\Delta)-\mathbb{E}^Xf(W,X,\Delta)\right|\right) \\
\nonumber &\leq& \mathbb{E}^X\exp\left(\lambda \mathbb{E}^{X,W}\sup_{\|\Delta\|=1}\left|f(W,X,\Delta)-f(\wt{W},X,\Delta)\right|\right) \\
\nonumber &\leq& \mathbb{E}^X\exp\left(\lambda \sup_{\|\Delta\|=1}\left|f(W,X,\Delta)-f(\wt{W},X,\Delta)\right|\right) \\
\nonumber &=& \mathbb{E}^X\exp\left(\lambda\sup_{\|\Delta\|=1}\left|\frac{1}{p}\sum_{j=1}^p\epsilon_j\left(\left|\sum_{i=1}^n\psi(W_j^Tx_i)\Delta_i\right|-\left|\sum_{i=1}^n\psi(\wt{W}_j^Tx_i)\Delta_i\right|\right)\right|\right) \\
\label{eq:mgf-b} &\leq& \mathbb{E}^X\exp\left(2\lambda\sup_{\|\Delta\|=1}\left|\frac{1}{p}\sum_{j=1}^p\epsilon_j\left|\sum_{i=1}^n\psi(W_j^Tx_i)\Delta_i\right|\right|\right),
\end{eqnarray}
where $\epsilon_1,...,\epsilon_p$ are independent Rademacher random variables. Let us adopt the notation that
$$F(\epsilon,W,X,\Delta)=\frac{1}{p}\sum_{j=1}^p\epsilon_j\left|\sum_{i=1}^n\psi(W_j^Tx_i)\Delta_i\right|.$$
We use a discretization argument. For the Euclidean sphere $S^{n-1}=\{\Delta\in\mathbb{R}^n: \|\Delta\|=1\}$, there exists a subset $\mathcal{N}\subset S^{n-1}$, such that for any $\Delta\in S^{n-1}$, there exists a $\Delta'\in\mathcal{N}$ that satisfies $\|\Delta-\Delta'\|\leq 1/2$, and we also have the bound $\log|\mathcal{N}|\leq 2n$. See, for example, Lemma 5.2 of \cite{vershynin2010introduction}. 
For any $\Delta\in S^{n-1}$ and the corresponding $\Delta'\in\mathcal{N}$ that satisfies $\|\Delta-\Delta'\|\leq 1/2$, we have
\begin{eqnarray*}
|F(\epsilon,W,X,\Delta)| &\leq& |F(\epsilon,W,X,\Delta')| + |F(\epsilon,W,X,\Delta-\Delta')| \\
&\leq& |F(\epsilon,W,X,\Delta')| + \frac{1}{2}\sup_{\|\Delta\|=1}|F(\epsilon,W,X,\Delta)|,
\end{eqnarray*}
which, by taking supremum over both sides, implies
$$\sup_{\|\Delta\|=1}|F(\epsilon,W,X,\Delta)|\leq 2\max_{\Delta\in\mathcal{N}}|F(\epsilon,W,X,\Delta)|.$$
Define $\bar{F}(\epsilon,X,\Delta)=\mathbb{E}^{\epsilon,X}F(\epsilon,W,X,\Delta)$, and then
$$\max_{\Delta\in\mathcal{N}}|F(\epsilon,W,X,\Delta)|\leq \max_{\Delta\in\mathcal{N}}|F(\epsilon,W,X,\Delta)-\bar{F}(\epsilon,X,\Delta)|+\max_{\Delta\in\mathcal{N}}|\bar{F}(\epsilon,X,\Delta)|.$$
In view of (\ref{eq:mgf-b}), we obtain the bound
\begin{eqnarray}
\nonumber && \mathbb{E}^X\exp\left(\lambda \sup_{\|\Delta\|=1}\left|f(W,X,\Delta)-\mathbb{E}^Xf(W,X,\Delta)\right|\right) \\
\nonumber &\leq& \mathbb{E}^X\exp\left(4\lambda \max_{\Delta\in\mathcal{N}}|F(\epsilon,W,X,\Delta)-\bar{F}(\epsilon,X,\Delta)|+4\lambda \max_{\Delta\in\mathcal{N}}|\bar{F}(\epsilon,X,\Delta)|\right) \\
\label{eq:tala-mgf1} &\leq& \frac{1}{2}\sum_{\Delta\in\mathcal{N}} \mathbb{E}^{X}\exp\left(4\lambda|F(\epsilon,W,X,\Delta)-\bar{F}(\epsilon,X,\Delta)|\right) \\
\label{eq:tala-mgf2} && + \frac{1}{2}\sum_{\Delta\in\mathcal{N}} \mathbb{E}^{X}\exp\left(4\lambda |\bar{F}(\epsilon,X,\Delta)|\right).
\end{eqnarray}
We will bound the two terms above on the event $E=\left\{\sum_{i=1}^n\|x_i\|^2\leq 3nd\right\}$. For any $W,\wt{W}$, we have
\begin{eqnarray*}
\left|F(\epsilon,W,X,\Delta)-F(\epsilon,\wt{W},X,\Delta)\right| &\leq& \frac{1}{p}\sum_{j=1}^p\sum_{i=1}^n\left|({\psi}(W_j^Tx_i)-{\psi}(\wt{W}_j^Tx_i))\Delta_i\right| \\
&\leq& \frac{1}{p}\sum_{j=1}^p\sum_{i=1}^n|(W_j-\wt{W}_j)^Tx_i||\Delta_i| \\
&\leq& \frac{1}{p}\sum_{j=1}^p\sum_{i=1}^n\|W_j-\wt{W}_j\|\|x_i\||\Delta_i| \\
&\leq& \frac{1}{\sqrt{p}}\sqrt{\sum_{j=1}^p\|W_j-\wt{W}_j\|^2}\sqrt{\sum_{i=1}^n\|x_i\|^2} \\
&\leq& \sqrt{\frac{3n}{p}}\sqrt{\sum_{j=1}^p\|\sqrt{d}W_j-\sqrt{d}\wt{W}_j\|^2},
\end{eqnarray*}
where the last inequality holds under the event $E$. By Lemma \ref{lem:talagrand}, we have for any $X$ such that $E$ holds,
$$\mathbb{P}\left(|F(\epsilon,W,X,\Delta)-\bar{F}(\epsilon,X,\Delta)|>t\big|X\right)\leq 2\exp\left(-\frac{pt^2}{6n}\right),$$
for any $t>0$. The sub-Gaussian tail implies a bound for the moment generating function. By Lemma 5.5 of \cite{vershynin2010introduction}, we have
$$\mathbb{E}^{X}\exp\left(4\lambda|F(\epsilon,W,X,\Delta)-\bar{F}(\epsilon,X,\Delta)|\right) \leq \exp\left(C_1\frac{n}{p}\lambda^2\right),$$
for some constant $C_1>0$. To bound the moment generating function of $\bar{F}(\epsilon,X,\Delta)$, we note that
\begin{eqnarray*}
|\bar{F}(\epsilon,X,\Delta)|\ &\leq& \left|\frac{1}{p}\sum_{j=1}^p\epsilon_j\right|\mathbb{E}^X\left|\sum_{i=1}^n\psi(W^Tx_i)\Delta_i\right| \\
&\leq& \left|\frac{1}{p}\sum_{j=1}^p\epsilon_j\right|\sqrt{\sum_{i=1}^n\mathbb{E}^X|\psi(W^Tx_i)|^2} \\
&\leq&\sqrt{n} \left|\frac{1}{p}\sum_{j=1}^p\epsilon_j\right|,
\end{eqnarray*}
With an application of Hoeffding-type inequality (Lemma 5.9 of \cite{vershynin2010introduction}), we have
$$\mathbb{E}^{X}\exp\left(4\lambda |\bar{F}(\epsilon,X,\Delta)|\right)\leq \mathbb{E}\exp\left(4\lambda\sqrt{n}\left|\frac{1}{p}\sum_{j=1}^p\epsilon_j\right|\right)\leq \exp\left(C_1\frac{n}{p}\lambda^2\right).$$
Note that we can use the same constant $C_1$ by making its value sufficiently large. Plug the two moment generating function bounds into (\ref{eq:tala-mgf1}) and (\ref{eq:tala-mgf2}), and we obtain the bound
$$\mathbb{E}^X\exp\left(\lambda \sup_{\|\Delta\|=1}\left|f(W,X,\Delta)-\mathbb{E}^Xf(W,X,\Delta)\right|\right)\leq \exp\left(C_1\frac{n}{p}\lambda^2+2n\right),$$
for any $X$ such that $E$ holds. To bound (\ref{eq:ep-f}), we apply Chernoff bound, and then
$$\mathbb{P}\left(\sup_{\|\Delta\|=1}\left|f(W,X,\Delta)-\mathbb{E}(f(W,X,\Delta)|X)\right| > t\right) \leq \exp\left(-\lambda t + C_1\frac{n}{p}\lambda^2+2n\right).$$
Optimize over $\lambda$, set $t\asymp \sqrt{\frac{n^2}{p}}$, and we have
$$\sup_{\|\Delta\|=1}\left|f(W,X,\Delta)-\mathbb{E}(f(W,X,\Delta)|X)\right|\lesssim \sqrt{\frac{n^2}{p}},$$
with high probability.

\paragraph{Analysis of (\ref{eq:ep-g}).} We use a discretization argument. There exists a subset $\mathcal{N}_{\zeta}\subset S^{n-1}$, such that for any $\Delta\in S^{n-1}$, there exists a $\Delta'\in\mathcal{N}_{\zeta}$ that satisfies $\|\Delta-\Delta'\|\leq\zeta$, and we also have the bound $\log|\mathcal{N}|\leq n\log\left(1+2/\zeta\right)$ according to Lemma 5.2 of \cite{vershynin2010introduction}. For any $\Delta\in S^{n-1}$ and the corresponding $\Delta'\in\mathcal{N}_{\zeta}$ that satisfies $\|\Delta-\Delta'\|\leq \zeta$, we have
\begin{eqnarray*}
|g(X,\Delta) - \mathbb{E}g(X,\Delta)| &\leq& |g(X,\Delta')-\mathbb{E}g(X,\Delta')| \\
&& + |g(X,\Delta-\Delta')-\mathbb{E}g(X,\Delta-\Delta')|  \\
&& + 2\mathbb{E}g(X,\Delta-\Delta') \\
&\leq& |g(X,\Delta')-\mathbb{E}g(X,\Delta')| \\
&& + \zeta\sup_{\|\Delta\|=1}|g(X,\Delta)-\mathbb{E}g(X,\Delta)|  \\
&& + 2\zeta\sup_{\|\Delta\|=1}\mathbb{E}g(X,\Delta).
\end{eqnarray*}
Take supremum over both sides, arrange the inequality, and we obtain the bound
\begin{eqnarray}
\label{eq:union-tala-g} \sup_{\|\Delta\|=1}|g(X,\Delta) - \mathbb{E}g(X,\Delta)| &\leq& (1-\zeta)^{-1}\max_{\Delta\in\mathcal{N}_{\zeta}}|g(X,\Delta) - \mathbb{E}g(X,\Delta)| \\
\label{eq:small-exp} && 2\zeta(1-\zeta)^{-1}\mathbb{E}g(X,\Delta).
\end{eqnarray}
To bound (\ref{eq:union-tala-g}), we will use Lemma \ref{lem:talagrand} together with a union bound argument. For any $X,\wt{X}$, we have
\begin{eqnarray*}
|g(X,\Delta)-g(\wt{X},\Delta)| &\leq& \mathbb{E}^X\left|\sum_{i=1}^n(\psi(W_j^Tx_i)-\psi(W_j^T\wt{x}_j))\Delta_i\right| \\
&\leq& \mathbb{E}^X\sqrt{\sum_{i=1}^n\left(\psi(W_j^Tx_i)-\psi(W_j^T\wt{x}_j)\right)^2} \\
&\leq& \sqrt{\sum_{i=1}^n\mathbb{E}^X\left(W_j^T(x_i-\wt{x}_i)\right)^2} \\
&=& \frac{1}{\sqrt{d}}\sqrt{\sum_{i=1}^n\|x_i-\wt{x}_i\|^2}.
\end{eqnarray*}
Therefore, by Lemma \ref{lem:talagrand},
$$\mathbb{P}\left(|g(X,\Delta)-g(\wt{X},\Delta)|>t\right) \leq 2\exp\left(-\frac{dt^2}{2}\right),$$
for any $t>0$. A union bound argument leads to
$$\mathbb{P}\left(\max_{\Delta\in\mathcal{N}_{\zeta}}|g(X,\Delta) - \mathbb{E}g(X,\Delta)|>t\right)\leq 2\exp\left(-\frac{dt^2}{2}+n\log\left(1+\frac{2}{\zeta}\right)\right),$$
which implies that
$$\max_{\Delta\in\mathcal{N}_{\zeta}}|g(X,\Delta) - \mathbb{E}g(X,\Delta)|\lesssim \sqrt{\frac{n\log(1+2/\zeta)}{d}},$$
with high probability. For (\ref{eq:small-exp}), we have
$$\mathbb{E}g(X,\Delta)\leq \sqrt{\mathbb{E}\Var\left(\sum_{i=1}^n\psi(W^Tx_i)\Delta_i\Big|W\right)}\leq \sqrt{\mathbb{E}|\psi(W^Tx)|^2}\leq 1.$$
Combining the bounds for (\ref{eq:union-tala-g}) and (\ref{eq:small-exp}), we have
$$\sup_{\|\Delta\|=1}|g(X,\Delta) - \mathbb{E}g(X,\Delta)|\lesssim \sqrt{\frac{n\log(1+2/\zeta)}{d}} + \zeta,$$
with high probability as long as $\zeta\leq 1/2$. We choose $\zeta=\sqrt{n/d}$, and thus the bound is sufficiently small as long as $n/d$ is sufficiently small.

Finally, combine results for (\ref{eq:exp-f-inf}), (\ref{eq:ep-f}) and (\ref{eq:ep-g}), and we obtain the desired conclusion as long as $n^2/p$ and $n/d$ are sufficiently small.
\end{proof}

To prove (\ref{eq:l2-upper-A}) of Lemma \ref{lem:design-rf}, we establish the following stronger result.
\begin{lemma}\label{lem:lim-G}
Consider independent $W_1,...,W_p\sim N(0,d^{-1}I_d)$ and $x_1,...,x_n\sim N(0,I_d)$. We define the matrices $G,\bar{G}\in\mathbb{R}^{n\times n}$ by
\begin{eqnarray*}
G_{il} &=& \frac{1}{p}\sum_{j=1}^p\psi(W^T_jx_i)\psi(W_j^Tx_l), \\
\bar{G}_{il} &=& |\mathbb{E}\psi'(Z)|^2\frac{x_i^Tx_l}{\|x_i\|\|x_l\|} + \left(\mathbb{E}|\psi(Z)|^2-|\mathbb{E}\psi'(Z)|^2\right)\mathbb{I}\{i=l\},
\end{eqnarray*}
where $Z\sim N(0,1)$.
Assume $d/\log n$ is sufficiently large, and then
$$\opnorm{G-\bar{G}}^2\lesssim \frac{n^2}{p} + \frac{\log n}{d} + \frac{n^2}{d^2},$$
with high probability. Therefore, if we assume $n^2/p$ and $n/d$ are sufficiently small, we also have
\begin{equation}
1\lesssim\lambda_{\min}(G)\leq \lambda_{\max}(G)\lesssim 1, \label{eq:spectrum-G-bound}
\end{equation}
with high probability.
\end{lemma}
\begin{proof}
Define $\wt{G}\in\mathbb{R}^{n\times n}$ with entries $\wt{G}_{il}=\mathbb{E}\left(\psi(W^Tx_i)\psi(W^Tx_l)|X\right)$, and we first bound the difference between $G$ and $\wt{G}$. Note that
$$\mathbb{E}(G_{il}-\wt{G}_{il})^2 = \mathbb{E}\Var(G_{il}|X) \leq \frac{1}{p}\mathbb{E}|\psi(W^Tx_i)\psi(W^Tx_l)|^2 \leq p^{-1}.$$
We then have
$$
\mathbb{E}\opnorm{G-\wt{G}}^2 \leq \mathbb{E}\fnorm{G-\wt{G}}^2 \leq \frac{n^2}{p}.
$$
By Markov's inequality,
\begin{equation}
\opnorm{G-\wt{G}}^2 \lesssim \frac{n^2}{p}, \label{eq:G-G-tilde}
\end{equation}
with high probability.

Next, we study the diagonal entries of $\wt{G}$. For any $i\in[n]$, 
$$\wt{G}_{ii}=\mathbb{E}(|\psi(W^Tx_i)|^2|X)=\mathbb{E}_{U\sim N(0,\|x_i\|^2/d)}|\psi(U)|^2.$$
Therefore,
$$\max_{1\leq i\leq n}|\wt{G}_{ii}-\bar{G}_{ii}|\leq \max_{1\leq i\leq n}\TV\left(N(0,\|x_i\|^2/d), N(0,1)\right)\leq\frac{3}{2}\max_{1\leq i\leq n}\left|\frac{\|x_i\|^2}{d}-1\right|.$$
By Lemma \ref{lem:chi-squared} and a union bound argument, we have
$$\mathbb{P}\left(\max_{1\leq i\leq n}|\wt{G}_{ii}-\bar{G}_{ii}|>3\sqrt{\frac{t}{d}}+3\frac{t}{d}\right)\leq 2ne^{-t},$$
for any $t>0$. Choosing $t\asymp\log n$, we obtain the bound
\begin{equation}
\max_{1\leq i\leq n}|\wt{G}_{ii}-\bar{G}_{ii}|\lesssim \sqrt{\frac{\log n}{d}}, \label{eq:G-diag}
\end{equation}
with high probability.

Now we analyze the off-diagonal entries. We use the notation $\bar{x}_i=\frac{\sqrt{d}}{\|x_i\|}x_i$. For any $i\neq l$, we have
\begin{eqnarray}
\label{eq:G-tilde-1} \wt{G}_{il} &=& \mathbb{E}\left(\psi(W^T\bar{x}_i)\psi(W^T\bar{x}_l)|X\right) \\
\label{eq:G-tilde-2} && + \mathbb{E}\left((\psi(W^Tx_i)-\psi(W^T\bar{x}_i))\psi(W^T\bar{x}_l)|X\right) \\
\label{eq:G-tilde-3} && + \mathbb{E}\left(\psi(W^T\bar{x}_i)(\psi(W^Tx_l)-\psi(W^T\bar{x}_l))|X\right) \\
\label{eq:G-tilde-4} && + \mathbb{E}\left((\psi(W^Tx_i)-\psi(W^T\bar{x}_i))(\psi(W^Tx_l)-\psi(W^T\bar{x}_l))|X\right).
\end{eqnarray}
For first term on the right hand side of (\ref{eq:G-tilde-1}), we observe that $\mathbb{E}\left(\psi(W^T\bar{x}_i)\psi(W^T\bar{x}_l)|X\right)$ is a function of $\frac{\bar{x}_i^T\bar{x}_l}{d}$, and thus we can write
$$\mathbb{E}\left(\psi(W^T\bar{x}_i)\psi(W^T\bar{x}_l)|X\right)=f\left(\frac{\bar{x}_i^T\bar{x}_l}{d}\right),$$
where
$$f(\rho) = \begin{cases}
\mathbb{E}\psi(\sqrt{1-\rho}U+\sqrt{\rho}Z)\psi(\sqrt{1-\rho}V+\sqrt{\rho}Z), & \rho \geq 0, \\
\mathbb{E}\psi(\sqrt{1+\rho}U-\sqrt{-\rho}Z)\psi(\sqrt{1+\rho}V+\sqrt{-\rho}Z), & \rho < 0,
\end{cases}$$
with $U,V,Z\stackrel{iid}{\sim} N(0,1)$. By some direct calculations, we have $f(0)=0$, $f'(0)=(\mathbb{E}\psi'(Z))^2$, and $\sup_{|\rho|\leq 0.2}|f''(\rho)|\lesssim 1$. Therefore, as long as $|\bar{x}_i^T\bar{x}_l|/d\leq 1/5$,
$$\left|f\left(\frac{\bar{x}_i^T\bar{x}_l}{d}\right)-(\mathbb{E}\psi'(Z))^2\frac{\bar{x}_i^T\bar{x}_l}{d}\right|\leq C_1\left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|^2,$$
for some constant $C_1>0$. By Lemma \ref{lem:inner-prod}, we know that $\max_{i\neq l}|\bar{x}_i^T\bar{x}_l|/d\lesssim \sqrt{\frac{\log n}{d}}\leq 1/5$ with high probability, which then implies
\begin{equation}
\sum_{i\neq l}\left(\mathbb{E}\left(\psi(W^T\bar{x}_i)\psi(W^T\bar{x}_l)|X\right)-\bar{G}_{il}\right)^2 \leq C_1\sum_{i\neq l}\left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|^4. \label{eq:G-H}
\end{equation}
The term on the right hand side can be bounded by
$$\sum_{i\neq l}\left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|^4\leq \frac{d}{\min_{1\leq i\leq n}\|x_i\|^2}\sum_{i\neq l}\left|\frac{x_i^Tx_l}{d}\right|^4.$$
By Lemma \ref{lem:chi-squared}, $\frac{d}{\min_{1\leq i\leq n}\|x_i\|^2}\lesssim 1$ with high probability. By integrating out the probability tail bound of $|x_i^Tx_l|$ given in Lemma \ref{lem:inner-prod}, we have $\sum_{i\neq l}\mathbb{E}\left|\frac{x_i^Tx_l}{d}\right|^4\lesssim \frac{n^2}{d^2}$, and by Markov's inequality, we have $\sum_{i\neq l}\left|\frac{x_i^Tx_l}{d}\right|^4\lesssim \frac{n^2}{d^2}$ with high probability.

We also need to analyze the contributions of (\ref{eq:G-tilde-2}) and (\ref{eq:G-tilde-3}). We can write (\ref{eq:G-tilde-2}) as
\begin{eqnarray}
\label{eq:second-order-G-1} && \mathbb{E}\left[\psi(W^T\bar{x}_l)\psi'(W^T\bar{x}_i)W^T(x_i-\bar{x}_i)|X\right] \\
\label{eq:second-order-G-2} && + \frac{1}{2}\mathbb{E}\left[\psi(W^T\bar{x}_i)\psi''(t_i)|W^T(x_i-\bar{x}_i)|^2|X\right],
\end{eqnarray}
where $t_i$ is some random variable between $W^Tx_i$ and $W^T\bar{x}_i$. The first term (\ref{eq:second-order-G-1}) can be expressed as
$$\left(\frac{\|x_i\|}{\sqrt{d}}-1\right)\mathbb{E}\left[\psi(W^T\bar{x}_l)\psi'(W^T\bar{x}_i)W^T\bar{x}_i|X\right]=\left(\frac{\|x_i\|}{\sqrt{d}}-1\right)g\left(\frac{\bar{x}_i^T\bar{x}_l}{d}\right),$$
where the function $g$ satisfies $g(0)=0$ and $\sup_{|\rho|\leq 0.2}|g'(\rho)|\lesssim 1$, and thus
$$\left|g\left(\frac{\bar{x}_i^T\bar{x}_l}{d}\right)\right|\lesssim \left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|\lesssim \left|\frac{x_i^Tx_l}{d}\right|,$$
because of the high probability bound $\max_{i\neq l}|\bar{x}_i^T\bar{x}_l|/d\lesssim \sqrt{\frac{\log n}{d}}\leq 1/5$.
Therefore,
\begin{eqnarray}
\nonumber && \sum_{i\neq l}\left(\mathbb{E}\left[\psi(W^T\bar{x}_l)\psi'(W^T\bar{x}_i)W^T(x_i-\bar{x}_i)|X\right]\right)^2 \\
\nonumber &\lesssim& \sum_{i\neq l}\left|\frac{\|x_i\|}{\sqrt{d}}-1\right|^2\left|\frac{{x}_i^T{x}_l}{d}\right|^2 \\
\label{eq:G-H-mixed} &\lesssim& n\sum_{i=1}^n \left|\frac{\|x_i\|}{\sqrt{d}}-1\right|^4 + \sum_{i\neq l}\left|\frac{{x}_i^T{x}_l}{d}\right|^4.
\end{eqnarray}
By integrating out the probability tail bound of Lemma \ref{lem:chi-squared}, we have $\mathbb{E}\left|\frac{\|x_i\|}{\sqrt{d}}-1\right|^4\lesssim d^{-2}$. We also have $\mathbb{E}\left|\frac{{x}_i^T{x}_l}{d}\right|^4\lesssim d^{-2}$. Hence, $\sum_{i\neq l}\left(\mathbb{E}\left[\psi(W^T\bar{x}_l)\psi'(W^T\bar{x}_i)W^T(x_i-\bar{x}_i)|X\right]\right)^2\lesssim \frac{n^2}{d^2}$ with high probability. To bound (\ref{eq:second-order-G-2}), we observe that
$$\frac{1}{2}\mathbb{E}\left[\psi(W^T\bar{x}_i)\psi''(t_i)|W^T(x_i-\bar{x}_i)|^2|X\right]\leq \mathbb{E}(|W^T(x_i-\bar{x}_i)|^2|X)=\left|\frac{\|x_i\|}{\sqrt{d}}-1\right|^2,$$
where the inequality above is by $\sup_x|\psi(x)|\leq 1$ and $\sup_x|\psi''(x)|\leq 2$. Since $\mathbb{E}\left|\frac{\|x_i\|}{\sqrt{d}}-1\right|^4\lesssim d^{-2}$, we then have
$$\sum_{i\neq l}\left(\frac{1}{2}\mathbb{E}\left[\psi(W^T\bar{x}_i)\psi''(t_i)|W^T(x_i-\bar{x}_i)|^2|X\right]\right)^2\lesssim \frac{n^2}{d^2},$$
with high probability. With a similar analysis of (\ref{eq:G-tilde-3}), we conclude that the contributions of (\ref{eq:G-tilde-2}) and (\ref{eq:G-tilde-3}) is at most at the order of $\frac{n^2}{d^2}$ with respect to the squared Frobenius norm.

Finally, we show that the contribution of (\ref{eq:G-tilde-4}) is negligible. Note that
\begin{eqnarray*}
&& \left|\mathbb{E}\left((\psi(W^Tx_i)-\psi(W^T\bar{x}_i))(\psi(W^Tx_l)-\psi(W^T\bar{x}_l))|X\right)\right| \\
&\leq& \left|\frac{\|x_i\|}{\sqrt{d}}-1\right|\left|\frac{\|x_l\|}{\sqrt{d}}-1\right|\mathbb{E}\left(|W^T\bar{x}_i||W^T\bar{x}_l||X\right) \\
&\leq& \left|\frac{\|x_i\|}{\sqrt{d}}-1\right|\left|\frac{\|x_l\|}{\sqrt{d}}-1\right|,
\end{eqnarray*}
where the last inequality is by $\mathbb{E}\left(|W^T\bar{x}_i||W^T\bar{x}_l||X\right)\leq \frac{1}{2}\mathbb{E}(|W^T\bar{x}_i|^2+|W^T\bar{x}_l|^2|X)=1$.
Since
$$\sum_{i\neq l}\mathbb{E}\left(\frac{\|x_i\|}{\sqrt{d}}-1\right)^2\mathbb{E}\left(\frac{\|x_l\|}{\sqrt{d}}-1\right)^2\lesssim \frac{n^2}{d^2},$$
we can conclude that (\ref{eq:G-tilde-4}) is bounded by $O\left(\frac{n^2}{d^2}\right)$ with high probability by Markov's inequality.

Combining the analyses of (\ref{eq:G-tilde-1}), (\ref{eq:G-tilde-2}), (\ref{eq:G-tilde-3}) and (\ref{eq:G-tilde-4}), we conclude that $\sum_{i\neq l}(\wt{G}_{il}-\bar{G}_{il})^2\lesssim \frac{n^2}{d^2}$ with high probability. Together with (\ref{eq:G-G-tilde}) and (\ref{eq:G-diag}), we obtain the desired bound for $\opnorm{G-\bar{G}}$.

To prove the last conclusion (\ref{eq:spectrum-G-bound}), it suffices to show $1\lesssim\lambda_{\min}(\bar{G})\leq \lambda_{\max}(\bar{G})\lesssim 1$. The lower bound of the smallest eigenvalue is because $\lambda_{\min}(\bar{G})\geq \mathbb{E}|\psi(Z)|^2-|\mathbb{E}\psi'(Z)|^2\gtrsim 1$. The largest eigenvalue can be bounded by
\begin{eqnarray*}
\lambda_{\max}(\bar{G}) &\leq& \left(\mathbb{E}|\psi(Z)|^2-|\mathbb{E}\psi'(Z)|^2\right) + |\mathbb{E}\psi'(Z)|^2\max_{\|v\|=1}\sum_{i=1}^n\sum_{l=1}^nv_iv_l\frac{x_i^Tx_l}{\|x_i\|\|x_l\|} \\
&\lesssim& 1 + \max_{\|v\|=1}\sum_{i=1}^n\sum_{l=1}^nv_iv_l\frac{x_i^Tx_l}{d} \\
&=& 1 + \opnorm{X}^2/d \\
&\lesssim& 1 + \frac{n}{d},
\end{eqnarray*}
with high probability, where the last inequality is by \cite{davidson2001local}. The proof is complete.
\end{proof}

\begin{proof}[Proof of Corollary \ref{cor:repair-rf}]
Since $\wh{\theta}$ belongs to the row space of $\wt{X}$, there exists some $u^*\in\mathbb{R}^n$ such that $\wh{\theta}=\wt{X}^Tu^*$.
By Theorem \ref{thm:robust-reg} and Lemma \ref{lem:design-rf}, we know that $\wt{u}=u^*$ with high probability, and therefore $\wt{\theta}=\wt{X}^T\wt{u}=\wt{X}^Tu^*=\wh{\theta}$.
\end{proof}


\subsection{Proof of Theorem \ref{thm:nn-grad}}


To prove Theorem \ref{thm:nn-grad}, we need the following kernel random matrix result.
\begin{lemma}\label{lem:lim-H}
Consider independent $W_1,...,W_p\sim N(0,d^{-1}I_d)$, $x_1,...,x_n\sim N(0,I_d)$, and $\beta_1,...,\beta_p\sim N(0,1)$. We define the matrices $H, \bar{H}\in\mathbb{R}^{n\times n}$ by
\begin{eqnarray*}
H_{il} &=& \frac{x_i^Tx_l}{d}\frac{1}{p}\sum_{j=1}^p\beta_j^2\psi'(W^T_jx_i)\psi'(W_j^Tx_l), \\
\bar{H}_{il} &=& |\mathbb{E}\psi'(Z)|^2\frac{x_i^Tx_l}{\|x_i\|\|x_l\|} + \left(\mathbb{E}|\psi'(Z)|^2-|\mathbb{E}\psi'(Z)|^2\right)\mathbb{I}\{i=l\},
\end{eqnarray*}
where $Z\sim N(0,1)$.
Assume $d/\log n$ is sufficiently large, and then
$$\opnorm{H-\bar{H}}^2 \lesssim \frac{n^2}{pd} + \frac{n}{p} + \frac{\log n}{d} + \frac{n^2}{d^2},$$
with high probability. If we assume that $d/n$ and $p/n$ are sufficiently large, we will also have
\begin{equation}
0.09\leq\lambda_{\min}(H)\leq \lambda_{\max}(H)\lesssim 1, \label{eq:spectrum-H-bound}
\end{equation}
with high probability.
\end{lemma}
\begin{proof}
Define $\wt{H}\in\mathbb{R}^{n\times n}$ with entries $\wt{H}_{il}=\frac{x_i^Tx_l}{d}\mathbb{E}\left(\psi'(W^Tx_i)\psi'(W^Tx_l)\big|X\right)$, and we first bound the difference between $H$ and $\wt{H}$. Note that
$$\mathbb{E}(H_{il}-\wt{H}_{il})^2=\mathbb{E}\Var(H_{il}|X)\leq \frac{1}{p}\mathbb{E}\left(\frac{|x_i^Tx_l|^2}{d^2}\beta^4\right)\leq\begin{cases}
\frac{3}{pd}, & i\neq l, \\
9p^{-1}, & i=l.
\end{cases}$$
We then have
$$\mathbb{E}\opnorm{H-\wt{H}}^2 \leq \mathbb{E}\fnorm{H-\wt{H}}^2 \leq \frac{3n^2}{pd} + \frac{9n}{p}.$$
By Markov's inequality,
\begin{equation}
\opnorm{H-\wt{H}}^2 \lesssim \frac{n^2}{pd} + \frac{n}{p}, \label{eq:H-H-tilde}
\end{equation}
with high probability.

Next, we study the diagonal entries of $\wt{H}$. For any $i\in[n]$,
$$\wt{H}_{ii}=\frac{\|x_i\|^2}{d}\mathbb{E}(|\psi'(W^Tx_i)|^2|X)=\frac{\|x_i\|^2}{d}\mathbb{E}_{U\sim N(0,\|x_i\|^2/d)}|\psi'(U)|^2.$$
Since $\sup_x|\psi'(x)|\leq 1$ and $\sup_x|\psi''(x)|\leq 2$, we have
\begin{eqnarray*}
|\wt{H}_{ii} - \bar{H}_{ii}| &\leq& \left|\frac{\|x_i\|^2}{d}-1\right| + \left|\mathbb{E}_{U\sim N(0,\|x_i\|^2/d)}|\psi'(U)|^2 - \mathbb{E}_{U\sim N(0,1)}|\psi'(U)|^2\right| \\
&\leq& \left|\frac{\|x_i\|^2}{d}-1\right| + 2\TV\left(N(0,\|x_i\|^2/d), N(0,1)\right) \\
&\leq& 4\left|\frac{\|x_i\|^2}{d}-1\right|
\end{eqnarray*}
Similar to (\ref{eq:G-diag}), Lemma \ref{lem:chi-squared} and a union bound argument imply
\begin{equation}
\max_{1\leq i\leq n}|\wt{H}_{ii}-\bar{H}_{ii}|\lesssim \sqrt{\frac{\log n}{d}}+\frac{\log n}{d}, \label{eq:H-diag}
\end{equation}
with high probability.

Now we analyze the off-diagonal entries. Recall the notation $\bar{x}_i=\frac{\sqrt{d}}{\|x_i\|}x_i$. For any $i\neq l$, we have
\begin{eqnarray}
\label{eq:H-tilde-1} \wt{H}_{il} &=& \frac{\bar{x}_i^T\bar{x}_l}{d}\mathbb{E}\left(\psi'(W^T\bar{x}_i)\psi'(W^T\bar{x}_l)\big|X\right) \\
\label{eq:H-tilde-2} && + \frac{x_i^Tx_l}{d}\mathbb{E}\left(\psi'(W^T{x}_i)\psi'(W^T{x}_l)-\psi'(W^T\bar{x}_i)\psi'(W^T\bar{x}_l)\big|X\right) \\
\label{eq:H-tilde-3} && + \left(\frac{\|x_i\|\|x_l\|}{d}-1\right)\frac{\bar{x}_i^T\bar{x}_l}{d}\mathbb{E}\left(\psi'(W^T\bar{x}_i)\psi'(W^T\bar{x}_l)\big|X\right).
\end{eqnarray}
For the first term on the right hand side of (\ref{eq:H-tilde-1}), we observe that $\frac{\bar{x}_i^T\bar{x}_l}{d}\mathbb{E}\left(\psi'(W^T\bar{x}_i)\psi'(W^T\bar{x}_l)\big|X\right)$ is a function of $\frac{\bar{x}_i^T\bar{x}_l}{d}$, and thus we can write
$$\frac{\bar{x}_i^T\bar{x}_l}{d}\mathbb{E}\left(\psi'(W^T\bar{x}_i)\psi'(W^T\bar{x}_l)\big|X\right)=f\left(\frac{\bar{x}_i^T\bar{x}_l}{d}\right),$$
where
$$f(\rho)=\begin{cases}
\rho\mathbb{E}\psi'(\sqrt{1-\rho}U+\sqrt{\rho}Z)\psi'(\sqrt{1-\rho}V+\sqrt{\rho}Z), & \rho \geq 0, \\
\rho\mathbb{E}\psi'(\sqrt{1+\rho}U-\sqrt{-\rho}Z)\psi'(\sqrt{1+\rho}V+\sqrt{-\rho}Z), & \rho < 0,
\end{cases}$$
with $U,V,Z\stackrel{iid}{\sim} N(0,1)$. By some direct calculations, we have $f(0)=0$, $f'(0)=(\mathbb{E}\psi'(Z))^2$, and $\sup_{|\rho|\leq 0.2}|f''(\rho)|\lesssim 1$. Therefore, using the same analysis that leads to (\ref{eq:G-H}), we have
$$\sum_{i\neq l}\left(\frac{\bar{x}_i^T\bar{x}_l}{d}\mathbb{E}\left(\psi'(W^T\bar{x}_i)\psi'(W^T\bar{x}_l)\big|X\right)-\bar{H}_{il}\right)^2 \lesssim \sum_{i\neq l}\left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|^4\lesssim \frac{n^2}{d^2},$$
with high probability.

For (\ref{eq:H-tilde-2}), we note that
\begin{eqnarray*}
&& \mathbb{E}\left(\psi'(W^T{x}_i)\psi'(W^T{x}_l)-\psi'(W^T\bar{x}_i)\psi'(W^T\bar{x}_l)\big|X\right) \\
&\leq& \mathbb{E}\left(|\psi'(W^T{x}_i)-\psi'(W^T\bar{x}_i)|\big|X\right) + \mathbb{E}\left(|\psi'(W^T{x}_l)-\psi'(W^T\bar{x}_l)|\big|X\right) \\
&\leq& 2\mathbb{E}\left(|W^T(x_i-\bar{x}_i)|\big|X\right) + 2\mathbb{E}\left(|W^T(x_l-\bar{x}_l)|\big|X\right) \\
&=& 2\left|\frac{\|x_i\|}{\sqrt{d}}-1\right| + 2\left|\frac{\|x_l\|}{\sqrt{d}}-1\right|,
\end{eqnarray*}
where we have used $\sup_x|\psi'(x)|\leq 1$ and $\sup_x|\psi''(x)|\leq 2$ in the above inequalities. Therefore, the contribution of (\ref{eq:H-tilde-2}) in terms of squared Frobenius norm is bounded by
\begin{eqnarray*}
&& \sum_{i\neq l}\left|\frac{x_i^Tx_l}{d}\right|^2\left(2\left|\frac{\|x_i\|}{\sqrt{d}}-1\right| + 2\left|\frac{\|x_l\|}{\sqrt{d}}-1\right|\right)^2 \\
&\lesssim& \sum_{i\neq l}\left|\frac{x_i^Tx_l}{d}\right|^4 + n\sum_{i=1}^n\left|\frac{\|x_l\|}{\sqrt{d}}-1\right|^4 \\
&\lesssim& \frac{n^2}{d^2},
\end{eqnarray*}
with high probability, and the last inequality above uses the same analysis that bounds (\ref{eq:G-H-mixed}).

Finally, since (\ref{eq:H-tilde-3}) can be bounded by $\left|\frac{\|x_i\|\|x_l\|}{d}-1\right|\left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|$, its contribution in terms of squared Frobenius norm is bounded by
\begin{eqnarray*}
&& \sum_{i\neq l}\left|\frac{\|x_i\|\|x_l\|}{d}-1\right|^2\left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|^2 \\
&\lesssim& \sum_{i\neq l}\left|\frac{\|x_i\|\|x_l\|}{d}-1\right|^4 + \sum_{i\neq l}\left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|^4.
\end{eqnarray*}
We have already shown that $\sum_{i\neq l}\left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|^4\lesssim \frac{n^2}{d^2}$ in the analysis of (\ref{eq:G-H}). For the first term on the right hand side of the above inequality, we use Lemma \ref{lem:inner-prod} and obtain a probability tail bound for $|\|x_i\|\|x_l\|-d|$. By integrating out this tail bound, we have
$$\sum_{i\neq l}\mathbb{E}\left(\frac{\|x_i\|\|x_l\|}{d}-1\right)^4\lesssim \frac{n^2}{d^2},$$
which, by Markov's inequality, implies $\sum_{i\neq l}\left(\frac{\|x_i\|\|x_l\|}{d}-1\right)^4\lesssim \frac{n^2}{d^2}$ with high probability. 

Combining the analyses of (\ref{eq:H-tilde-1}), (\ref{eq:H-tilde-2}), and (\ref{eq:H-tilde-3}), we conclude that $\sum_{i\neq l}(\wt{H}_{il}-\bar{H}_{il})^2\lesssim \frac{n^2}{d^2}$ with high probability. Together with (\ref{eq:H-H-tilde}) and (\ref{eq:H-diag}), we obtain the desired bound for $\opnorm{H-\bar{H}}$.

The last conclusion (\ref{eq:spectrum-H-bound}) follows a similar argument used in the proof of Lemma \ref{lem:lim-G}.
\end{proof}


Now we are ready to prove Theorem \ref{thm:nn-grad}.
\begin{proof}[Proof of Theorem \ref{thm:nn-grad}]
We first establish some high probability events:
\begin{eqnarray}
\label{eq:p1e1} \max_{1\leq j\leq p}|\beta_j(0)| &\leq& 2\sqrt{\log p}, \\
\label{eq:p1e1.5} \max_{k\in\{1,2,3\}}\frac{1}{p}\sum_{j=1}^p|\beta_j(0)|^k &\lesssim& 1, \\
\label{eq:p1e2} \sum_{i=1}^n\|x_i\|^2 &\leq& 2nd, \\
\label{eq:max-norm-x} \max_{1\leq i\leq n}\|x_i\| &\lesssim& \sqrt{d}, \\
\label{eq:p1e2.5}\max_{1\leq l\leq n}\sum_{i=1}^n\left|\frac{x_i^Tx_l}{d}\right| &\lesssim& 1+\frac{n}{\sqrt{d}}, \\
 \label{eq:p1e3}\|u(0)\| &\leq& \sqrt{n\log p}.
\end{eqnarray}
The bound (\ref{eq:p1e1}) is a consequence of a standard Gaussian tail inequality and a union bound argument. The second bound (\ref{eq:p1e1.5}) is by Markov's inequality and the fact that $\frac{1}{p}\sum_{j=1}^p\mathbb{E}|\beta_j(0)|^k\lesssim 1$. Then, we have (\ref{eq:p1e2}) and (\ref{eq:max-norm-x}) derived from Lemma \ref{lem:chi-squared} and a union bound. For (\ref{eq:p1e2.5}), we first have the bound
\begin{eqnarray*}
\max_{1\leq l\leq n}\sum_{i=1}^n\left|\frac{x_i^Tx_l}{d}\right| &\leq& \max_{1\leq l\leq n}\frac{\|x_l\|^2}{d} + \max_{1\leq l\leq n}\frac{\|x_l\|}{d}\sum_{i\in[n]\backslash\{l\}}\left|\frac{x_i^Tx_l}{\|x_l\|}\right| \\
&\lesssim&  1 + \frac{1}{\sqrt{d}}\max_{1\leq l\leq n}\sum_{i\in[n]\backslash\{l\}}\left|\frac{x_i^Tx_l}{\|x_l\|}\right|,
\end{eqnarray*}
where the last inequality is by (\ref{eq:max-norm-x}). Since
$$\sum_{i\in[n]\backslash\{l\}}\left|\frac{x_i^Tx_l}{\|x_l\|}\right|\leq \sqrt{n}\sqrt{\sum_{i\in[n]\backslash\{l\}}\left|\frac{x_i^Tx_l}{\|x_l\|}\right|^2},$$
and $\sum_{i\in[n]\backslash\{l\}}\left|\frac{x_i^Tx_l}{\|x_l\|}\right|^2\sim \chi_{n-1}^2$, we can then use Lemma \ref{lem:chi-squared} and a union bound and obtain
$$\max_{1\leq l\leq n}\sum_{i\in[n]\backslash\{l\}}\left|\frac{x_i^Tx_l}{\|x_l\|}\right|\lesssim n,$$
with high probability, which then leads to (\ref{eq:p1e2.5}). To obtain the last bound (\ref{eq:p1e3}), we note that $\mathbb{E}|u_i(0)|^2 = \mathbb{E}\Var(u_i(0)|X) \leq 1$, which then implies (\ref{eq:p1e3}) by Markov's inequality.

Now we are ready to prove the main result. It suffices to show the following to claims are true.
\begin{thm1}
With high probability, for any integer $k\geq 1$, as long as (\ref{eq:iter-function}) and (\ref{eq:iter-parameter}) holds for all $t\leq k$, then (\ref{eq:iter-parameter}) holds for $t=k+1$.
\end{thm1}
\begin{thm2}
With high probability, for any integer $k\geq 1$, as long as (\ref{eq:iter-function}) holds for all $t\leq k$ and (\ref{eq:iter-parameter}) holds for all $t\leq k+1$, then (\ref{eq:iter-function}) holds for $t=k+1$.
\end{thm2}
\noindent With both the claims above being true, we can then deduce (\ref{eq:iter-function}) and (\ref{eq:iter-parameter}) for all $t\geq 1$ by mathematical induction.

\paragraph{Proof of Claim A.} We bound $\|W_j(k+1)-W_j(0)\|$ by $\sum_{t=0}^k\|W_j(t+1)-W_j(t)\|$. Then by the gradient descent formula, we have
\begin{eqnarray*}
\|W_j(k+1)-W_j(0)\| &\leq& \frac{\gamma}{d\sqrt{p}}\sum_{t=0}^k\left\|\beta_j(t)\sum_{i=1}^n(u_i(t)-y_i)\psi'(W_j(t)^Tx_i)x_i\right\| \\
&\leq& \frac{\gamma}{d\sqrt{p}}\sum_{t=0}^k|\beta_j(t)|\sum_{i=1}^n|y_i-u_i(t)|\|x_i\| \\
&\leq& \frac{\gamma}{d\sqrt{p}}(|\beta_j(0)|+R_2)\sqrt{\sum_{i=1}^n\|x_i\|^2}\sum_{t=0}^k\|y-u(t)\| \\
&\leq& \frac{16}{d\sqrt{p}}(|\beta_j(0)|+R_2)\sqrt{\sum_{i=1}^n\|x_i\|^2}\|y-u(0)\| \\
&\leq& \frac{100n\log p}{\sqrt{pd}} = R_1,
\end{eqnarray*}
where we have used (\ref{eq:p1e1}), (\ref{eq:p1e2}) and (\ref{eq:p1e3}) in the above inequalities.
Similarly, we also have
\begin{eqnarray*}
|\beta_j(k+1)-\beta_j(0)| &\leq& \sum_{t=0}^k|\beta_j(t+1)-\beta_j(t)| \\
&\leq& \frac{\gamma}{\sqrt{p}}\sum_{t=0}^k\left|\sum_{i=1}^n(u_i(t)-y_i)\psi(W_j(t)^Tx_i)\right| \\
&\leq& \frac{\gamma}{\sqrt{p}}\sum_{t=0}^k\sum_{i=1}^n|y_i-u_i(t)| \\
&\leq& \gamma\sqrt{\frac{n}{p}}\sum_{t=0}^k\|y-u(t)\| \\
&\leq& 16\sqrt{\frac{n}{p}}\|y-u(0)\| \\
&\leq& 32\sqrt{\frac{n^2\log p}{p}} = R_2,
\end{eqnarray*}
where we have used (\ref{eq:p1e3}).
Hence, (\ref{eq:iter-function}) holds for $t=k+1$, and Claim A is true.

\paragraph{Proof of Claim B.} We first analyze $u(k+1)-u(k)$. For each $i\in[n]$, we have
\begin{eqnarray*}
&& u_i(k+1) - u_i(k) \\
&=& \frac{1}{\sqrt{p}}\sum_{j=1}^p\beta_j(k+1)\left(\psi(W_j(k+1)^Tx_i)-\psi(W_j(k)^Tx_i)\right) \\
&& + \frac{1}{\sqrt{p}}\sum_{j=1}^p(\beta_j(k+1)-\beta_j(k))\psi(W_j(k)^Tx_i) \\
&=& \frac{1}{\sqrt{p}}\sum_{j=1}^p\beta_j(k+1)(W_j(k+1)-W_j(k))^Tx_i\psi'(W_j(k)^Tx_i) \\
&& + \frac{1}{\sqrt{p}}\sum_{j=1}^p(\beta_j(k+1)-\beta_j(k))\psi(W_j(k)^Tx_i) \\
&& + \frac{1}{2\sqrt{p}}\sum_{j=1}^p\beta_j(k+1)|(W_j(k+1)-W_j(k))^Tx_i|^2\psi''(\xi_{ijk}) \\
&=& \gamma\sum_{l=1}^n(H_{il}(k)+G_{il}(k))(y_l-u_l(k)) + r_i(k),
\end{eqnarray*}
where
\begin{eqnarray*}
G_{il}(k) &=& \frac{1}{p}\sum_{j=1}^p\psi(W_j(k)^Tx_l)\psi(W_j(k)^Tx_i), \\
H_{il}(k) &=& \frac{x_i^Tx_l}{d}\frac{1}{p}\sum_{j=1}^p\beta_j(k)\beta_j(k+1)\psi'(W_j(k)^Tx_i)\psi'(W_j(k)^Tx_l),
\end{eqnarray*}
and
$$r_i(k)=\frac{1}{2\sqrt{p}}\sum_{j=1}^p\beta_j(k+1)|(W_j(k+1)-W_j(k))^Tx_i|^2\psi''(\xi_{ijk}).$$
We need to understand the eigenvalues of $G(k)$ and $H(k)$, and bound the absolute value of $r_i(k)$. By its definition, it is easy to see that $\lambda_{\min}(G(k))\geq 0$. 
To analyze $\lambda_{\max}(G(k))$, we note that
\begin{eqnarray*}
|G_{il}(k) - G_{il}(0)| &\leq& \frac{1}{p}\sum_{j=1}^p|\psi(W_j(k)^Tx_l) - \psi(W_j(0)^Tx_l)| \\
&& + \frac{1}{p}\sum_{j=1}^p|\psi(W_j(k)^Tx_i) - \psi(W_j(0)^Tx_i)| \\
&\leq& \frac{1}{p}\sum_{j=1}^p|(W_j(k)-W_j(0))^Tx_l| + \frac{1}{p}\sum_{j=1}^p|(W_j(k)-W_j(0))^Tx_i| \\
&\leq& R_1\left(\|x_l\| + \|x_i\|\right).
\end{eqnarray*}
Thus, by (\ref{eq:max-norm-x}),
$$\max_{1\leq l\leq n}\sum_{i=1}^n|G_{il}(k) - G_{il}(0)|\leq 2R_1n\max_{1\leq i\leq n}\|x_i\|\lesssim \frac{n^2\log p}{\sqrt{p}}.$$
Then, we have
\begin{equation}
\opnorm{G(k)-G(0)} \leq \max_{1\leq l\leq n}\sum_{i=1}^n|G_{il}(k) - G_{il}(0)|\lesssim \frac{n^2\log p}{\sqrt{p}}, \label{eq:G-last-op}
\end{equation}
which leads to the bound $\lambda_{\max}(G(k))\lesssim 1 + \frac{n^2\log p}{\sqrt{p}}$ for all $k$ by Lemma \ref{lem:lim-G}.
For the matrix $H(k)$, we show its eigenvalues can be controlled by those of $H(0)$ We have
\begin{eqnarray*}
|H_{il}(k)-H_{il}(0)| &\leq& \left|\frac{x_i^Tx_l}{d}\right|\frac{1}{p}\sum_{j=1}^p|\beta_j(k)\beta_j(k+1)-\beta_j^2(0)| \\
&& + \left|\frac{x_i^Tx_l}{d}\right|\frac{1}{p}\sum_{j=1}^p\beta_j^2(0)|\psi'(W_j(k)^Tx_i) - \psi'(W_j(0)^Tx_i)| \\
&& + \left|\frac{x_i^Tx_l}{d}\right|\frac{1}{p}\sum_{j=1}^p\beta_j^2(0)|\psi'(W_j(k)^Tx_l) - \psi'(W_j(0)^Tx_l)| \\
&\leq& \left|\frac{x_i^Tx_l}{d}\right|\frac{1}{p}\sum_{j=1}^pR_2(R_2+2|\beta_j(0)|) \\
&& + 2R_1\left(\|x_l\| + \|x_i\|\right)\left|\frac{x_i^Tx_l}{d}\right|\frac{1}{p}\sum_{j=1}^p\beta_j^2(0).
\end{eqnarray*}
Thus, by (\ref{eq:max-norm-x}) and (\ref{eq:p1e2.5}),
$$\max_{1\leq l\leq n}\sum_{i=1}^n|H_{il}(k) - H_{il}(0)|\lesssim \max_{1\leq l\leq n}\sum_{i=1}^n(R_2+R_1\sqrt{d})\left|\frac{x_i^Tx_l}{d}\right|\lesssim \frac{n\log p}{\sqrt{p}}\left(1+\frac{n}{\sqrt{d}}\right).$$
Then, we have
\begin{equation}
\opnorm{H(k)-H(0)}\leq \max_{1\leq l\leq n}\sum_{i=1}^n|H_{il}(k) - H_{il}(0)|\lesssim \frac{n\log p}{\sqrt{p}}\left(1+\frac{n}{\sqrt{d}}\right). \label{eq:H-time-stable}
\end{equation}
Next, we give a bound for $r_i(k)$. By $\sup_x|\psi''(x)|\leq 2$ and $\sup_x|\psi'(x)|\leq 1$, we have
\begin{eqnarray*}
|r_i(k)| &\leq& \frac{1}{\sqrt{p}}\sum_{j=1}^p|\beta_j(k+1)||(W_j(k+1)-W_j(k))^Tx_i|^2 \\
&\leq& \frac{\|x_i\|^2}{\sqrt{p}}\sum_{j=1}^p|\beta_j(k+1)|\|W_j(k+1)-W_j(k)\|^2 \\
&\leq& \frac{\gamma^2}{pd^2}\frac{\|x_i\|^2}{\sqrt{p}}\sum_{j=1}^p|\beta_j(k+1)||\beta_j(k)|^2\left(\sum_{l=1}^n|y_l-u_l(k)|\|x_l\|\right)^2 \\
&\leq& \frac{\gamma^2}{pd^2}\frac{\|x_i\|^2\sum_{l=1}^n\|x_l\|^2}{\sqrt{p}}\|y-u(k)\|^2\sum_{j=1}^p|\beta_j(k+1)||\beta_j(k)|^2 \\
&\lesssim& \frac{\gamma^2n}{\sqrt{p}}\|y-u(k)\|^2 \\
&\lesssim& \frac{\gamma^2n\sqrt{n\log p}}{\sqrt{p}}\|y-u(k)\|,
\end{eqnarray*}
where we have used (\ref{eq:p1e1.5}), (\ref{eq:p1e2}), (\ref{eq:max-norm-x}) and (\ref{eq:p1e3}) in the above inequalities.
This leads to the bound
\begin{equation}
\|r(k)\|=\sqrt{\sum_{i=1}^n|r_i(k)|^2}\lesssim \frac{\gamma^2n^2\sqrt{\log p}}{\sqrt{p}}\|y-u(k)\|.\label{eq:bound-res-k}
\end{equation}
Given the relation that
$$u(k+1)-u(k)=\gamma(H(k)+G(k))(y-u(k))+r(k),$$
we have
\begin{eqnarray*}
\|y-u(k+1)\|^2 &=& \|y-u(k)\|^2 - 2\iprod{y-u(k)}{u(k+1)-u(k)} + \|u(k)-u(k+1)\|^2 \\
&=& \|y-u(k)\|^2 - 2\gamma(y-u(k))^T(H(k)+G(k))(y-u(k)) \\
&& - 2\iprod{y-u(k)}{r(k)} + \|u(k)-u(k+1)\|^2.
\end{eqnarray*}
By $\lambda_{\min}(G(k))\geq 0$, the operator norm bound (\ref{eq:H-time-stable}), and Lemma \ref{lem:lim-H}, we have
\begin{equation}
- 2\gamma(y-u(k))^T(H(k)+G(k))(y-u(k)) \leq -\frac{\gamma}{6}\|y-u(k)\|^2. \label{eq:main-inner}
\end{equation}
The bound (\ref{eq:bound-res-k}) implies
$$- 2\iprod{y-u(k)}{r(k)}\leq 2\|y-u(k)\|\|r(k)\|\lesssim \frac{\gamma^2n^2\sqrt{\log p}}{\sqrt{p}}\|y-u(k)\|^2,$$
and together with $\lambda_{\max}(H(0))\lesssim 1$, (\ref{eq:H-time-stable}), and the bound $\lambda_{\max}(G(k))\lesssim 1 + \frac{n^2\log p}{\sqrt{p}}$, we have
\begin{eqnarray*}
\|u(k)-u(k+1)\|^2 &\leq& 2\gamma^2\|(H(k)+G(k))(y-u(k))\|^2 + 2\|r(k)\|^2 \\
&\lesssim& \gamma^2\left(1+\frac{n^4(\log p)^2}{p}\right)\|y-u(k)\|^2 + \frac{\gamma^4n^4\log p}{p}\|y-u(k)\|^2 .
\end{eqnarray*}
Therefore, as long as $\gamma\frac{n^4(\log p)^2}{p}$ is sufficiently small, we have
$$- 2\iprod{y-u(k)}{r(k)} + \|u(k)-u(k+1)\|^2 \leq \frac{\gamma}{24}\|y-u(k)\|^2.$$
Together with the bound (\ref{eq:main-inner}), we have
$$\|y-u(k+1)\|^2 \leq \left(1-\frac{\gamma}{8}\right)\|y-u(k)\|^2\leq \left(1-\frac{\gamma}{8}\right)^{k+1}\|y-u(0)\|^2,$$
and thus Claim B is true. The proof is complete.
\end{proof}


\subsection{Proofs of Theorem \ref{thm:repair-nn-1} and Theorem \ref{thm:repair-nn-2}}

\begin{proof}[Proof of Theorem \ref{thm:repair-nn-1}]
We first analyze $\wh{v}_1,...,\wh{v}_p$. The idea is to apply the result of Theorem \ref{thm:robust-reg} to each of the $p$ robust regression problems. Thus, it suffices to check if the conditions of Theorem \ref{thm:robust-reg} hold for the $p$ regression problems simultaneously. Since the $p$ regression problems share the same Gaussian design matrix, Lemma \ref{lem:design-linear} implies that Conditions A and B hold for all the $p$ regression problems. Next, by scrutinizing the proof of Theorem \ref{thm:robust-reg}, the randomness of the conclusion is from the noise vector $Z_j$ through the empirical process bound given by Lemma \ref{lem:EP}. With an additional union bound argument applied to (\ref{eq:double-ub}), Lemma \ref{lem:EP} can be extended to $Z_j$ for all $j\in[p]$ with an additional assumption that $\frac{\log p}{d}$ is sufficiently small. Then, by the same argument in the proof of Corollary \ref{cor:repair-linear}, we have $\wt{W}_j=\wh{W}_j$ for all $j\in[p]$ with high probability.

To analyze $\wh{u}$, we apply Theorem \ref{thm:robust-reg-b}. Note that
\begin{eqnarray*}
\eta_j - \beta_j(0) &=& \beta_j(t_{\max}) - \beta_j(0) +z_j \\
&=& \sum_{t=0}^{t_{\max}-1}\left(\beta_j(t+1)-\beta_j(t)\right) + z_j \\
&=& \frac{\gamma}{\sqrt{p}}\sum_{t=0}^{t_{\max}-1}\sum_{i=1}^n(y_i-u_i(t))\psi(W_j(t)^Tx_i) + z_j \\
&=& \frac{\gamma}{\sqrt{p}}\sum_{t=0}^{t_{\max}-1}\sum_{i=1}^n(y_i-u_i(t))(\psi(W_j(t)^Tx_i)-\psi(W_j(0)^Tx_i)) \\
&& + \frac{\gamma}{\sqrt{p}}\sum_{t=0}^{t_{\max}-1}\sum_{i=1}^n(y_i-u_i(t))\psi(W_j(0)^Tx_i) + z_j.
\end{eqnarray*}
Thus, in the framework of Theorem \ref{thm:robust-reg-b}, we can view $\eta-\beta(0)$ as the response, $\psi(X^TW(0)^T)$ as the design, $z$ as the noise, and $b_j=\frac{\gamma}{\sqrt{p}}\sum_{t=0}^{t_{\max}-1}\sum_{i=1}^n(y_i-u_i(t))(\psi(W_j(t)^Tx_i)-\psi(W_j(0)^Tx_i))$ as the $j$th entry of the bias vector. By Lemma \ref{lem:design-rf}, we know that the design matrix $\psi(X^TW(0)^T)$ satisfies Condition A and Condition B. So it suffices to bound $\frac{1}{p}\sum_{j=1}^p|b_j|$. With the help of Theorem \ref{thm:nn-grad}, we have
\begin{eqnarray*}
\frac{1}{p}\sum_{j=1}^p|b_j| &\leq& \frac{\gamma}{p^{3/2}}\sum_{j=1}^p\sum_{t=0}^{t_{\max}-1}\sum_{i=1}^n|y_i-u_i(t)||(W_j(t)-W_j(0))^Tx_i| \\
&\leq& \frac{R_1\gamma}{p^{1/2}}\sum_{t=0}^{t_{\max}-1}\sum_{i=1}^n|y_i-u_i(t)|\|x_i\| \\
&\leq& \frac{R_1\gamma}{p^{1/2}}\sum_{t=0}^{t_{\max}-1}\|y-u(t)\|\sqrt{\sum_{i=1}^n\|x_i\|^2} \\
&\lesssim& \frac{R_1}{p^{1/2}}\|y-u(0)\|\sqrt{\sum_{i=1}^n\|x_i\|^2} \\
&\lesssim& \frac{n^2\log p}{p},
\end{eqnarray*}
where the last inequality is by $\sum_{i=1}^n\|x_i\|^2\lesssim nd$ due to Lemma \ref{lem:chi-squared}, and $\|u(0)\|^2\lesssim n$ due to Markov's inequality and $\mathbb{E}|u_i(0)|^2 = \mathbb{E}\Var(u_i(0)|X) \leq 1$. By Theorem \ref{thm:robust-reg-b} and Lemma \ref{lem:lim-G}, we have $\frac{1}{p}\|\wt{\beta}-\wh{\beta}\|^2 \lesssim \frac{n^2\log p}{p(1-\epsilon)}$, which is the desired conclusion.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:repair-nn-2}]
The analysis of $\wh{v}_1,...,\wh{v}_p$ is the same as that in the proof of Theorem \ref{thm:repair-nn-1}, and we have $\wt{W}_j=\wh{W}_j$ for all $j\in[p]$ with high probability.

To analyze $\wh{u}$, we apply Theorem \ref{thm:robust-reg}. It suffices to check Condition A and Condition B for the design matrix $\psi(X^T\wt{W}^T)=\psi(X^T\wh{W}^T)$. To check Condition A, we consider i.i.d. Rademacher random variables $\delta_1,...,\delta_m$. Then, we define a different gradient update with initialization $\check{W}_j(0)=\delta_jW_j(0)$ and $\check{\beta}_j(0)=\delta_j\beta_j(0)$, and
\begin{eqnarray*}
\check{W}_j(t) &=& \check{W}_j(t-1) - \frac{\gamma}{d}\frac{\partial L(\beta,W)}{\partial W_j}|_{(\beta,W)=(\check{\beta}(t-1),\check{W}(t-1))}, \\
\check{\beta}_j(t) &=& \check{\beta}_j(t-1) - \gamma\frac{\partial L(\beta,W)}{\partial \beta_j}|_{(\beta,W)=(\check{\beta}(t-1),\check{W}(t-1))}.
\end{eqnarray*}
In other words, $(W(t),\beta(t))$ and $(\check{W}(t),\check{\beta}(t))$ only differ in terms of the initialization. We also define $\check{u}_i(t)=\frac{1}{\sqrt{p}}\sum_{j=1}^p\check{\beta}_j(t)\psi(\check{W}_j(t)^Tx_i)$. It is easy to see that
$$\check{u}_i(t)=\frac{1}{\sqrt{p}}\sum_{j=1}^p\delta_j\beta_j(t)\psi(\delta_jW_j(t)^Tx_i)=\frac{1}{\sqrt{p}}\sum_{j=1}^p\beta_j(t)\psi(W_j(t)^Tx_i)=u_i(t).$$
Suppose $\check{W}_j(k)=\delta_jW_j(k)$ and $\check{\beta}_j(k)=\delta_j\beta_j(k)$ are true. Since
\begin{eqnarray*}
\frac{\partial L(\beta,W)}{\partial W_j}|_{(\beta,W)=(\check{\beta}(k),\check{W}(k))} &=& \frac{1}{\sqrt{p}}\check{\beta}_j(k)\sum_{i=1}^n(\check{u}_i(k)-y_i)\psi'(\check{W}_j(k)^Tx_i)x_i \\
&=& \frac{1}{\sqrt{p}}\delta_j{\beta}_j(k)\sum_{i=1}^n({u}_i(k)-y_i)\psi'(\delta_j{W}_j(k)^Tx_i)x_i \\
&=& \frac{1}{\sqrt{p}}\delta_j{\beta}_j(k)\sum_{i=1}^n({u}_i(k)-y_i)\psi'({W}_j(k)^Tx_i)x_i \\
&=& \delta_j\frac{\partial L(\beta,W)}{\partial W_j}|_{(\beta,W)=({\beta}(k),{W}(k))},
\end{eqnarray*}
and
\begin{eqnarray*}
\frac{\partial L(\beta,W)}{\partial \beta_j}|_{(\beta,W)=(\check{\beta}(k),\check{W}(k))} &=& \frac{1}{\sqrt{p}}\sum_{i=1}^n(\check{u}_i(k)-y_i)\psi(\check{W}_j(k)^Tx_i) \\
&=& \frac{1}{\sqrt{p}}\sum_{i=1}^n({u}_i(k)-y_i)\psi(\delta_j{W}_j(k)^Tx_i) \\
&=& \delta_j\frac{1}{\sqrt{p}}\sum_{i=1}^n({u}_i(k)-y_i)\psi({W}_j(k)^Tx_i) \\
&=& \delta_j\frac{\partial L(\beta,W)}{\partial \beta_j}|_{(\beta,W)=({\beta}(k),{W}(k))},
\end{eqnarray*}
we then have $\check{W}_j(k+1)=\delta_jW_j(k+1)$ and $\check{\beta}_j(k+1)=\delta_j\beta_j(k+1)$. A mathematical induction argument leads to $\check{W}_j(t)=\delta_jW_j(t)$ and $\check{\beta}_j(t)=\delta_j\beta_j(t)$ for all $t\geq 1$. Since $(\check{W}(0),\check{\beta}(0))$ and $(W(0),\beta(0))$ have the same distribution, we can conclude that $(\check{W}(t),\check{\beta}(t))$ and $(W(t),\beta(t))$ also have the same distribution. Therefore, Condition A holds for the design matrix $\psi(X^T\wh{W}^T)=\psi(X^TW(t_{\max})^T)$.

We also need to check Condition B. By Theorem \ref{thm:nn-grad}, we have
\begin{eqnarray*}
&& \left|\frac{1}{p}\sum_{j=1}^p\left|\sum_{i=1}^n\psi(\wh{W}_j^Tx_i)\Delta_i\right| - \frac{1}{p}\sum_{j=1}^p\left|\sum_{i=1}^n\psi(W_j(0)^Tx_i)\Delta_i\right|\right| \\
&\leq& \frac{1}{p}\sum_{j=1}^p\sum_{i=1}^n|\wh{W}_j^Tx_i-W_j(0)^Tx_i||\Delta_i| \\
&\leq& R_1\sum_{i=1}^n\|x_i\||\Delta_i| \\
&\leq& R_1\sqrt{\sum_{i=1}^n\|x_i\|^2} \\
&\lesssim& \frac{n^{3/2}\log p}{\sqrt{p}},
\end{eqnarray*}
where $\sum_{i=1}^n\|x_i\|^2\lesssim nd$ is by Lemma \ref{lem:chi-squared}. By Lemma \ref{lem:design-rf}, we can deduce that
$$\inf_{\|\Delta\|=1}\frac{1}{p}\sum_{j=1}^p\left|\sum_{i=1}^n\psi(\wh{W}_j^Tx_i)\Delta_i\right|\gtrsim 1,$$
as long as $\frac{n^{3/2}\log p}{\sqrt{p}}$ is sufficiently small. By (\ref{eq:G-last-op}) and the result of (\ref{lem:lim-G}), we also have
$$\sup_{\|\Delta\|=1}\frac{1}{p}\sum_{j=1}^p\left|\sum_{i=1}^n\psi(\wh{W}_j^Tx_i)\Delta_i\right|^2\lesssim 1+\frac{n^2\log p}{\sqrt{p}}.$$
Therefore, Condition B holds with $\overline{\lambda}^2\asymp 1+\frac{n^2\log p}{\sqrt{p}}$ and $\underline{\lambda}\asymp 1$. Apply Theorem \ref{thm:robust-reg}, we have $\wt{\beta}=\wh{\beta}$ with high probability as desired.
\end{proof}






