% !TEX root = ./repair.tex



\section{Proofs}
\label{sec:proof}

\subsection{Technical Lemmas}

We present a few technical lemmas that will be used in the proofs. The first lemma is Hoeffding's inequality
\begin{lemma}[\cite{hoeffding1963probability}]\label{lem:hoeffding}
Consider independent random variables $X_1,...,X_n$ that satisfy $X_i\in[a_i,b_i]$ for all $i\in[n]$. Then, for any $t>0$,
$$\mathbb{P}\left(\left|\sum_{i=1}^n(X_i-\mathbb{E}X_i)\right|>t\right) \leq 2\exp\left(-\frac{2t^2}{\sum_{i=1}^n(b_i-a_i)^2}\right).$$
\end{lemma}

Next, we need a central limit theorem with an explicit third moment bound. The following lemma is Theorem 2.20 of \cite{ross2007second}.
\begin{lemma}\label{lem:stein}
If $Z\sim N(0,1)$ and $W=\sum_{i=1}^nX_i$ where $X_i$ are independent mean $0$ and $\Var(W)=1$, then
$$\sup_z\left|\mathbb{P}(W\leq z)-\mathbb{P}(Z\leq z)\right|\leq 2\sqrt{3\sum_{i=1}^n\mathbb{E}|X_i|^3}.$$
\end{lemma}

We also need a Talagrand Gaussian concentration inequality. The following version has explicit constants.
\begin{lemma}[\cite{cirel1976norms}]\label{lem:talagrand}
Let $f:\mathbb{R}^k\rightarrow\mathbb{R}$ be a Lipschitz function with constant $L>0$. That is, $|f(x)-f(y)|\leq L\|x-y\|$ for all $x,y\in\mathbb{R}^k$. Then, for any $t>0$,
$$\mathbb{P}\left(|f(Z)-\mathbb{E}f(Z)|>t\right)\leq 2\exp\left(-\frac{t^2}{2L^2}\right),$$
where $Z\sim N(0,I_k)$.
\end{lemma}

Finally, we present two lemmas on the concentration of norms and inner products of multivariate Gaussian.
\begin{lemma}[\cite{laurent2000adaptive}]\label{lem:chi-squared}
For any $t>0$, we have
\begin{eqnarray*}
\mathbb{P}\left(\chi_k^2\geq k+2\sqrt{tk}+2t\right) &\leq& e^{-t}, \\
\mathbb{P}\left(\chi_k^2\leq k-2\sqrt{tk}\right) &\leq& e^{-t}.
\end{eqnarray*}
\end{lemma}

\begin{lemma}\label{lem:inner-prod}
Consider independent $Y_1,Y_2\sim N(0,I_k)$. For any $t>0$, we have
\begin{eqnarray*}
\mathbb{P}\left(|\|Y_1\|\|Y_2\|-k|\geq 2\sqrt{tk}+2t\right) &\leq& 4e^{-t}, \\
\mathbb{P}\left(|Y_1^TY_2| \geq \sqrt{2kt}+2t\right) &\leq& 2e^{-t}.
\end{eqnarray*}
\end{lemma}
\begin{proof}
By Lemma \ref{lem:chi-squared}, we have
\begin{align*}
\mathbb{P}\left(\|Y_1\|\|Y_2\| -k \geq 2\sqrt{tk}+2t\right)
&\leq \mathbb{P}\left(\|Y_1\|^2 \geq k +2\sqrt{tk}+2t\right) + \mathbb{P}\left(\|Y_2\|^2 \geq k +2\sqrt{tk}+2t\right) \\
&\leq 2e^{-t},
\end{align*}
and
\begin{eqnarray*}
&& \mathbb{P}\left(\|Y_1\|\|Y_2\| -k \leq -2\sqrt{tk}-2t\right) \\
&\leq& \mathbb{P}\left(\|Y_1\|^2 \leq k -2\sqrt{tk}\right) + \mathbb{P}\left(\|Y_2\|^2 \leq k -2\sqrt{tk}\right) \\
&\leq& 2e^{-t}.
\end{eqnarray*}
Summing up the two bounds above, we obtain the first conclusion. For the second conclusion, note that
$$\mathbb{P}\left(Y_1^TY_2 \geq x\right)\leq e^{-\lambda x}\mathbb{E}e^{\lambda Y_1^TY_2}=\exp\left(-\lambda x-\frac{k}{2}\log(1-\lambda^2)\right)\leq \exp\left(-\lambda x+\frac{k}{2}\lambda^2\right),$$
for any $x>0$ and $\lambda\in (0,1)$. Optimize over $\lambda\in (0,1)$, and we obtain $\mathbb{P}\left(Y_1^TY_2>x\right)\leq e^{-\frac{1}{2}\left(\frac{x^2}{k}\wedge x\right)}$. Take $x=\sqrt{2kt}+2t$, and then we obtain the bound
$$\mathbb{P}\left(Y_1^TY_2 \geq \sqrt{2kt}+2t\right)\leq e^{-t},$$
which immediately implies the second conclusion.
\end{proof}



\subsection{Proof of Theorem \ref{thm:robust-reg}} \label{sec:pf-robust-reg}

In order to prove Theorem \ref{thm:robust-reg}, we need the following empirical process result.
\begin{lemma}\label{lem:EP}
Consider independent random variables $z_1,...,z_m$. Assume $k/m\leq 1$. Then, for any $t\in (0,1/2)$ and any fixed $A^T=(a_1,...,a_m)^T\in\mathbb{R}^{m\times k}$ such that (\ref{eq:l2-upper-A}) holds, we have
$$\sup_{\|\Delta\|\leq t}\left|\frac{1}{m}\sum_{i=1}^m[(|a_i^T\Delta-z_i|-|z_i|)-\mathbb{E}(|a_i^T\Delta-z_i|-|z_i|)]\right|| \lesssim t\overline{\lambda}\sqrt{\frac{k}{m}\log\left(\frac{em}{k}\right)},$$
with high probability.
\end{lemma}
\begin{proof}
We use the notation $G_m(\Delta)=\frac{1}{m}\sum_{i=1}^m[(|a_i^T\Delta-z_i|-|z_i|)-\mathbb{E}(|a_i^T\Delta-z_i|-|z_i|)]$, and we apply a discretization argument. For the Euclidian ball $B_k(t)=\{\Delta\in\mathbb{R}^k:\|\Delta\|\leq t\}$, there exists a subset $\mathcal{N}_{t,\zeta}\subset B_k(t)$, such that for any $\Delta\in B_k(t)$, there exists a $\Delta'\in\mathcal{N}_{t,\zeta}$ that satisfies $\|\Delta-\Delta'\|\leq \zeta$, and we also have the bound $\log|\mathcal{N}_{t,\zeta}|\leq k\log(1+2t/\zeta)$ according to Lemma 5.2 of \cite{vershynin2010introduction}. For any $\Delta\in B_k(t)$ and the corresponding $\Delta'\in\mathcal{N}_{t,\zeta}$ that satisfies $\|\Delta-\Delta'\|\leq \zeta$, we have
\begin{eqnarray*}
|G_m(\Delta)-G_m(\Delta')| &\leq& 2\frac{1}{m}\sum_{i=1}^m|a_i^T(\Delta-\Delta')| \\
&\leq& 2\sqrt{\frac{1}{m}\sum_{i=1}^m|a_i^T(\Delta-\Delta')|^2} \leq 2\overline{\lambda}\zeta,
\end{eqnarray*}
where the last line is due to the condition (\ref{eq:l2-upper-A}). Thus,
$$\left|G_m(\Delta)\right| \leq \left|G_m(\Delta')\right| + 2\overline{\lambda}\zeta.$$
Taking supremum over both sides of the inequality, we obtain
\begin{equation}
\sup_{\|\Delta\|\leq t}|G_m(\Delta)| \leq \max_{\Delta\in\mathcal{N}_{t,\zeta}}\left|G_m(\Delta)\right| + 2\overline{\lambda}\zeta. \label{eq:disc-not-good}
\end{equation}
For any $\Delta\in B_k(t)$, we have
$$\frac{1}{m}\sum_{i=1}^m\left(|a_i^T\Delta-z_i|-|z_i|\right)^2\leq \frac{1}{m}\sum_{i=1}^m|a_i^T\Delta|^2\leq \overline{\lambda}^2t^2.$$
By Lemma \ref{lem:hoeffding}, we have
$$\mathbb{P}\left(\left|G_m(\Delta)\right| > x\right) \leq 2\exp\left(-\frac{2mx^2}{\overline{\lambda}^2t^2}\right).$$
A union bound argument leads to
\begin{equation}
\mathbb{P}\left(\max_{\Delta\in\mathcal{N}_{t,\zeta}}\left|G_m(\Delta)\right| > x\right) \leq 2\exp\left(-\frac{2mx^2}{\overline{\lambda}^2t^2}+k\log\left(1+\frac{2t}{\zeta}\right)\right).\label{eq:double-ub}
\end{equation}
By choosing $x^2\asymp \frac{t^2\bar{\lambda}^2k\log(1+2t/\zeta)}{m}$, we have
$$\max_{\Delta\in\mathcal{N}_{t,\zeta}}\left|G_m(\Delta)\right| \lesssim t\bar{\lambda}\sqrt{\frac{k\log(1+2t/\zeta)}{m}},$$
with high probability. Together with the bound (\ref{eq:disc-not-good}), we have
$$\sup_{\|\Delta\|\leq t}|G_m(\Delta)| \lesssim t\bar{\lambda}\sqrt{\frac{k\log(1+2t/\zeta)}{m}} + \bar{\lambda}\zeta,$$
with high probability.
The choice $\zeta=t\sqrt{k/m}$ leads to the desired result.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:robust-reg}]
Recall the definition of $L_m(u)$ in the proof of Theorem \ref{thm:main-improved}.
We introduce i.i.d. Rademacher random variables $\delta_1,...,\delta_m$. With the notation $\wt{a}_i=\delta_ia_i$, $\wt{b}_i=\delta_ib_i$ and $\wt{z}_i=\delta_iz_i$, we can write
$$
L_m(u) = \frac{1}{m}\sum_{i=1}^m\left(|\wt{a}_i^T(u^*-u)+\wt{z}_i|-|\wt{z}_i|\right).
$$
Let $\wt{A}\in\mathbb{R}^{m\times k}$ be the matrix whose $i$th row is $\wt{a}_i^T$. By the symmetry of $A$, we have $\mathbb{P}(\wt{A}\in U|\delta)=\mathbb{P}(\wt{A}\in U)=\mathbb{P}(A\in U)$ for any measurable set $U$. Therefore, for any measurable sets $U$ and $V$, we have
\begin{eqnarray*}
\mathbb{P}(\wt{A}\in U, \wt{z}\in V) &=& \mathbb{E}\mathbb{P}(\wt{A}\in U, \wt{z}\in V|\delta) \\
&=& \mathbb{E}\mathbb{P}(\wt{A}\in U|\delta)\mathbb{P}(\wt{z}\in V|\delta) \\
&=& \mathbb{E}\mathbb{P}(\wt{A}\in U)\mathbb{P}(\wt{z}\in V|\delta) \\
&=& \mathbb{P}(\wt{A}\in U)\mathbb{P}(\wt{z}\in V),
\end{eqnarray*}
and thus $\wt{A}$ ad $\wt{z}$ are independent. Define $L(u)=\mathbb{E}(L_m(u)|\wt{A})$. Suppose $\|\wh{u}-u^*\|\geq t$, we must have
$$\inf_{\|u-u^*\|\geq t}L_m(u) \leq L_m(u^*).$$
By the convexity of $L_m(u)$, we can replace $\|u-u^*\|\geq t$ by $\|u-u^*\| = t$ and the above inequality still holds, and therefore $\inf_{\|u-u^*\|= t}L_m(u)\leq 0$. This implies
\begin{equation}
\inf_{\|u-u^*\|=t}L(u) \leq \sup_{\|u-u^*\|= t}|L_m(u)-L(u)|. \label{eq:basic-L}
\end{equation}
Now we study $L(u)$. Introduce the function $f_i(x)=\mathbb{E}(|x+\wt{z}_i|-|\wt{z}_i|)$ so that we can write $L(u)=\frac{1}{m}\sum_{i=1}^mf_i(\wt{a}_i^T(u^*-u))$. For any $x\geq 0$,
\begin{eqnarray*}
f_i(x) &=& \mathbb{E}(|x+\wt{z}_i|-|\wt{z}_i|)\mathbb{I}\{\wt{z}_i<-x\} + \mathbb{E}(|x+\wt{z}_i|-|\wt{z}_i|)\mathbb{I}\{\wt{z}_i> 0\} \\
&& + \mathbb{E}(|x+\wt{z}_i|-|\wt{z}_i|)\mathbb{I}\{-x\leq \wt{z}_i< 0\} + x\mathbb{P}(\wt{z}_i=0) \\
&=& -x\mathbb{P}(\wt{z}_i<-x) + x\mathbb{P}(\wt{z}_i> 0) + \mathbb{E}(x+2\wt{z}_i)\mathbb{I}\{-x\leq \wt{z}_i<0\}   + x\mathbb{P}(\wt{z}_i=0) \\
&\geq& -x\mathbb{P}(\wt{z}_i<-x) + x\mathbb{P}(\wt{z}_i> 0) - x\mathbb{P}(-x\leq \wt{z}_i< 0)   + x\mathbb{P}(\wt{z}_i=0) \\
&=& -x\mathbb{P}(\wt{z}_i<-x) + x\mathbb{P}(\wt{z}_i< 0) - x\mathbb{P}(-x\leq \wt{z}_i< 0)  + x\mathbb{P}(\wt{z}_i=0) \\
&\geq& x\mathbb{P}(\wt{z}_i=0) \\
&\geq& (1-\epsilon)x.
\end{eqnarray*}
By the symmetry of $\wt{z}_i$, we also have
$$f_i(-x)=\mathbb{E}(|-x+\wt{z}_i|-|\wt{z}_i|)=\mathbb{E}(|x-\wt{z}_i|-|\wt{z}_i|)=\mathbb{E}(|x+\wt{z}_i|-|\wt{z}_i|)=f_i(x),$$
which implies $f_i(x)\geq (1-\epsilon)|x|$. Therefore, for any $u$ such that $\|u-u^*\|=t$, we have
\begin{eqnarray*}
L(u) &=& \frac{1}{m}\sum_{i=1}^mf_i(\wt{a}_i^T(u^*-u)) \\
&\geq& (1-\epsilon)\frac{1}{m}\sum_{i=1}^m|\wt{a}_i^T(u^*-u)| \\
&=& (1-\epsilon)\frac{1}{m}\sum_{i=1}^m|a_i^T(u^*-u)| \\
&\geq& \underline{\lambda}(1-\epsilon)t,
\end{eqnarray*}
where the last inequality is by (\ref{eq:l1-upper-A}). Together with (\ref{eq:basic-L}), we have
\begin{equation}
\mathbb{P}\left(\|\wh{u}-u\|\geq t\right) \leq \mathbb{P}\left(\sup_{\|u-u^*\|= t}|L_m(u)-L(u)| \geq \underline{\lambda}(1-\epsilon)t/2\right). \label{eq:larger-t-prob}
\end{equation}
Since the condition (\ref{eq:l2-upper-A}) continues to hold with $A$ replaced by $\wt{A}$, we can apply Lemma \ref{lem:EP} and obtain that
$$\sup_{\|u-u^*\|= t}|L_m(u)-L(u)| \lesssim t\overline{\lambda}\sqrt{\frac{k}{m}\log\left(\frac{em}{k}\right)},$$
with high probability. Under the conditions of the theorem, we know that $\frac{t\overline{\lambda}\sqrt{\frac{k}{m}\log\left(\frac{em}{k}\right)}}{\underline{\lambda}(1-\epsilon)t}$ is sufficiently small, and thus by (\ref{eq:larger-t-prob}), $\|\wh{u}-u^*\|<t$ with high probability. Since $t$ s arbitrary, we must have $\wh{u}=u^*$.
\end{proof}


\subsection{Proofs of Lemma \ref{lem:design-linear}, Corollary \ref{cor:repair-linear}, Lemma \ref{lem:design-rf-relu} and Corollary \ref{cor:repair-rf-relu}}


\begin{proof}[Proof of Lemma \ref{lem:design-linear}]
Condition A is obvious. For Condition B, we have
$$
\inf_{\|\Delta\|=1}\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta| \geq \sqrt{\frac{2}{\pi}} - \sup_{\|\Delta\|=1}\left|\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta| - \sqrt{\frac{2}{\pi}} \right|,
$$
and we will analyze the second term on the right hand side of the inequality above via a discretization argument for the Euclidean sphere $S^{n-1}=\{\Delta\in\mathbb{R}^n: \|\Delta\|=1\}$. There exists a subset $\mathcal{N}_{\zeta}\subset S^{n-1}$, such that for any $\Delta\in S^{n-1}$, there exists a $\Delta'\in\mathcal{N}_{\zeta}$ that satisfies $\|\Delta-\Delta'\|\leq\zeta$, and we also have the bound $\log|\mathcal{N}_{\zeta}|\leq n\log\left(1+2/\zeta\right)$ according to Lemma 5.2 of \cite{vershynin2010introduction}. For any $\Delta\in S^{n-1}$ and the corresponding $\Delta'\in\mathcal{N}_{\zeta}$ that satisfies $\|\Delta-\Delta'\|\leq \zeta$, we have
\begin{eqnarray*}
\left|\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta| - \sqrt{\frac{2}{\pi}} \right| &\leq& \left|\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta'| - \sqrt{\frac{2}{\pi}} \right|  + \zeta\sup_{\|\Delta\|=1}\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta| \\
&\leq& \left|\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta'| - \sqrt{\frac{2}{\pi}} \right| + \zeta\sup_{\|\Delta\|=1}\left|\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta| - \sqrt{\frac{2}{\pi}} \right| + \zeta\sqrt{\frac{2}{\pi}}.
\end{eqnarray*}
Taking supremum on both sides of the inequality, with some arrangements, we obtain
$$\sup_{\|\Delta\|=1}\left|\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta| - \sqrt{\frac{2}{\pi}} \right|\leq (1-\zeta)^{-1}\max_{\Delta\in\mathcal{N}_{\zeta}}\left|\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta| - \sqrt{\frac{2}{\pi}} \right| + \frac{\zeta}{1-\zeta}\sqrt{\frac{2}{\pi}}.$$
Set $\zeta=1/3$, and we then have
$$\inf_{\|\Delta\|=1}\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta| \geq (2\pi)^{-1} - \frac{3}{2}\max_{\Delta\in\mathcal{N}_{1/3}}\left|\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta| - \sqrt{\frac{2}{\pi}} \right|.$$
Lemma \ref{lem:talagrand} together with a union bound argument leads to
$$\mathbb{P}\left(\max_{\Delta\in\mathcal{N}_{1/3}}\left|\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta| - \sqrt{\frac{2}{\pi}} \right|>t\right)\leq 2\exp\left(n\log(7)-\frac{pt^2}{2}\right),$$
which implies $\max_{\Delta\in\mathcal{N}_{1/3}}\left|\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta| - \sqrt{\frac{2}{\pi}} \right|\lesssim \sqrt{\frac{n}{p}}$ with high probability. Since $n/p$ is sufficiently small, we have $\inf_{\|\Delta\|=1}\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta|\gtrsim 1$ with high probability as desired. The high probability bound $\sup_{\|\Delta\|=1}\frac{1}{p}\sum_{j=1}^p|a_j^T\Delta|^2=\opnorm{A}^2/p\lesssim 1+n/p$ is by \cite{davidson2001local}, and the proof is complete.
\end{proof}

\begin{proof}[Proof of Corollary \ref{cor:repair-linear}]
Since $\wh{\theta}$ belongs to the row space of $X$, there exists some $u^*\in\mathbb{R}^n$ such that $\wh{\theta}=X^Tu^*$.
By Theorem \ref{thm:main-improved} and Lemma \ref{lem:design-linear}, we know that $\wt{u}=u^*$ with high probability, and therefore $\wt{\theta}=X^T\wt{u}=X^Tu^*=\wh{\theta}$.
\end{proof}


Now we state the proof of Lemma \ref{lem:design-rf-relu}. The conclusion of Condition A is obvious by
$$\sum_{i=1}^n\mathbb{E}\left(\frac{1}{p}\sum_{j=1}^pc_j\psi(W_j^Tx_i)\right)^2\leq \sum_{i=1}^n\frac{1}{p}\sum_{j=1}^p\mathbb{E}|W_j^Tx_i|^2= n,$$
and Markov's inequality. To check Condition B, we prove (\ref{eq:l1-upper-A}) and (\ref{eq:l2-upper-A}) separately.
\begin{proof}[Proof of (\ref{eq:l1-upper-A}) of Lemma \ref{lem:design-rf-relu}]
Let us adopt the notation that
$$f(W,X,\Delta)=\frac{1}{p}\sum_{j=1}^p\left|\sum_{i=1}^n\psi(W_j^Tx_i)\Delta_i\right|.$$
Define $g(X,\Delta)=\mathbb{E}(f(W,X,\Delta)|X)$.
We then have
\begin{eqnarray}
\nonumber \inf_{\|\Delta\|=1}f(W,X,\Delta) &\geq& \inf_{\|\Delta\|=1}\mathbb{E}f(W,X,\Delta) - \sup_{\|\Delta\|=1}\left|f(W,X,\Delta)-\mathbb{E}f(W,X,\Delta)\right| \\
\label{eq:exp-f-inf-relu} &\geq& \inf_{\|\Delta\|=1}\mathbb{E}f(W,X,\Delta) \\
\label{eq:ep-f-relu} && - \sup_{\|\Delta\|=1}\left|f(W,X,\Delta)-\mathbb{E}(f(W,X,\Delta)|X)\right| \\
\label{eq:ep-g-relu} && - \sup_{\|\Delta\|=1}\left|g(X,\Delta)-\mathbb{\mathbb{E}}g(X,\Delta)\right|.
\end{eqnarray}
We will analyze the three terms above separately.

\paragraph{Analysis of (\ref{eq:exp-f-inf-relu}).} Define $h(W_j)=\mathbb{E}(\psi(W_j^Tx_i)|W_j)$ and $\bar{\psi}(W_j^Tx_i)=\psi(W_j^Tx_i)-h(W_j)$. We then have
\begin{equation}
\mathbb{E}f(W,X,\Delta)=\mathbb{E}\left|\sum_{i=1}^n\bar{\psi}(W^Tx_i)\Delta_i+h(W)\sum_{i=1}^n\Delta_i\right|. \label{eq:will-be-split}
\end{equation}
A lower bound of (\ref{eq:will-be-split}) is
$$\mathbb{E}f(W,X,\Delta)\geq \left|\sum_{i=1}^n\Delta_i\right|\left|\mathbb{E}h(W)\right|-\mathbb{E}\left|\sum_{i=1}^n\bar{\psi}(W^Tx_i)\Delta_i\right|,$$
where the second term can be bounded by
\begin{eqnarray*}
\mathbb{E}\left|\sum_{i=1}^n\bar{\psi}(W^Tx_i)\Delta_i\right| &\leq& \sqrt{\mathbb{E}\left|\sum_{i=1}^n\bar{\psi}(W^Tx_i)\Delta_i\right|^2} \\
&=& \sqrt{\mathbb{E}\Var\left(\left|\sum_{i=1}^n\psi(W^Tx_i)\Delta_i\right|\Big|W\right)} \\
&=& \sqrt{\mathbb{E}\sum_{i=1}^n\Delta_i^2\Var(\psi(W^Tx_i)|W)} \\
&=& \sqrt{\mathbb{E}\sum_{i=1}^n\Delta_i^2\mathbb{E}(|\psi(W^Tx_i)|^2|W)} \\
&=& \sqrt{\mathbb{E}|\psi(W^Tx)|^2} \leq \sqrt{\mathbb{E}|W^Tx|^2} = 1.
\end{eqnarray*}
Since
$$\mathbb{E}h(W)=\frac{1}{\sqrt{2\pi}}\mathbb{E}\|W\|=\frac{1}{\sqrt{\pi}}\frac{\Gamma((d+1)/2)}{\sqrt{d}\Gamma(d/2)}\geq \frac{1}{\sqrt{2\pi}}\sqrt{\frac{d-1}{d}}.$$
Therefore, as long as $d\geq 3$ and $\left|\sum_{i=1}^n\Delta_i\right|\geq 7$, we have $\mathbb{E}f(W,X,\Delta)\geq 1$, and we thus can conclude that
\begin{equation}
\inf_{\|\Delta\|=1,|\sum_{i=1}^n\Delta_i|\geq 7}\mathbb{E}f(W,X,\Delta) \gtrsim 1.\label{eq:l1-1-1}
\end{equation}

Now we consider the case $\left|\sum_{i=1}^n\Delta_i\right|< 7$. A lower bound for $\left|\sum_{i=1}^n\psi(W^Tx_i)\Delta_i\right|$ is
\begin{equation}
\left|\sum_{i=1}^n\psi(W^Tx_i)\Delta_i\right| \geq \left|\sum_{i=1}^n\bar{\psi}(W^Tx_i)\Delta_i\right| - 7h(W) = \left|\sum_{i=1}^n\bar{\psi}(W^Tx_i)\Delta_i\right| - \frac{7}{\sqrt{2\pi}}\|W\|. \label{eq:seven}
\end{equation}
Thus,
\begin{eqnarray}
\nonumber \mathbb{E}f(W,X,\Delta) &\geq& \mathbb{E}\left(\left|\sum_{i=1}^n\psi(W^Tx_i)\Delta_i\right|\mathbb{I}\left\{\left|\sum_{i=1}^n\bar{\psi}(W^Tx_i)\Delta_i\right|\geq 6, 1/2\leq \|W\|^2\leq 2\right\}\right) \\
\nonumber &\geq& \mathbb{P}\left(\left|\sum_{i=1}^n\bar{\psi}(W^Tx_i)\Delta_i\right|\geq 6, 1/2\leq \|W\|^2\leq 2\right) \\
\nonumber &=& \mathbb{P}\left(\left|\sum_{i=1}^n\bar{\psi}(W^Tx_i)\Delta_i\right|\geq 6\Big|1/2\leq \|W\|^2\leq 2\right)\mathbb{P}\left(1/2\leq \|W\|^2\leq 2\right) \\
\nonumber &\geq& \mathbb{P}\left(\left|\sum_{i=1}^n\bar{\psi}(W^Tx_i)\Delta_i\right|\geq 6\Big|1/2\leq \|W\|^2\leq 2\right)\left(1-2\exp(-d/16)\right),
\end{eqnarray}
where the last inequality is by Lemma \ref{lem:chi-squared}. By direct calculation, we have
\begin{equation}
\Var\left(\bar{\psi}(W^Tx)|W\right)=\|W\|^2\Var(\max(0,W^Tx/\|W\|)|W)=\|W\|^2\frac{1-\pi^{-1}}{2}, \label{eq:cond-var-X-W}
\end{equation}
and
$$\mathbb{E}\left(|\bar{\psi}(W^Tx)|^3|W\right) \leq 3\mathbb{E}\left(|\psi(W^Tx)|^3|W\right)+3|h(W)|^3 \leq \frac{3}{2}\|W\|^3.$$
Therefore, by Lemma \ref{lem:stein}, we have
\begin{eqnarray}
\nonumber && \mathbb{P}\left(\left|\sum_{i=1}^n\bar{\psi}(W^Tx_i)\Delta_i\right|\geq 6\Big|1/2\leq \|W\|^2\leq 2\right) \\
\nonumber &\geq& \mathbb{P}\left(\frac{\left|\sum_{i=1}^n\bar{\psi}(W^Tx_i)\Delta_i\right|}{\|W\|\sqrt{\frac{1-\pi^{-1}}{2}}}\geq 21\Bigg|1/2\leq \|W\|^2\leq 2\right) \\
\nonumber &\geq& \mathbb{P}\left(N(0,1)>21\right) - \sup_{1/2\leq \|W\|^2\leq 2} 2\sqrt{3\sum_{i=1}^n|\Delta_i|^3\frac{\mathbb{E}\left(|\bar{\psi}(W^Tx_i)|^3|W\right)}{\|W\|^3\left(\frac{1-\pi^{-1}}{2}\right)^{3/2}}} \\
\nonumber &\geq& \mathbb{P}\left(N(0,1)>21\right) - 10\sqrt{\sum_{i=1}^n|\Delta_i|^3} \\
\nonumber &\geq& \mathbb{P}\left(N(0,1)>21\right) - 10\max_{1\leq i\leq n}|\Delta_i|^{3/2}.
\end{eqnarray}
Hence, when $\max_{1\leq i\leq n}|\Delta_i|^{3/2}\leq \delta_0^{3/2}:=\mathbb{P}\left(N(0,1)>21\right)/20$ and $\left|\sum_{i=1}^n\Delta_i\right|< 7$, we can lower bound $\mathbb{E}f(W,X,\Delta)$ by an absolute constant, and we conclude that
\begin{equation}
\inf_{\|\Delta\|=1,|\sum_{i=1}^n\Delta_i|\leq 7, \max_{1\leq i\leq n}|\Delta_i|\leq\delta_0}\mathbb{E}f(W,X,\Delta) \gtrsim 1.\label{eq:l1-1-2-relu}
\end{equation}

Finally, we consider the case when $\max_{1\leq i\leq n}|\Delta_i|> \delta_0$ and $\left|\sum_{i=1}^n\Delta_i\right|< 7$. Without loss of generality, we can assume $\Delta_1>\delta_0$. Note that the lower bound (\ref{eq:seven}) still holds, and thus we have
$$\left|\sum_{i=1}^n\psi(W^Tx_i)\Delta_i\right|\geq  \bar{\psi}(W^Tx_1)\Delta_1 - \left|\sum_{i=2}^n\bar{\psi}(W^Tx_i)\Delta_i\right| - \frac{7}{\sqrt{2\pi}}\|W\|.$$
We then lower bound $\mathbb{E}f(W,X,\Delta)$ by
\begin{eqnarray*}
&& \mathbb{E}\left(\left|\sum_{i=1}^n\psi(W^Tx_i)\Delta_i\right|\mathbb{I}\left\{ \bar{\psi}(W^Tx_1)\Delta_1 \geq 8, \left|\sum_{i=2}^n\bar{\psi}(W^Tx_i)\Delta_i\right|\leq 2, 1/2\leq \|W\|^2\leq 2\right\}\right) \\
&\geq& \mathbb{P}\left(\bar{\psi}(W^Tx_1)\Delta_1 \geq 8, \left|\sum_{i=2}^n\bar{\psi}(W^Tx_i)\Delta_i\right|\leq 2\Big|1/2\leq \|W\|^2\leq 2\right)\mathbb{P}\left(1/2\leq \|W\|^2\leq 2\right) \\
&\geq& \mathbb{P}\left(\bar{\psi}(W^Tx_1)\Delta_1 \geq 8\Big|1/2\leq \|W\|^2\leq 2\right) \\
&& \times \mathbb{P}\left(\left|\sum_{i=2}^n\bar{\psi}(W^Tx_i)\Delta_i\right|\leq 2\Big|1/2\leq \|W\|^2\leq 2\right)\left(1-2\exp(-d/16)\right).
\end{eqnarray*}
For any $W$ that satisfies $1/2\leq \|W\|^2\leq 2$, we have
\begin{eqnarray*}
\mathbb{P}\left(\bar{\psi}(W^Tx_1)\Delta_1 \geq 8\Big|W\right) &\geq& \mathbb{P}\left(\bar{\psi}(W^Tx_1)\geq 8/\delta_0\Big|W\right) \\
&\geq&  \mathbb{P}\left(\psi(W^Tx_1)\geq 8/\delta_0+1/\sqrt{\pi}\Big|W\right) \\
&\geq& \mathbb{P}\left(W^Tx_1\geq 8/\delta_0+1/\sqrt{\pi}\Big|W\right) \\
&\geq& \mathbb{P}\left(N(0,1)\geq \sqrt{2}8/\delta_0 + \sqrt{2/\pi}\right),
\end{eqnarray*}
which is a constant. We also have
\begin{eqnarray*}
&& \mathbb{P}\left(\left|\sum_{i=2}^n\bar{\psi}(W^Tx_i)\Delta_i\right|\leq 2\Big|1/2\leq \|W\|^2\leq 2\right) \\
&\geq& 1 - \frac{1}{4}\Var\left(\sum_{i=2}^n\bar{\psi}(W^Tx_i)\Delta_i\Big|W\right) \\
&\geq& \frac{1}{2},
\end{eqnarray*}
where the last inequality is by (\ref{eq:cond-var-X-W}). Therefore, we have
$$\mathbb{E}f(W,X,\Delta)\geq \frac{1}{2}\left(1-2\exp(-d/16)\right)\mathbb{P}\left(N(0,1)\geq \sqrt{2}8/\delta_0 + \sqrt{2/\pi}\right)\gtrsim 1,$$
and we can conclude that
\begin{equation}
\inf_{\|\Delta\|=1,|\sum_{i=1}^n\Delta_i|\leq 7, \max_{1\leq i\leq n}|\Delta_i|\geq\delta_0}\mathbb{E}f(W,X,\Delta) \gtrsim 1.\label{eq:l1-1-3-relu}
\end{equation}

In the end, we combine the three cases (\ref{eq:l1-1-1}), (\ref{eq:l1-1-2-relu}), and (\ref{eq:l1-1-3-relu}),  and we obtain the conclusion that $\inf_{\|\Delta\|=1}\mathbb{E}f(W,X,\Delta)\gtrsim 1$.


\paragraph{Analysis of (\ref{eq:ep-f-relu}).} We shorthand the conditional expectation operator $\mathbb{E}(\cdot|X)$ by $\mathbb{E}^X$. Let $\wt{W}$ be an independent copy of $W$, and we first bound the moment generating function via a standard symmetrization argument. For any $\lambda>0$,
\begin{eqnarray}
\nonumber && \mathbb{E}^X\exp\left(\lambda \sup_{\|\Delta\|=1}\left|f(W,X,\Delta)-\mathbb{E}^Xf(W,X,\Delta)\right|\right) \\
\nonumber &\leq& \mathbb{E}^X\exp\left(\lambda \mathbb{E}^{X,W}\sup_{\|\Delta\|=1}\left|f(W,X,\Delta)-f(\wt{W},X,\Delta)\right|\right) \\
\nonumber &\leq& \mathbb{E}^X\exp\left(\lambda \sup_{\|\Delta\|=1}\left|f(W,X,\Delta)-f(\wt{W},X,\Delta)\right|\right) \\
\nonumber &=& \mathbb{E}^X\exp\left(\lambda\sup_{\|\Delta\|=1}\left|\frac{1}{p}\sum_{j=1}^p\epsilon_j\left(\left|\sum_{i=1}^n\psi(W_j^Tx_i)\Delta_i\right|-\left|\sum_{i=1}^n\psi(\wt{W}_j^Tx_i)\Delta_i\right|\right)\right|\right) \\
\label{eq:mgf-b-relu} &\leq& \mathbb{E}^X\exp\left(2\lambda\sup_{\|\Delta\|=1}\left|\frac{1}{p}\sum_{j=1}^p\epsilon_j\left|\sum_{i=1}^n\psi(W_j^Tx_i)\Delta_i\right|\right|\right),
\end{eqnarray}
where $\epsilon_1,...,\epsilon_p$ are independent Rademacher random variables. Let us adopt the notation that
$$F(\epsilon,W,X,\Delta)=\frac{1}{p}\sum_{j=1}^p\epsilon_j\left|\sum_{i=1}^n\psi(W_j^Tx_i)\Delta_i\right|.$$
We use a discretization argument. For the Euclidean sphere $S^{n-1}=\{\Delta\in\mathbb{R}^n: \|\Delta\|=1\}$, there exists a subset $\mathcal{N}\subset S^{n-1}$, such that for any $\Delta\in S^{n-1}$, there exists a $\Delta'\in\mathcal{N}$ that satisfies $\|\Delta-\Delta'\|\leq 1/2$, and we also have the bound $\log|\mathcal{N}|\leq 2n$. See, for example, Lemma 5.2 of \cite{vershynin2010introduction}.
For any $\Delta\in S^{n-1}$ and the corresponding $\Delta'\in\mathcal{N}$ that satisfies $\|\Delta-\Delta'\|\leq 1/2$, we have
\begin{eqnarray*}
|F(\epsilon,W,X,\Delta)| &\leq& |F(\epsilon,W,X,\Delta')| + |F(\epsilon,W,X,\Delta-\Delta')| \\
&\leq& |F(\epsilon,W,X,\Delta')| + \frac{1}{2}\sup_{\|\Delta\|=1}|F(\epsilon,W,X,\Delta)|,
\end{eqnarray*}
which, by taking supremum over both sides, implies
$$\sup_{\|\Delta\|=1}|F(\epsilon,W,X,\Delta)|\leq 2\max_{\Delta\in\mathcal{N}}|F(\epsilon,W,X,\Delta)|.$$
Define $\bar{F}(\epsilon,X,\Delta)=\mathbb{E}^{\epsilon,X}F(\epsilon,W,X,\Delta)$, and then
$$\max_{\Delta\in\mathcal{N}}|F(\epsilon,W,X,\Delta)|\leq \max_{\Delta\in\mathcal{N}}|F(\epsilon,W,X,\Delta)-\bar{F}(\epsilon,X,\Delta)|+\max_{\Delta\in\mathcal{N}}|\bar{F}(\epsilon,X,\Delta)|.$$
In view of (\ref{eq:mgf-b-relu}), we obtain the bound
\begin{eqnarray}
\nonumber && \mathbb{E}^X\exp\left(\lambda \sup_{\|\Delta\|=1}\left|f(W,X,\Delta)-\mathbb{E}^Xf(W,X,\Delta)\right|\right) \\
\nonumber &\leq& \mathbb{E}^X\exp\left(4\lambda \max_{\Delta\in\mathcal{N}}|F(\epsilon,W,X,\Delta)-\bar{F}(\epsilon,X,\Delta)|+4\lambda \max_{\Delta\in\mathcal{N}}|\bar{F}(\epsilon,X,\Delta)|\right) \\
\label{eq:tala-mgf1-relu} &\leq& \frac{1}{2}\sum_{\Delta\in\mathcal{N}} \mathbb{E}^{X}\exp\left(4\lambda|F(\epsilon,W,X,\Delta)-\bar{F}(\epsilon,X,\Delta)|\right) \\
\label{eq:tala-mgf2-relu} && + \frac{1}{2}\sum_{\Delta\in\mathcal{N}} \mathbb{E}^{X}\exp\left(4\lambda |\bar{F}(\epsilon,X,\Delta)|\right).
\end{eqnarray}
We will bound the two terms above on the event $E=\left\{\sum_{i=1}^n\|x_i\|^2\leq 3nd\right\}$. For any $W,\wt{W}$, we have
\begin{eqnarray*}
\left|F(\epsilon,W,X,\Delta)-F(\epsilon,\wt{W},X,\Delta)\right| &\leq& \frac{1}{p}\sum_{j=1}^p\sum_{i=1}^n\left|({\psi}(W_j^Tx_i)-{\psi}(\wt{W}_j^Tx_i))\Delta_i\right| \\
&\leq& \frac{1}{p}\sum_{j=1}^p\sum_{i=1}^n|(W_j-\wt{W}_j)^Tx_i||\Delta_i| \\
&\leq& \frac{1}{p}\sum_{j=1}^p\sum_{i=1}^n\|W_j-\wt{W}_j\|\|x_i\||\Delta_i| \\
&\leq& \frac{1}{\sqrt{p}}\sqrt{\sum_{j=1}^p\|W_j-\wt{W}_j\|^2}\sqrt{\sum_{i=1}^n\|x_i\|^2} \\
&\leq& \sqrt{\frac{3n}{p}}\sqrt{\sum_{j=1}^p\|\sqrt{d}W_j-\sqrt{d}\wt{W}_j\|^2},
\end{eqnarray*}
where the last inequality holds under the event $E$. By Lemma \ref{lem:talagrand}, we have for any $X$ such that $E$ holds,
$$\mathbb{P}\left(|F(\epsilon,W,X,\Delta)-\bar{F}(\epsilon,X,\Delta)|>t\big|X\right)\leq 2\exp\left(-\frac{pt^2}{6n}\right),$$
for any $t>0$. The sub-Gaussian tail implies a bound for the moment generating function. By Lemma 5.5 of \cite{vershynin2010introduction}, we have
$$\mathbb{E}^{X}\exp\left(4\lambda|F(\epsilon,W,X,\Delta)-\bar{F}(\epsilon,X,\Delta)|\right) \leq \exp\left(C_1\frac{n}{p}\lambda^2\right),$$
for some constant $C_1>0$. To bound the moment generating function of $\bar{F}(\epsilon,X,\Delta)$, we note that
\begin{eqnarray*}
|\bar{F}(\epsilon,X,\Delta)|\ &\leq& \left|\frac{1}{p}\sum_{j=1}^p\epsilon_j\right|\mathbb{E}^X\left|\sum_{i=1}^n\psi(W^Tx_i)\Delta_i\right| \\
&\leq& \left|\frac{1}{p}\sum_{j=1}^p\epsilon_j\right|\sqrt{\sum_{i=1}^n\mathbb{E}^X|\psi(W^Tx_i)|^2} \\
&\leq& \left|\frac{1}{p}\sum_{j=1}^p\epsilon_j\right|\sqrt{\sum_{i=1}^n\|x_i\|^2/d} \leq \sqrt{3n}\left|\frac{1}{p}\sum_{j=1}^p\epsilon_j\right|,
\end{eqnarray*}
where the last inequality holds under the event $E$. With an application of Hoeffding-type inequality (Lemma 5.9 of \cite{vershynin2010introduction}), we have
$$\mathbb{E}^{X}\exp\left(4\lambda |\bar{F}(\epsilon,X,\Delta)|\right)\leq \mathbb{E}\exp\left(4\lambda\sqrt{3n}\left|\frac{1}{p}\sum_{j=1}^p\epsilon_j\right|\right)\leq \exp\left(C_1\frac{n}{p}\lambda^2\right).$$
Note that we can use the same constant $C_1$ by making its value sufficiently large. Plug the two moment generating function bounds into (\ref{eq:tala-mgf1-relu}) and (\ref{eq:tala-mgf2-relu}), and we obtain the bound
$$\mathbb{E}^X\exp\left(\lambda \sup_{\|\Delta\|=1}\left|f(W,X,\Delta)-\mathbb{E}^Xf(W,X,\Delta)\right|\right)\leq \exp\left(C_1\frac{n}{p}\lambda^2+2n\right),$$
for any $X$ such that $E$ holds. To bound (\ref{eq:ep-f}), we apply Chernoff bound, and then
$$\mathbb{P}\left(\sup_{\|\Delta\|=1}\left|f(W,X,\Delta)-\mathbb{E}(f(W,X,\Delta)|X)\right| > t\right) \leq \exp\left(-\lambda t + C_1\frac{n}{p}\lambda^2+2n\right).$$
Optimize over $\lambda$, set $t\asymp \sqrt{\frac{n^2}{p}}$, and we have
$$\sup_{\|\Delta\|=1}\left|f(W,X,\Delta)-\mathbb{E}(f(W,X,\Delta)|X)\right|\lesssim \sqrt{\frac{n^2}{p}},$$
with high probability.

\paragraph{Analysis of (\ref{eq:ep-g-relu}).} We use a discretization argument. There exists a subset $\mathcal{N}_{\zeta}\subset S^{n-1}$, such that for any $\Delta\in S^{n-1}$, there exists a $\Delta'\in\mathcal{N}_{\zeta}$ that satisfies $\|\Delta-\Delta'\|\leq\zeta$, and we also have the bound $\log|\mathcal{N}|\leq n\log\left(1+2/\zeta\right)$ according to Lemma 5.2 of \cite{vershynin2010introduction}. For any $\Delta\in S^{n-1}$ and the corresponding $\Delta'\in\mathcal{N}_{\zeta}$ that satisfies $\|\Delta-\Delta'\|\leq \zeta$, we have
\begin{eqnarray*}
|g(X,\Delta) - \mathbb{E}g(X,\Delta)| &\leq& |g(X,\Delta')-\mathbb{E}g(X,\Delta')| \\
&& + |g(X,\Delta-\Delta')-\mathbb{E}g(X,\Delta-\Delta')|  \\
&& + 2\mathbb{E}g(X,\Delta-\Delta') \\
&\leq& |g(X,\Delta')-\mathbb{E}g(X,\Delta')| \\
&& + \zeta\sup_{\|\Delta\|=1}|g(X,\Delta)-\mathbb{E}g(X,\Delta)|  \\
&& + 2\zeta\sup_{\|\Delta\|=1}\mathbb{E}g(X,\Delta).
\end{eqnarray*}
Take supremum over both sides, arrange the inequality, and we obtain the bound
\begin{eqnarray}
\label{eq:union-tala-g-relu} \sup_{\|\Delta\|=1}|g(X,\Delta) - \mathbb{E}g(X,\Delta)| &\leq& (1-\zeta)^{-1}\max_{\Delta\in\mathcal{N}_{\zeta}}|g(X,\Delta) - \mathbb{E}g(X,\Delta)| \\
\label{eq:small-exp-relu} && 2\zeta(1-\zeta)^{-1}\mathbb{E}g(X,\Delta).
\end{eqnarray}
To bound (\ref{eq:union-tala-g-relu}), we will use Lemma \ref{lem:talagrand} together with a union bound argument. For any $X,\wt{X}$, we have
\begin{eqnarray*}
|g(X,\Delta)-g(\wt{X},\Delta)| &\leq& \mathbb{E}^X\left|\sum_{i=1}^n(\psi(W_j^Tx_i)-\psi(W_j^T\wt{x}_j))\Delta_i\right| \\
&\leq& \mathbb{E}^X\sqrt{\sum_{i=1}^n\left(\psi(W_j^Tx_i)-\psi(W_j^T\wt{x}_j)\right)^2} \\
&\leq& \sqrt{\sum_{i=1}^n\mathbb{E}^X\left(W_j^T(x_i-\wt{x}_i)\right)^2} \\
&=& \frac{1}{\sqrt{d}}\sqrt{\sum_{i=1}^n\|x_i-\wt{x}_i\|^2}.
\end{eqnarray*}
Therefore, by Lemma \ref{lem:talagrand},
$$\mathbb{P}\left(|g(X,\Delta)-g(\wt{X},\Delta)|>t\right) \leq 2\exp\left(-\frac{dt^2}{2}\right),$$
for any $t>0$. A union bound argument leads to
$$\mathbb{P}\left(\max_{\Delta\in\mathcal{N}_{\zeta}}|g(X,\Delta) - \mathbb{E}g(X,\Delta)|>t\right)\leq 2\exp\left(-\frac{dt^2}{2}+n\log\left(1+\frac{2}{\zeta}\right)\right),$$
which implies that
$$\max_{\Delta\in\mathcal{N}_{\zeta}}|g(X,\Delta) - \mathbb{E}g(X,\Delta)|\lesssim \sqrt{\frac{n\log(1+2/\zeta)}{d}},$$
with high probability. For (\ref{eq:small-exp-relu}), we have
$$\mathbb{E}g(X,\Delta)\leq \mathbb{E}\sqrt{\sum_{i=1}^n|\psi(W^Tx_i)|^2}\leq \sqrt{\sum_{i=1}^n\mathbb{E}|\psi(W^Tx_i)|^2}\leq \sqrt{n}.$$
Combining the bounds for (\ref{eq:union-tala-g-relu}) and (\ref{eq:small-exp-relu}), we have
$$\sup_{\|\Delta\|=1}|g(X,\Delta) - \mathbb{E}g(X,\Delta)|\lesssim \sqrt{\frac{n\log(1+2/\zeta)}{d}} + \zeta\sqrt{n},$$
with high probability as long as $\zeta\leq 1/2$. We choose $\zeta=\frac{c}{\sqrt{n}}$ with a sufficiently small constant $c>0$, and thus the bound is sufficiently small as long as $\frac{n\log n}{d}$ is sufficiently small.

Finally, combine results for (\ref{eq:exp-f-inf-relu}), (\ref{eq:ep-f-relu}) and (\ref{eq:ep-g-relu}), and we obtain the desired conclusion as long as $n^2/p$ and $n\log n/d$ are sufficiently small.
\end{proof}

To prove (\ref{eq:l2-upper-A}) of Lemma \ref{lem:design-rf-relu}, we establish the following stronger result.

\begin{lemma}\label{lem:lim-G-relu}
Consider independent $W_1,...,W_p\sim N(0,d^{-1}I_d)$ and $x_1,...,x_n\sim N(0,I_d)$. We define the matrices $G,\bar{G}\in\mathbb{R}^{n\times n}$ by
$$
G_{il}=\frac{1}{p}\sum_{j=1}^p\psi(W^T_jx_i)\psi(W_j^Tx_l),
$$
and
$$\bar{G}_{il}=\begin{cases}
\frac{1}{2}, & i=l, \\
\frac{1}{2\pi}+\frac{1}{4}\frac{\bar{x}_i^T\bar{x}_l}{d} + \frac{1}{2\pi}\left(\frac{\|x_i\|}{\sqrt{d}}-1+\frac{\|x_l\|}{\sqrt{d}}-1\right), & i\neq l.
\end{cases}$$
Assume $d/\log n$ is sufficiently large, and then
$$\opnorm{G-\bar{G}}^2\lesssim \frac{n^2}{p} + \frac{\log n}{d} + \frac{n^2}{d^2},$$
with high probability. Moreover, we also have $\opnorm{G}\lesssim n$ with high probability.
\end{lemma}
\begin{proof}
Define $\wt{G}\in\mathbb{R}^{n\times n}$ with entries $\wt{G}_{il}=\mathbb{E}\left(\psi(W^Tx_i)\psi(W^Tx_l)|X\right)$, and we first bound the difference between $G$ and $\wt{G}$. Note that
$$\mathbb{E}(G_{il}-\wt{G}_{il})^2 = \mathbb{E}\Var(G_{il}|X) \leq \frac{1}{p}\mathbb{E}|\psi(W^Tx_i)\psi(W^Tx_l)|^2=\frac{3}{2p}\mathbb{E}\|W\|^4\leq 5p^{-1}.$$
We then have
$$
\mathbb{E}\opnorm{G-\wt{G}}^2 \leq \mathbb{E}\fnorm{G-\wt{G}}^2 \leq \frac{5n^2}{p}.
$$
By Markov's inequality,
\begin{equation}
\opnorm{G-\wt{G}}^2 \lesssim \frac{n^2}{p}, \label{eq:G-G-tilde-relu}
\end{equation}
with high probability.

Next, we study the diagonal entries of $\wt{G}$. For any $i\in[n]$, $\wt{G}_{ii}=\mathbb{E}(|\psi(W^Tx_i)|^2|X)=\frac{\|x_i\|^2}{2d}$. By Lemma \ref{lem:chi-squared} and a union bound argument, we have
\begin{equation}
\max_{1\leq i\leq n}|\wt{G}_{ii}-\bar{G}_{ii}|\lesssim \sqrt{\frac{\log n}{d}}, \label{eq:G-diag-relu}
\end{equation}
with high probability.

Now we analyze the off-diagonal entries. We use the notation $\bar{x}_i=\frac{\sqrt{d}}{\|x_i\|}x_i$. For any $i\neq l$, we have
\begin{eqnarray}
\label{eq:G-tilde-1-relu} \wt{G}_{il} &=& \mathbb{E}\left(\psi(W^T\bar{x}_i)\psi(W^T\bar{x}_l)|X\right) \\
\label{eq:G-tilde-2-relu} && + \mathbb{E}\left((\psi(W^Tx_i)-\psi(W^T\bar{x}_i))\psi(W^T\bar{x}_l)|X\right) \\
\label{eq:G-tilde-3-relu} && + \mathbb{E}\left(\psi(W^T\bar{x}_i)(\psi(W^Tx_l)-\psi(W^T\bar{x}_l))|X\right) \\
\label{eq:G-tilde-4-relu} && + \mathbb{E}\left((\psi(W^Tx_i)-\psi(W^T\bar{x}_i))(\psi(W^Tx_l)-\psi(W^T\bar{x}_l))|X\right).
\end{eqnarray}
For the first term on the right hand side of (\ref{eq:G-tilde-1-relu}), we observe that $\mathbb{E}\left(\psi(W^T\bar{x}_i)\psi(W^T\bar{x}_l)|X\right)$ is a function of $\frac{\bar{x}_i^T\bar{x}_l}{d}$, and thus we can write
$$\mathbb{E}\left(\psi(W^T\bar{x}_i)\psi(W^T\bar{x}_l)|X\right)=f\left(\frac{\bar{x}_i^T\bar{x}_l}{d}\right),$$
where
$$f(\rho) = \begin{cases}
\mathbb{E}\psi(\sqrt{1-\rho}U+\sqrt{\rho}Z)\psi(\sqrt{1-\rho}V+\sqrt{\rho}Z), & \rho \geq 0, \\
\mathbb{E}\psi(\sqrt{1+\rho}U-\sqrt{-\rho}Z)\psi(\sqrt{1+\rho}V+\sqrt{-\rho}Z), & \rho < 0,
\end{cases}$$
with $U,V,Z\stackrel{iid}{\sim} N(0,1)$. By some direct calculations, we have $f(0)=\frac{1}{2\pi}$, $f'(0)=\frac{1}{4}$, and $\sup_{|\rho|\leq 0.2}\frac{|f'(\rho)-f'(0)|}{|\rho|}\lesssim 1$. Therefore, as long as $|\bar{x}_i^T\bar{x}_l|/d\leq 1/5$,
$$\left|f\left(\frac{\bar{x}_i^T\bar{x}_l}{d}\right)-\frac{1}{2\pi}-\frac{1}{4}\frac{\bar{x}_i^T\bar{x}_l}{d}\right|\leq C_1\left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|^2,$$
for some constant $C_1>0$. By Lemma \ref{lem:inner-prod}, we know that $\max_{i\neq l}|\bar{x}_i^T\bar{x}_l|/d\lesssim \sqrt{\frac{\log n}{d}}\leq 1/5$ with high probability, which then implies
\begin{equation}
\sum_{i\neq l}\left(\mathbb{E}\left(\psi(W^T\bar{x}_i)\psi(W^T\bar{x}_l)|X\right)-\bar{G}_{il}\right)^2 \leq C_1\sum_{i\neq l}\left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|^4.\label{eq:4th-bd-later}
\end{equation}
The term on the right hand side can be bounded by
$$\sum_{i\neq l}\left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|^4\leq \frac{d}{\min_{1\leq i\leq n}\|x_i\|^2}\sum_{i\neq l}\left|\frac{x_i^Tx_l}{d}\right|^4.$$
By Lemma \ref{lem:chi-squared}, $\frac{d}{\min_{1\leq i\leq n}\|x_i\|^2}\lesssim 1$ with high probability. By integrating out the probability tail bound of $|x_i^Tx_l|$ given in Lemma \ref{lem:inner-prod}, we have $\sum_{i\neq l}\mathbb{E}\left|\frac{x_i^Tx_l}{d}\right|^4\lesssim \frac{n^2}{d^2}$, and by Markov's inequality, we have $\sum_{i\neq l}\left|\frac{x_i^Tx_l}{d}\right|^4\lesssim \frac{n^2}{d^2}$ with high probability.

We also need to analyze the contributions of (\ref{eq:G-tilde-2-relu}) and (\ref{eq:G-tilde-3-relu}). Observe the fact that $\mathbb{I}\{W^Tx_i\geq 0\}=\mathbb{I}\{W^T\bar{x}_i\geq 0\}$, which implies
\begin{eqnarray}
\nonumber \psi(W^Tx_i)-\psi(W^T\bar{x}_i) &=& W^T(x_i-\bar{x}_i)\mathbb{I}\{W^T\bar{x}_i\geq 0\}\psi(W^T\bar{x}_l) \\
\label{eq:interesting-rep} &=& \left(\frac{\|x_i\|}{\sqrt{d}}-1\right)\psi(W^T\bar{x}_i)\psi(W^T\bar{x}_l).
\end{eqnarray}
Then, the sum of (\ref{eq:G-tilde-2-relu}) and (\ref{eq:G-tilde-3-relu}) can be written as
$$\left(\frac{\|x_i\|}{\sqrt{d}}-1+\frac{\|x_l\|}{\sqrt{d}}-1\right)f\left(\frac{\bar{x}_i^T\bar{x}_l}{d}\right).$$
Note that
\begin{eqnarray*}
&& \sum_{i\neq l} \left(\frac{\|x_i\|}{\sqrt{d}}-1+\frac{\|x_l\|}{\sqrt{d}}-1\right)^2\left[f\left(\frac{\bar{x}_i^T\bar{x}_l}{d}\right)-\frac{1}{2\pi}\right]^2 \\
&\lesssim& \sum_{i\neq l} \left(\frac{\|x_i\|}{\sqrt{d}}-1+\frac{\|x_l\|}{\sqrt{d}}-1\right)^4 + \sum_{i\neq l}\left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|^4.
\end{eqnarray*}
We have already shown that $\sum_{i\neq l}\left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|^4\lesssim \frac{n^2}{d^2}$ with high probability. By integrating out the probability tail bound of Lemma \ref{lem:chi-squared}, we have $\mathbb{E}\left(\frac{\|x_i\|}{\sqrt{d}}-1\right)^4\lesssim d^{-2}$, which then implies
$$\sum_{i\neq l} \mathbb{E}\left(\frac{\|x_i\|}{\sqrt{d}}-1+\frac{\|x_l\|}{\sqrt{d}}-1\right)^4\lesssim \frac{n^2}{d^2}$$
and the corresponding high-probability bound by Markov's inequality.

Finally, we show that the contribution of (\ref{eq:G-tilde-4-relu}) is negligible. By (\ref{eq:interesting-rep}), we can write (\ref{eq:G-tilde-4-relu}) as
$$\left(\frac{\|x_i\|}{\sqrt{d}}-1\right)\left(\frac{\|x_l\|}{\sqrt{d}}-1\right)\mathbb{E}\left(\psi(W^T\bar{x}_i)^2\psi(W^T\bar{x}_l)^2\Big|X\right),$$
whose absolute value can be bounded by $\frac{3}{2}\left|\frac{\|x_i\|}{\sqrt{d}}-1\right|\left|\frac{\|x_l\|}{\sqrt{d}}-1\right|$. Since
$$\sum_{i\neq l}\mathbb{E}\left(\frac{\|x_i\|}{\sqrt{d}}-1\right)^2\mathbb{E}\left(\frac{\|x_l\|}{\sqrt{d}}-1\right)^2\lesssim \frac{n^2}{d^2},$$
we can conclude that (\ref{eq:G-tilde-4-relu}) is bounded by $O\left(\frac{n^2}{d^2}\right)$ with high probability by Markov's inequality.

Combining the analyses of (\ref{eq:G-tilde-1-relu}), (\ref{eq:G-tilde-2-relu}), (\ref{eq:G-tilde-3-relu}) and (\ref{eq:G-tilde-4-relu}), we conclude that $\sum_{i\neq l}(\wt{G}_{il}-\bar{G}_{il})^2\lesssim \frac{n^2}{d^2}$ with high probability. Together with (\ref{eq:G-G-tilde-relu}) and (\ref{eq:G-diag-relu}), we obtain the desired bound for $\opnorm{G-\bar{G}}$.

To prove the last conclusion $\opnorm{\bar{G}}\lesssim n$, it suffices to analyze $\lambda_{\max}(\bar{G})$. We bound this quantity by $\mathbb{E}\lambda_{\max}(\bar{G})^2\leq \mathbb{E}\fnorm{\bar{G}}^2\lesssim n^2$, which leads to the desired conclusion.
\end{proof}


\begin{proof}[Proof of Corollary \ref{cor:repair-rf-relu}]
Since $\wh{\theta}$ belongs to the row space of $\wt{X}$, there exists some $u^*\in\mathbb{R}^n$ such that $\wh{\theta}=\wt{X}^Tu^*$.
By Theorem \ref{thm:main-improved} and Lemma \ref{lem:design-rf-relu}, we know that $\wt{u}=u^*$ with high probability, and therefore $\wt{\theta}=\wt{X}^T\wt{u}=\wt{X}^Tu^*=\wh{\theta}$.
\end{proof}



\subsection{Proof of Theorem \ref{thm:nn-grad-relu}}

To prove Theorem \ref{thm:nn-grad-relu}, we need the following kernel random matrix result.
\begin{lemma}\label{lem:lim-H-relu}
Consider independent $W_1,\ldots,W_p\sim N(0,d^{-1}I_d)$, $x_1,\ldots,x_n\sim N(0,I_d)$, and parameters $\beta_1,\ldots,\beta_p\sim N(0,1)$. We define the matrices $H, \bar{H}\in\mathbb{R}^{n\times n}$ by
\begin{eqnarray*}
H_{il} &=& \frac{x_i^Tx_l}{d}\frac{1}{p}\sum_{j=1}^p\beta_j^2\mathbb{I}\{W_j^Tx_i\geq 0, W_j^Tx_l\geq 0\}, \\
\bar{H}_{il} &=& \frac{1}{4}\frac{x_i^Tx_l}{\|x_i\|\|x_l\|} + \frac{1}{4}\mathbb{I}\{i=l\}.
\end{eqnarray*}
Assume $d/\log n$ is sufficiently large, and then
$$\opnorm{H-\bar{H}}^2 \lesssim \frac{n^2}{pd} + \frac{n}{p} + \frac{\log n}{d} + \frac{n^2}{d^2},$$
with high probability. If we additionally assume that $d/n$ and $p/n$ are sufficiently large, we will also have
$$\frac{1}{5}\leq\lambda_{\min}(H)\leq\lambda_{\max}(H)\lesssim 1,$$
with high probability.
%If we assume that $d/n$ and $p/n$ are sufficiently large, we will also have
%$$\lambda_{\min}(H)\geq \frac{1}{8},$$
%with high probability.
\end{lemma}
\begin{proof}
Define $\wt{H}\in\mathbb{R}^{n\times n}$ with entries $\wt{H}_{il}=\frac{x_i^Tx_l}{d}\mathbb{E}\left(\beta^2\mathbb{I}\{W^Tx_i\geq 0, W^Tx_l\geq 0\}\big|X\right)$, and we first bound the difference between $H$ and $\wt{H}$. Note that
$$\mathbb{E}(H_{il}-\wt{H}_{il})^2=\mathbb{E}\Var(H_{il}|X)\leq \frac{1}{p}\mathbb{E}\left(\frac{|x_i^Tx_l|^2}{d^2}\beta^4\right)\leq\begin{cases}
\frac{3}{pd}, & i\neq l, \\
9p^{-1}, & i=l.
\end{cases}$$
We then have
$$\mathbb{E}\opnorm{H-\wt{H}}^2 \leq \mathbb{E}\fnorm{H-\wt{H}}^2 \leq \frac{3n^2}{pd} + \frac{9n}{p}.$$
By Markov's inequality,
\begin{equation}
\opnorm{H-\wt{H}}^2 \lesssim \frac{n^2}{pd} + \frac{n}{p}, \label{eq:H-H-tilde-relu}
\end{equation}
with high probability.

Next, we study the diagonal entries of $\wt{H}$. For any $i\in[n]$, $\wt{H}_{ii}=\frac{\|x_i\|^2}{d}\mathbb{E}(\beta^2\mathbb{I}\{W^Tx_i\geq 0\}|X)=\frac{\|x_i\|^2}{2d}$. The same analysis that leads to the bound (\ref{eq:G-diag-relu}) also implies that
\begin{equation}
\max_{1\leq i\leq n}|\wt{H}_{ii}-\bar{H}_{ii}|\lesssim \sqrt{\frac{\log n}{d}}, \label{eq:H-diag-relu}
\end{equation}
with high probability.

Now we analyze the off-diagonal entries. Recall the notation $\bar{x}_i=\frac{\sqrt{d}}{\|x_i\|}x_i$. For any $i\neq l$, we have
\begin{eqnarray}
\nonumber \wt{H}_{il} &=& \frac{\|x_i\|\|x_l\|}{d}\frac{\bar{x}_i^T\bar{x}_l}{d}\mathbb{P}\left(W^T\bar{x}_i\geq 0, W^T\bar{x}_l\geq 0|X\right) \\
\label{eq:H-tilde-il-relu} &=& \frac{\bar{x}_i^T\bar{x}_l}{d}\mathbb{P}\left(W^T\bar{x}_i\geq 0, W^T\bar{x}_l\geq 0|X\right) \\
\nonumber && + \left(\frac{\|x_i\|\|x_l\|}{d}-1\right)\frac{\bar{x}_i^T\bar{x}_l}{d}\mathbb{P}\left(W^T\bar{x}_i\geq 0, W^T\bar{x}_l\geq 0|X\right).
\end{eqnarray}
Since $\mathbb{P}\left(W^T\bar{x}_i\geq 0, W^T\bar{x}_l\geq 0|X\right)$ is a function of $\frac{\bar{x}_i^T\bar{x}_l}{d}$, we can write
\begin{equation}
\frac{\bar{x}_i^T\bar{x}_l}{d}\mathbb{P}\left(W^T\bar{x}_i\geq 0, W^T\bar{x}_l\geq 0|X\right)=f\left(\frac{\bar{x}_i^T\bar{x}_l}{d}\right), \label{eq:f-H-tilde-relu}
\end{equation}
where for $\rho>0$,
\begin{eqnarray*}
f(\rho) &=& \rho\mathbb{P}\left(\sqrt{1-\rho}U+\sqrt{\rho}Z\geq 0, \sqrt{1-\rho}V+\sqrt{\rho}Z\geq0\right) \\
&=& \rho\mathbb{E}\mathbb{P}\left(\sqrt{1-\rho}U+\sqrt{\rho}Z\geq 0, \sqrt{1-\rho}V+\sqrt{\rho}Z\geq 0|Z\right) \\
&=& \rho\mathbb{E}\Phi\left(\sqrt{\frac{\rho}{1-\rho}}Z\right)^2,
\end{eqnarray*}
with $U,V,Z\stackrel{iid}{\sim} N(0,1)$ and $\Phi(\cdot)$ being the cumulative distribution function of $N(0,1)$. Similarly, for $\rho<0$,
$$f(\rho) = \rho\mathbb{E}\left[\Phi\left(\sqrt{\frac{-\rho}{1+\rho}}Z\right)\left(1-\Phi\left(\sqrt{\frac{-\rho}{1+\rho}}Z\right)\right)\right].$$
By some direct calculations, we have $f(0)=0$, $f'(0)=\frac{1}{4}$, and
$$\sup_{|\rho|\leq 1/5}|f''(\rho)|\lesssim \sup_{|t|\leq 1/2}\left|\mathbb{E}\phi(tZ)\Phi(tZ)Z/t\right| + \sup_{|t|\leq 1/2}\left|\mathbb{E}\phi(tZ)Z/t\right|,$$
where $\phi(x)=(2\pi)^{-1/2}e^{-x^2/2}$. For any $|t|\leq 1/2$,
$$
\left|\mathbb{E}\phi(tZ)Z/t\right| = \left|\mathbb{E}\frac{\phi(tZ)-\phi(0)}{tZ}Z^2\right| = \left|\mathbb{E}\xi\phi(\xi)Z^2\right| \leq \frac{|t|}{\sqrt{2\pi}}\mathbb{E}|Z|^3\lesssim 1,
$$
where $\xi$ is a scalar between $0$ and $tZ$ so that $|\xi|\leq |tZ|$. By a similar argument, we also have $\sup_{|t|\leq 1/2}\left|\mathbb{E}\phi(tZ)\Phi(tZ)Z/t\right|\lesssim 1$ so that $\sup_{|\rho|\leq 1/5}|f''(\rho)|\lesssim 1$. Therefore, as long as $|\bar{x}_i^T\bar{x}_l|/d\leq 1/5$,
$$\left|f\left(\frac{\bar{x}_i^T\bar{x}_l}{d}\right)-\frac{1}{4}\frac{\bar{x}_i^T\bar{x}_l}{d}\right|\leq C_1\left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|^2,$$
for some constant $C_1>0$. By Lemma \ref{lem:inner-prod}, we know that $\max_{i\neq l}|\bar{x}_i^T\bar{x}_l|/d\lesssim \sqrt{\frac{\log n}{d}}\leq 1/5$ with high probability. In view of the identities (\ref{eq:H-tilde-il-relu}) and (\ref{eq:f-H-tilde-relu}), we then have the high probability bound,
\begin{eqnarray}
\nonumber \sum_{i\neq l}\left(\wt{H}_{il}-\frac{1}{4}\frac{\bar{x}_i^T\bar{x}_l}{d}\right)^2 &\leq& 2\sum_{i\neq l}\left(\frac{\|x_i\|\|x_l\|}{d}-1\right)^2\left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|^2  + 2C_1\sum_{i\neq l}\left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|^4 \\
 \label{eq:high-prob-off-diag} &\leq& \sum_{i\neq l}\left(\frac{\|x_i\|\|x_l\|}{d}-1\right)^4 + (2C_1+1)\sum_{i\neq l}\left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|^4.
\end{eqnarray}
For the first term on the right hand side of (\ref{eq:high-prob-off-diag}), we use Lemma \ref{lem:inner-prod} and obtain a probability tail bound for $|\|x_i\|\|x_l\|-d|$. By integrating out this tail bound, we have
$$\sum_{i\neq l}\mathbb{E}\left(\frac{\|x_i\|\|x_l\|}{d}-1\right)^4\lesssim \frac{n^2}{d^2},$$
which, by Markov's inequality, implies $\sum_{i\neq l}\left(\frac{\|x_i\|\|x_l\|}{d}-1\right)^4\lesssim \frac{n^2}{d^2}$ with high probability.
Using the same argument in the proof of Lemma \ref{lem:lim-G-relu}, we have $\sum_{i\neq l}\left|\frac{x_i^Tx_l}{d}\right|^4\lesssim \frac{n^2}{d^2}$ with high probability.
Finally, combining (\ref{eq:H-H-tilde-relu}), (\ref{eq:H-diag-relu}), and the bound for (\ref{eq:high-prob-off-diag}), we obtain the desired bound for $\opnorm{H-\bar{H}}$.
%The lower bound for $\lambda_{\min}(H)$ is a direct application of Weyl's inequality.
For the last conclusion, since $\opnorm{H-\bar{H}}$ is sufficiently small, it is sufficient to show $\frac{1}{4}\leq\lambda_{\min}(\bar{H})\leq\lambda_{\max}(\bar{H})\lesssim 1$. The bound $\frac{1}{4}\leq\lambda_{\min}(\bar{H})$ is a direct consequence of the definition of $\bar{H}$. To upper bound $\lambda_{\max}(\bar{H})$, we have
\begin{eqnarray*}
\lambda_{\max}(\bar{H}) &\leq& \frac{1}{4} + \frac{1}{4}\max_{\|v\|=1}\sum_{i=1}^n\sum_{l=1}^nv_iv_l\frac{x_i^Tx_l}{\|x_i\|\|x_l\|} \\
&\lesssim& 1 + \max_{\|v\|=1}\sum_{i=1}^n\sum_{l=1}^nv_iv_l\frac{x_i^Tx_l}{d} \\
&=& 1 + \opnorm{X}^2/d \\
&\lesssim& 1 + \frac{n}{d},
\end{eqnarray*}
with high probability, where the last inequality is by \cite{davidson2001local}. The proof is complete.
\end{proof}


Now we are ready to prove Theorem \ref{thm:nn-grad-relu}.
\begin{proof}[Proof of Theorem \ref{thm:nn-grad-relu}]
We first establish some high probability events:
\begin{eqnarray}
\label{eq:r1e1} \max_{1\leq j\leq p}|\beta_j(0)| &\leq& 2\sqrt{\log p}, \\
\label{eq:r1e2} \max_{k\in\{1,2,3\}}\frac{1}{p}\sum_{j=1}^p|\beta_j(0)|^k &\lesssim& 1, \\
\label{eq:r1e3} \sum_{i=1}^n\|x_i\|^2 &\leq& 2nd, \\
\label{eq:r1e4} \max_{1\leq i\leq n}\|x_i\| &\lesssim& \sqrt{d}, \\
\label{eq:r1e5} \max_{1\leq i\neq l\leq n}\left|\frac{x_i^Tx_l}{d}\right| &\lesssim& d^{-1/2}, \\
\label{eq:r1e6}\max_{1\leq l\leq n}\sum_{i=1}^n\left|\frac{x_i^Tx_l}{d}\right| &\lesssim& 1+\frac{n}{\sqrt{d}}, \\
 \label{eq:r1e7}\|u(0)\| &\leq& \sqrt{n}(\log p)^{1/4}, \\
 \label{eq:r1e8}\max_{1\leq j\leq p}\sum_{i=1}^n|W_j(0)^Tx_i|^2 &\leq& 6n+18\log p, \\
 \label{eq:r1e9}\max_{1\leq i\leq n}\frac{1}{p}\sum_{j=1}^p\mathbb{I}\{|W_j(0)^Tx_i|\leq R_1\|x_i\|\} &\lesssim& \sqrt{d}R_1 + \sqrt{\frac{\log n}{p}}.
\end{eqnarray}
The bound (\ref{eq:r1e1}) is a consequence of a standard Gaussian tail inequality and a union bound argument. The second bound (\ref{eq:r1e2}) is by Markov's inequality and the fact that $\frac{1}{p}\sum_{j=1}^p\mathbb{E}|\beta_j(0)|^k\lesssim 1$. Then, we have (\ref{eq:r1e3}), (\ref{eq:r1e4}) and (\ref{eq:r1e8}) derived from Lemma \ref{lem:chi-squared} and a union bound. Similarly, (\ref{eq:r1e5}) is by Lemma \ref{lem:inner-prod} and a union bound. The bound (\ref{eq:r1e6}) is a direct consequence of (\ref{eq:r1e4}) and (\ref{eq:r1e5}). To obtain (\ref{eq:r1e7}), we note that $\mathbb{E}|u_i(0)|^2 = \mathbb{E}\Var(u_i(0)|X) \lesssim 1$, which then implies (\ref{eq:r1e7}) by Markov's inequality. Finally, for (\ref{eq:r1e9}), we have
\begin{eqnarray*}
&&\max_{1\leq i\leq n}\frac{1}{p}\sum_{j=1}^p\mathbb{I}\{|W_j(0)^Tx_i|\leq R_1\|x_i\|\} \leq \mathbb{P}\left(|N(0,1)|\leq \sqrt{d}R_1\right) \\
&&+ \max_{1\leq i\leq n}\frac{1}{p}\sum_{j=1}^p\left(\mathbb{I}\{|W_j(0)^Tx_i|\leq R_1\|x_i\|\}-\mathbb{P}\left(|N(0,1)|\leq \sqrt{d}R_1\right)\right),
\end{eqnarray*}
where the first term $\mathbb{P}\left(|N(0,1)|\leq \sqrt{d}R_1\right)$ can be bounded by $O(\sqrt{d}R_1)$, and the second term can be bounded by $\sqrt{\frac{\log n}{p}}$ according to Lemma \ref{lem:hoeffding} and a union bound.

Now we are ready to prove the main result. We introduce the function
$$v_i(t)=\frac{1}{\sqrt{p}}\sum_{j=1}^p\beta_j(t)\psi(W_j(t-1)^Tx_i).$$
Besides (\ref{eq:iter-parameter-relu}), (\ref{eq:iter-parameter-beta-relu}) and (\ref{eq:iter-function-relu}), we will also establish
\begin{equation}
\|y-v(t)\|^2 \leq \left(1-\frac{\gamma}{8}\right)^t\|y-v(0)\|^2. \label{eq:v-seq}
\end{equation}
It suffices to show the following to claims are true.
\begin{thm1}
With high probability, for any integer $k\geq 1$, as long as (\ref{eq:v-seq}), (\ref{eq:iter-function-relu}), (\ref{eq:iter-parameter-relu}) and (\ref{eq:iter-parameter-beta-relu}) hold for all $t\leq k$, then (\ref{eq:iter-parameter-beta-relu}) holds for $t=k+1$.
\end{thm1}
\begin{thm2}
With high probability, for any integer $k\geq 1$, as long as (\ref{eq:v-seq}), (\ref{eq:iter-function-relu}) and (\ref{eq:iter-parameter-relu}) hold for all $t\leq k$, and (\ref{eq:iter-parameter-beta-relu}) holds for all $t\leq k+1$, then (\ref{eq:v-seq}) holds for $t=k+1$.
\end{thm2}
\begin{thm3}
With high probability, for any integer $k\geq 1$, as long as (\ref{eq:iter-function-relu}) and (\ref{eq:iter-parameter-relu}) hold for all $t\leq k$, and (\ref{eq:iter-parameter-beta-relu}) and (\ref{eq:v-seq}) hold for all $t\leq k+1$, then (\ref{eq:iter-parameter-relu}) holds for $t=k+1$.
\end{thm3}
\begin{thm4}
With high probability, for any integer $k\geq 1$, as long as (\ref{eq:iter-function-relu}) holds for all $t\leq k$, and (\ref{eq:v-seq}), (\ref{eq:iter-parameter-relu}) and (\ref{eq:iter-parameter-beta-relu}) hold for all $t\leq k+1$, then (\ref{eq:iter-function-relu}) holds for $t=k+1$.
\end{thm4}
\noindent With all the claims above being true, we can then deduce (\ref{eq:iter-parameter-relu}), (\ref{eq:iter-parameter-beta-relu}), (\ref{eq:iter-function-relu}) and (\ref{eq:v-seq}) for all $t\geq 1$ by mathematical induction.

\paragraph{Proof of Claim A.}
By triangle inequality and the gradient formula,
\begin{eqnarray*}
|\beta_j(k+1)-\beta_j(0)| &\leq& \sum_{t=0}^k|\beta_j(t+1)-\beta_j(t)| \\
&\leq& \frac{\gamma}{\sqrt{p}}\sum_{t=0}^k\left|\sum_{i=1}^n(u_i(t)-y_i)\psi(W_j(t)^Tx_i)\right| \\
&\leq& \frac{\gamma}{\sqrt{p}}\sum_{t=0}^k\sum_{i=1}^n|y_i-u_i(t)||W_j(t)^Tx_i| \\
&\leq& \frac{\gamma}{\sqrt{p}}\sum_{t=0}^k\|y-u(t)\|\sqrt{\sum_{i=1}^n|W_j(t)^Tx_i|^2} \\
&\leq& \frac{\gamma}{\sqrt{p}}\sum_{t=0}^k\|y-u(t)\|\left(R_1\sqrt{\sum_{i=1}^n\|x_i\|^2}+\sqrt{\sum_{i=1}^n|W_j(0)^Tx_i|^2}\right) \\
&\leq& \gamma\sqrt{\frac{7n+18\log p}{p}}\sum_{t=0}^k\|y-u(t)\| \\
&\leq& 16\sqrt{\frac{7n+18\log p}{p}}\|y-u(0)\| \\
&\leq& 32\sqrt{\frac{n^2\log p}{p}} = R_2,
\end{eqnarray*}
where we have used (\ref{eq:r1e4}), (\ref{eq:r1e7}) and (\ref{eq:r1e8}).
Hence, (\ref{eq:iter-parameter-beta-relu}) holds for $t=k+1$, and Claim A is true.

\paragraph{Proof of Claim B.} We omit this step, because the analysis uses the same argument as that of the proof of Claim D.

\paragraph{Proof of Claim C.} We bound $\|W_j(k+1)-W_j(0)\|$ by $\sum_{t=0}^k\|W_j(t+1)-W_j(t)\|$. Then by the gradient descent formula, we have
\begin{eqnarray*}
\|W_j(k+1)-W_j(0)\| &\leq& \frac{\gamma}{d\sqrt{p}}\sum_{t=0}^k\left\|\beta_j(t+1)\sum_{i=1}^n(v_i(t+1)-y_i)\psi'(W_j(t)^Tx_i)x_i\right\| \\
&\leq& \frac{\gamma}{d\sqrt{p}}\sum_{t=0}^k|\beta_j(t+1)|\sum_{i=1}^n|y_i-v_i(t+1)|\|x_i\| \\
&\leq& \frac{\gamma}{d\sqrt{p}}(|\beta_j(0)|+R_2)\sqrt{\sum_{i=1}^n\|x_i\|^2}\sum_{t=0}^k\|y-v(t+1)\| \\
&\leq& \frac{16}{d\sqrt{p}}(|\beta_j(0)|+R_2)\sqrt{\sum_{i=1}^n\|x_i\|^2}\|y-v(0)\| \\
&\leq& \frac{100n\log p}{\sqrt{pd}} = R_1,
\end{eqnarray*}
where we have used (\ref{eq:r1e1}), (\ref{eq:r1e3}) and (\ref{eq:r1e7}) in the above inequalities. Thus, Claim C is true.

\paragraph{Proof of Claim D.} We first analyze $u(k+1)-u(k)$. For each $i\in[n]$, we have
\begin{eqnarray*}
&& u_i(k+1) - u_i(k) \\
&=& \frac{1}{\sqrt{p}}\sum_{j=1}^p\beta_j(k+1)\left(\psi(W_j(k+1)^Tx_i)-\psi(W_j(k)^Tx_i)\right) \\
&& + \frac{1}{\sqrt{p}}\sum_{j=1}^p(\beta_j(k+1)-\beta_j(k))\psi(W_j(k)^Tx_i) \\
&=& \frac{1}{\sqrt{p}}\sum_{j=1}^p\beta_j(k+1)(W_j(k+1)-W_j(k))^Tx_i\psi'(W_j(k)^Tx_i) \\
&& + \frac{1}{\sqrt{p}}\sum_{j=1}^p(\beta_j(k+1)-\beta_j(k))\psi(W_j(k)^Tx_i) + r_i(k) \\
&=& \gamma\sum_{l=1}^n(H_{il}(k)+G_{il}(k))(y_l-u_l(k)) + r_i(k),
\end{eqnarray*}
where
\begin{eqnarray*}
G_{il}(k) &=& \frac{1}{p}\sum_{j=1}^p\psi(W_j(k)^Tx_l)\psi(W_j(k)^Tx_i), \\
H_{il}(k) &=& \frac{x_i^Tx_l}{d}\frac{1}{p}\sum_{j=1}^p\beta_j(k+1)^2\psi'(W_j(k)^Tx_i)\psi'(W_j(k)^Tx_l),
\end{eqnarray*}
and
\begin{eqnarray*}
r_i(k) &=& \frac{1}{\sqrt{p}}\sum_{j=1}^p\beta_j(k+1)\left(\psi(W_j(k+1)^Tx_i)-\psi(W_j(k)^Tx_i)\right) \\
&& - \frac{1}{\sqrt{p}}\sum_{j=1}^p\beta_j(k+1)(W_j(k+1)-W_j(k))^Tx_i\psi'(W_j(k)^Tx_i).
\end{eqnarray*}
The above iteration formula can be summarized in a vector form as
\begin{equation}
u(k+1)-u(k)=\gamma(H(k)+G(k))(y-u(k))+r(k). \label{eq:iter-u-relu}
\end{equation}
We need to understand the eigenvalues of $G(k)$ and $H(k)$, and bound the absolute value of $r_i(k)$.

To analyze $G(k)$, we first control the difference between $G(k)$ and $G(0)$. Since
\begin{eqnarray*}
|G_{il}(k) - G_{il}(0)| &\leq& \frac{1}{p}\sum_{j=1}^p|\psi(W_j(k)^Tx_l) - \psi(W_j(0)^Tx_l)| \\
&& + \frac{1}{p}\sum_{j=1}^p|\psi(W_j(k)^Tx_i) - \psi(W_j(0)^Tx_i)| \\
&\leq& \frac{1}{p}\sum_{j=1}^p|(W_j(k)-W_j(0))^Tx_l| + \frac{1}{p}\sum_{j=1}^p|(W_j(k)-W_j(0))^Tx_i| \\
&\leq& R_1\left(\|x_l\| + \|x_i\|\right),
\end{eqnarray*}
then, by (\ref{eq:r1e4}),
\begin{equation}
\opnorm{G(k)-G(0)}\leq\max_{1\leq l\leq n}\sum_{i=1}^n|G_{il}(k) - G_{il}(0)|\leq 2R_1n\max_{1\leq i\leq n}\|x_i\|\lesssim \frac{n^2\log p}{\sqrt{p}}. \label{eq:x-japan}
\end{equation}
By Lemma \ref{lem:lim-G-relu} and the fact that $G(k)$ is positive semi-definite, we have
\begin{equation}
0 \leq \lambda_{\min}(G(k)) \leq \lambda_{\max}(G(k)) \lesssim n. \label{eq:Gk-spec}
\end{equation}

We also need to control the difference between $H(k)$ and $H(0)$. By the definition, we have
\begin{eqnarray}
\label{eq:r-H-d-1-relu} |H_{il}(k)-H_{il}(0)| &\leq& \left|\frac{x_i^Tx_l}{d}\right|\frac{1}{p}\sum_{j=1}^p|\beta_j(k+1)^2-\beta_j^2(0)| \\
\label{eq:r-H-d-2-relu} && + \left|\frac{x_i^Tx_l}{d}\right|\frac{1}{p}\sum_{j=1}^p\beta_j^2(0)|\psi'(W_j(k)^Tx_i) - \psi'(W_j(0)^Tx_i)| \\
\label{eq:r-H-d-3-relu} && + \left|\frac{x_i^Tx_l}{d}\right|\frac{1}{p}\sum_{j=1}^p\beta_j^2(0)|\psi'(W_j(k)^Tx_l) - \psi'(W_j(0)^Tx_l)|.
\end{eqnarray}
We can bound (\ref{eq:r-H-d-1-relu}) by $\left|\frac{x_i^Tx_l}{d}\right|\frac{1}{p}\sum_{j=1}^pR_2(R_2+2|\beta_j(0)|)$. To bound (\ref{eq:r-H-d-2-relu}), we note that
\begin{eqnarray}
\nonumber |\psi'(W_j(k)^Tx_i) - \psi'(W_j(0)^Tx_i)| &\leq& \mathbb{I}\{|W_j(0)^Tx_i|\leq |(W_j(k)-W_j(0))^Tx_i|\} \\
\label{eq:simon-bound} &\leq& \mathbb{I}\{|W_j(0)^Tx_i|\leq R_1\|x_i\|\},
\end{eqnarray}
which implies
\begin{eqnarray*}
&& \left|\frac{x_i^Tx_l}{d}\right|\frac{1}{p}\sum_{j=1}^p\beta_j^2(0)|\psi'(W_j(k)^Tx_i) - \psi'(W_j(0)^Tx_i)| \\
&\leq& \left|\frac{x_i^Tx_l}{d}\right|\frac{1}{p}\sum_{j=1}^p\beta_j^2(0)\mathbb{I}\{|W_j(0)^Tx_i|\leq R_1\|x_i\|\},
\end{eqnarray*}
and a similar bound holds for (\ref{eq:r-H-d-3-relu}). Then,
\begin{eqnarray*}
\opnorm{H(k)-H(0)} &\leq& \max_{1\leq i\leq n}|H_{ii}(k)-H_{ii}(0)| + \max_{1\leq l\leq n}\sum_{i\in[n]\backslash\{l\}}|H_{il}(k)-H_{il}(0)| \\
&\lesssim& \max_{1\leq i\leq n} \frac{1}{p}\sum_{j=1}^p\beta_j^2(0)\mathbb{I}\{|W_j^T(0)^Tx_i|\leq R_1\|x_i\|\} \\
&& + d^{-1/2}n\max_{1\leq i\leq n}\frac{1}{p}\sum_{j=1}^p\beta_j^2(0)\mathbb{I}\{|W_j^T(0)^Tx_i|\leq R_1\|x_i\|\} \\
&& + \max_{1\leq l\leq n}\sum_{i=1}^n\left|\frac{x_i^Tx_l}{d}\right|R_2\frac{1}{p}\sum_{j=1}^p(R_2+2|\beta_j(0)|) \\
&\lesssim& \left(1+\frac{n}{\sqrt{d}}\right)\left(\sqrt{d}R_1\log p+\frac{\sqrt{\log n}\log p}{\sqrt{p}} + R_2^2 + R_2\sqrt{\log p}\right) \\
&\lesssim& \left(1+\frac{n}{\sqrt{d}}\right)\frac{n(\log p)^2}{\sqrt{p}},
\end{eqnarray*}
where we have used (\ref{eq:r1e1}), (\ref{eq:r1e4}), (\ref{eq:r1e5}), (\ref{eq:r1e6}) and (\ref{eq:r1e9}). In view of Lemma \ref{lem:lim-H-relu}, we then have
\begin{equation}
\frac{1}{6} \leq \lambda_{\min}(H(k)) \leq \lambda_{\max}(H(k)) \lesssim 1, \label{eq:Hk-spec}
\end{equation}
under the conditions of $d,p$ and $n$.

Next, we give a bound for $r_i(k)$. Observe that
$$\psi(W_j(k+1)^Tx_i)-\psi(W_j(k)^Tx_i)=(W_j(k+1)-W_j(k))^Tx_i\psi'(W_j(k)^Tx_i),$$
when $\mathbb{I}\{W_j(k+1)^Tx_i>0\}=\mathbb{I}\{W_j(k)^Tx_i>0\}$. Thus, we only need to sum over those $j\in[p]$ that $\mathbb{I}\{W_j(k+1)^Tx_i>0\}\neq \mathbb{I}\{W_j(k)^Tx_i>0\}$. By (\ref{eq:simon-bound}), we have
\begin{eqnarray*}
&& \left|\mathbb{I}\{W_j(k+1)^Tx_i>0\}-\mathbb{I}\{W_j(k)^Tx_i>0\}\right| \\
&\leq& \left|\mathbb{I}\{W_j(k+1)^Tx_i>0\}-\mathbb{I}\{W_j(0)^Tx_i>0\}\right|+ \left|\mathbb{I}\{W_j(k)^Tx_i>0\}-\mathbb{I}\{W_j(0)^Tx_i>0\}\right| \\
&\leq& 2\mathbb{I}\{|W_j^T(0)^Tx_i|\leq R_1\|x_i\|\}.
\end{eqnarray*}
Therefore,
\begin{eqnarray*}
&& \left|\psi(W_j(k+1)^Tx_i)-\psi(W_j(k)^Tx_i)-(W_j(k+1)-W_j(k))^Tx_i\psi'(W_j(k)^Tx_i)\right| \\
&\leq& 4|(W_j(k+1)-W_j(k))^Tx_i|\mathbb{I}\{|W_j^T(0)^Tx_i|\leq R_1\|x_i\|\} \\
&\leq& \frac{4\gamma}{d\sqrt{p}}|\beta_j(k+1)|\|y-u(k)\|\|x_i\|\sqrt{\sum_{l=1}^n\|x_l\|^2}\mathbb{I}\{|W_j^T(0)^Tx_i|\leq R_1\|x_i\|\},
\end{eqnarray*}
which implies
\begin{eqnarray*}
|r_i(k)| &\leq& \frac{4\gamma}{dp}\sum_{j=1}^p|\beta_j(k+1)|^2\|y-u(k)\|\|x_i\|\sqrt{\sum_{l=1}^n\|x_l\|^2}\mathbb{I}\{|W_j^T(0)^Tx_i|\leq R_1\|x_i\|\} \\
&\lesssim& \sqrt{n}\|y-u(k)\|\gamma\frac{1}{p}\sum_{j=1}^p(\beta_j(0)^2+R_2^2)\mathbb{I}\{|W_j^T(0)^Tx_i|\leq R_1\|x_i\|\} \\
&\lesssim& \gamma\sqrt{n}\log p\left(R_1+\sqrt{\frac{\log n}{p}}\right)\|y-u(k)\|.
\end{eqnarray*}
This leads to the bound
\begin{equation}
\|r(k)\|=\sqrt{\sum_{i=1}^n|r_i(k)|^2}\lesssim \gamma n\log p\left(R_1+\sqrt{\frac{\log n}{p}}\right)\|y-u(k)\|.\label{eq:bound-res-k-relu}
\end{equation}

Now we are ready to analyze $\|y-u(k+1)\|^2$. Given the relation (\ref{eq:iter-u-relu}), we have
\begin{eqnarray*}
\|y-u(k+1)\|^2 &=& \|y-u(k)\|^2 - 2\iprod{y-u(k)}{u(k+1)-u(k)} + \|u(k)-u(k+1)\|^2 \\
&=& \|y-u(k)\|^2 - 2\gamma(y-u(k))^T(H(k)+G(k))(y-u(k)) \\
&& - 2\iprod{y-u(k)}{r(k)} + \|u(k)-u(k+1)\|^2.
\end{eqnarray*}
By (\ref{eq:Gk-spec}) and (\ref{eq:Hk-spec}), we have
\begin{equation}
- 2\gamma(y-u(k))^T(H(k)+G(k))(y-u(k)) \leq -\frac{\gamma}{6}\|y-u(k)\|^2. \label{eq:main-inner-relu}
\end{equation}
The bound (\ref{eq:bound-res-k-relu}) implies
$$- 2\iprod{y-u(k)}{r(k)}\leq 2\|y-u(k)\|\|r(k)\|\lesssim \gamma n\log p\left(R_1+\sqrt{\frac{\log n}{p}}\right)\|y-u(k)\|^2.$$
By (\ref{eq:Gk-spec}), (\ref{eq:Hk-spec}) and (\ref{eq:bound-res-k-relu}), we also have
\begin{eqnarray*}
\|u(k)-u(k+1)\|^2 &\leq& 2\gamma^2\|(H(k)+G(k))(y-u(k))\|^2 + 2\|r(k)\|^2 \\
&\lesssim& \gamma^2n\|y-u(k)\|^2 + (\gamma n\log p)^2\left(R_1+\sqrt{\frac{\log n}{p}}\right)^2\|y-u(k)\|^2 .
\end{eqnarray*}
Therefore, as long as $\frac{n\log n}{d}$, $\frac{n^3(\log p)^4}{p}$ and $\gamma n$ are all sufficiently small, we have
$$- 2\iprod{y-u(k)}{r(k)} + \|u(k)-u(k+1)\|^2 \leq \frac{\gamma}{24}\|y-u(k)\|^2.$$
Together with the bound (\ref{eq:main-inner-relu}), we have
$$\|y-u(k+1)\|^2 \leq \left(1-\frac{\gamma}{8}\right)\|y-u(k)\|^2\leq \left(1-\frac{\gamma}{8}\right)^{k+1}\|y-u(0)\|^2,$$
and thus Claim D is true. The proof is complete.
\end{proof}


\subsection{Proofs of Theorem \ref{thm:repair-nn-1-relu} and Theorem \ref{thm:repair-nn-2-relu}}

To prove Theorem \ref{thm:repair-nn-1-relu}, we need to extend Theorem \ref{thm:main-improved}. Consider $\eta=b+Au^*+z\in\mathbb{R}^m$, where the noise vector $z$ satisfies (\ref{eq:noise-add-con}), and $b\in\mathbb{R}^m$ is an arbitrary bias vector. Then, the estimator $\wh{u}=\argmin_{u\in\mathbb{R}^k}\|\eta-Au\|_1$ satisfies the following theoretical guarantee.
\begin{thm}\label{thm:robust-reg-b}
Assume the design matrix $A$ satisfies Condition A and Condition B. Then, as long as $\frac{\overline{\lambda}\sqrt{\frac{k}{m}\log\left(\frac{em}{k}\right)}+\epsilon\sigma\sqrt{\frac{k}{m}}}{\underline{\lambda}(1-\epsilon)}$ is sufficiently small and $\frac{8\frac{1}{m}\sum_{i=1}^m|b_i|}{\underline{\lambda}(1-\epsilon)}<1$, we have
$$\|\wh{u}-u^*\|\leq \frac{4\frac{1}{m}\sum_{i=1}^m|b_i|}{\underline{\lambda}(1-\epsilon)},$$
with high probability.
\end{thm}
It is easy to see that Theorem \ref{thm:main-improved} is a special case when $b=0$.
\begin{proof}[Proof of Theorem \ref{thm:robust-reg-b}]
Recall the definitions of $L_m(u)$ and $L(u)$ in the proof of Theorem \ref{thm:main-improved}. Define
$$K_m(u)=\frac{1}{m}\sum_{i=1}^m\left(|b_i+a_i^T(u^*-u)+z_i|-|z_i|\right).$$
It is easy to see that
\begin{equation}
\sup_u|L_m(u)-K_m(u)|\leq \frac{1}{m}\sum_{i=1}^m|b_i|.\label{eq:K-L-b}
\end{equation}
Suppose $\|\wh{u}-u^*\|\geq t$, we must have $\inf_{\|u-u^*\|\geq t}K_m(u) \leq K_m(u^*)$.
By the convexity of $K_m(u)$, we can replace $\|u-u^*\|\geq t$ by $\|u-u^*\| = t$ and the inequality still holds. By (\ref{eq:K-L-b}), we have $K_m(u^*)\leq \frac{1}{m}\sum_{i=1}^m|b_i|$, and therefore $\inf_{\|u-u^*\|= t}K_m(u)\leq \frac{1}{m}\sum_{i=1}^m|b_i|$. Since
\begin{eqnarray*}
\inf_{\|u-u^*\|= t}K_m(u) &\geq& \inf_{\|u-u^*\|= t}L_m(u) - \frac{1}{m}\sum_{i=1}^m|b_i| \\
&\geq& \inf_{\|u-u^*\|=t}L(u) + \inf_{\|u-u^*\|= t}(L_m(u)-L(u)) - \frac{1}{m}\sum_{i=1}^m|b_i|,
\end{eqnarray*}
we then have
\begin{equation}
\inf_{\|u-u^*\|=t}L(u) \leq \sup_{\|u-u^*\|= t}|L_m(u)-L(u)| + 2\frac{1}{m}\sum_{i=1}^m|b_i|. %\label{eq:basic-L}
\end{equation}
With the lower bound for (\ref{eq:L-decomp}) and the upper bound (\ref{eq:upper-EP}), we obtain
$$\underline{\lambda}(1-\epsilon)t - \epsilon t\sigma\sqrt{\frac{k}{m}} - 2\frac{1}{m}\sum_{i=1}^m|b_i|\lesssim t\overline{\lambda}\sqrt{\frac{k}{m}\log\left(\frac{em}{k}\right)},$$
which is impossible with the choice $t=\frac{4\frac{1}{m}\sum_{i=1}^m|b_i|}{\underline{\lambda}(1-\epsilon)}$ when $\frac{\overline{\lambda}\sqrt{\frac{k}{m}\log\left(\frac{em}{k}\right)}+\epsilon\sigma\sqrt{\frac{k}{m}}}{\underline{\lambda}(1-\epsilon)}$ is sufficiently small. Thus, we obtain the desired conclusion.
\end{proof}

Now we are ready to prove Theorem \ref{thm:repair-nn-1-relu}.
\begin{proof}[Proof of Theorem \ref{thm:repair-nn-1-relu}]
We first analyze $\wh{v}_1,...,\wh{v}_p$. The idea is to apply the result of Theorem \ref{thm:main-improved} to each of the $p$ robust regression problems. Thus, it suffices to check if the conditions of Theorem \ref{thm:main-improved} hold for the $p$ regression problems simultaneously. Since the $p$ regression problems share the same Gaussian design matrix, Lemma \ref{lem:design-linear} implies that Conditions A and B hold for all the $p$ regression problems. Next, by scrutinizing the proof of Theorem \ref{thm:main-improved}, the randomness of the conclusion is from the noise vector $Z_j$ through the empirical process bound given by Lemma \ref{lem:EP}. With an additional union bound argument applied to (\ref{eq:double-ub}), Lemma \ref{lem:EP} can be extended to $Z_j$ for all $j\in[p]$ with an additional assumption that $\frac{\log p}{d}$ is sufficiently small. Then, by the same argument in the proof of Corollary \ref{cor:repair-linear}, we have $\wt{W}_j=\wh{W}_j$ for all $j\in[p]$ with high probability.

To analyze $\wh{u}$, we apply Theorem \ref{thm:robust-reg-b}. Note that
\begin{eqnarray*}
\eta_j - \beta_j(0) &=& \beta_j(t_{\max}) - \beta_j(0) +z_j \\
&=& \sum_{t=0}^{t_{\max}-1}\left(\beta_j(t+1)-\beta_j(t)\right) + z_j \\
&=& \frac{\gamma}{\sqrt{p}}\sum_{t=0}^{t_{\max}-1}\sum_{i=1}^n(y_i-u_i(t))\psi(W_j(t)^Tx_i) + z_j \\
&=& \frac{\gamma}{\sqrt{p}}\sum_{t=0}^{t_{\max}-1}\sum_{i=1}^n(y_i-u_i(t))(\psi(W_j(t)^Tx_i)-\psi(W_j(0)^Tx_i)) \\
&& + \frac{\gamma}{\sqrt{p}}\sum_{t=0}^{t_{\max}-1}\sum_{i=1}^n(y_i-u_i(t))\psi(W_j(0)^Tx_i) + z_j.
\end{eqnarray*}
Thus, in the framework of Theorem \ref{thm:robust-reg-b}, we can view $\eta-\beta(0)$ as the response, $\psi(X^TW(0)^T)$ as the design, $z$ as the noise, and $b_j=\frac{\gamma}{\sqrt{p}}\sum_{t=0}^{t_{\max}-1}\sum_{i=1}^n(y_i-u_i(t))(\psi(W_j(t)^Tx_i)-\psi(W_j(0)^Tx_i))$ as the $j$th entry of the bias vector. By Lemma \ref{lem:design-rf-relu}, we know that the design matrix $\psi(X^TW(0)^T)$ satisfies Condition A and Condition B. So it suffices to bound $\frac{1}{p}\sum_{j=1}^p|b_j|$. With the help of Theorem \ref{thm:nn-grad-relu}, we have
\begin{eqnarray*}
\frac{1}{p}\sum_{j=1}^p|b_j| &\leq& \frac{\gamma}{p^{3/2}}\sum_{j=1}^p\sum_{t=0}^{t_{\max}-1}\sum_{i=1}^n|y_i-u_i(t)||(W_j(t)-W_j(0))^Tx_i| \\
&\leq& \frac{R_1\gamma}{p^{1/2}}\sum_{t=0}^{t_{\max}-1}\sum_{i=1}^n|y_i-u_i(t)|\|x_i\| \\
&\leq& \frac{R_1\gamma}{p^{1/2}}\sum_{t=0}^{t_{\max}-1}\|y-u(t)\|\sqrt{\sum_{i=1}^n\|x_i\|^2} \\
&\lesssim& \frac{R_1}{p^{1/2}}\|y-u(0)\|\sqrt{\sum_{i=1}^n\|x_i\|^2} \\
&\lesssim& \frac{n^2\log p}{p},
\end{eqnarray*}
where the last inequality is by $\sum_{i=1}^n\|x_i\|^2\lesssim nd$ due to Lemma \ref{lem:chi-squared}, and $\|u(0)\|^2\lesssim n$ due to Markov's inequality and $\mathbb{E}|u_i(0)|^2 = \mathbb{E}\Var(u_i(0)|X) \leq 1$. By Theorem \ref{thm:robust-reg-b} and Lemma \ref{lem:lim-G-relu}, we have $\frac{1}{p}\|\wt{\beta}-\wh{\beta}\|^2 \lesssim \frac{n^3\log p}{p}$, which is the desired conclusion.
\end{proof}


\begin{proof}[Proof of Theorem \ref{thm:repair-nn-1-relu}]
The analysis of $\wh{v}_1,...,\wh{v}_p$ is the same as that in the proof of Theorem \ref{thm:repair-nn-1-relu}, and we have $\wt{W}_j=\wh{W}_j$ for all $j\in[p]$ with high probability.

To analyze $\wh{u}$, we apply Theorem \ref{thm:main-improved}. It suffices to check Condition A and Condition B for the design matrix $\psi(X^T\wt{W}^T)=\psi(X^T\wh{W}^T)$. Since
$$\sum_{i=1}^n\mathbb{E}\left(\frac{1}{p}\sum_{j=1}^pc_j\psi(\wh{W}_j^Tx_i)\right)^2\leq \sum_{i=1}^n\frac{1}{p}\sum_{j=1}^p\mathbb{E}\psi(\wh{W}^Tx_i)^2,$$
and $\mathbb{E}\psi(\wh{W}^Tx_i)^2\leq \mathbb{E}|\wh{W}_j^Tx_i|^2\lesssim 1 + R_1d\lesssim 1$, Condition A holds with $\sigma^2\asymp p$.
We also need to check Condition B. By Theorem \ref{thm:nn-grad-relu}, we have
\begin{eqnarray*}
&& \left|\frac{1}{p}\sum_{j=1}^p\left|\sum_{i=1}^n\psi(\wh{W}_j^Tx_i)\Delta_i\right| - \frac{1}{p}\sum_{j=1}^p\left|\sum_{i=1}^n\psi(W_j(0)^Tx_i)\Delta_i\right|\right| \\
&\leq& \frac{1}{p}\sum_{j=1}^p\sum_{i=1}^n|\wh{W}_j^Tx_i-W_j(0)^Tx_i||\Delta_i| \\
&\leq& R_1\sum_{i=1}^n\|x_i\||\Delta_i| \\
&\leq& R_1\sqrt{\sum_{i=1}^n\|x_i\|^2} \\
&\lesssim& \frac{n^{3/2}\log p}{\sqrt{p}},
\end{eqnarray*}
where $\sum_{i=1}^n\|x_i\|^2\lesssim nd$ is by Lemma \ref{lem:chi-squared}. By Lemma \ref{lem:design-rf-relu}, we can deduce that
$$\inf_{\|\Delta\|=1}\frac{1}{p}\sum_{j=1}^p\left|\sum_{i=1}^n\psi(\wh{W}_j^Tx_i)\Delta_i\right|\gtrsim 1,$$
as long as $\frac{n^{3/2}\log p}{\sqrt{p}}$ is sufficiently small. By (\ref{eq:Gk-spec}), we also have
$$\sup_{\|\Delta\|=1}\frac{1}{p}\sum_{j=1}^p\left|\sum_{i=1}^n\psi(\wh{W}_j^Tx_i)\Delta_i\right|^2\lesssim n.$$
Therefore, Condition B holds with $\overline{\lambda}^2\asymp n$ and $\underline{\lambda}\asymp 1$. Apply Theorem \ref{thm:main-improved}, and we have $\wt{\beta}=\wh{\beta}$ with high probability as desired.
\end{proof}
