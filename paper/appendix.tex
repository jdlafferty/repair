% !TEX root = ./repair.tex

\appendix

\section{Results with hyperbolic tangent activation}


\subsection{Repair of random feature model and neural nets} \label{app:results}

In this section, we present analogous results of Sections \ref{sec:linear} and \ref{sec:neural} with ReLU activation. First, consider the random feature model with design $\wt{X}=\psi(XW)=\{\psi(W_j^Tx_i)\}_{i\in[n],j\in[p]}$, where $\psi(t)=\max(0,t)$. Recall that $x_i\sim N(0,I_d)$ and $W_j\sim N(0,d^{-1}I_d)$ independently for all $i\in[n]$ and $j\in[p]$. The random matrix $\wt{X}$ has good properties, which is given by the following lemma.
\begin{lemma}\label{lem:design-rf-relu}
Assume $n/p^2$ and $n\log n/d$ are sufficiently small. Then, Condition A and Condition B hold for $A=\wt{X}^T$, $m=p$ and $k=n$ with some $\sigma^2\asymp p$, $\overline{\lambda}^2\asymp n$ and $\underline{\lambda}\asymp 1$.
\end{lemma}

Now consider a model $\wh{\theta}$ that lies in the row space of $\wt{X}$. For example, $\wh{\theta}$ can be computed from a gradient-based algorithm initialized at $0$. We observe a contaminated version $\eta=\wh{\theta}+z$.
We can then compute the procedure $\wt{u}=\argmin_{u\in\mathbb{R}^n}\|\eta-\wt{X}^Tu\|_1$ and use $\wt{\theta}=\wt{X}^T\wt{u}$ for model repair.

\begin{corollary}\label{cor:repair-rf-relu}
Assume $\epsilon\sqrt{n}$, $n/p^2$ and $n\log n/d$ are sufficiently small. We then have $\wt{\theta}=\wh{\theta}$ with high probability.
\end{corollary}

Next, we study the repair of neural network $f(x)=\frac{1}{\sqrt{p}}\sum_{j=1}^p\beta_j\psi(W_j^Tx)$ with ReLU activation $\psi(t)=\max(0,t)$. The gradient descent algorithm (Algorithm \ref{alg:GD}) enjoys the following property. Recall the notation that $u_i(t)=\frac{1}{\sqrt{p}}\sum_{j=1}^p\beta_j(t)\psi(W_j(t)^Tx_i)$. We assume $x_i$ is i.i.d. $N(0,I_d)$ and $|y_i|\leq 1$ for all $i\in[n]$.
\begin{thm}\label{thm:nn-grad-relu}
Assume $\frac{n\log n}{d}$, $\frac{n^3(\log p)^4}{p}$ and $\gamma n$ are all sufficiently small. Then we have
\begin{eqnarray}
\label{eq:iter-parameter-relu} \max_{1\leq j\leq p}\|W_j(t)-W_j(0)\| &\leq& R_1, \\
\label{eq:iter-parameter-beta-relu} \max_{1\leq j\leq p}|\beta_j(t)-\beta_j(0)| &\leq& R_2,
\end{eqnarray}
and
\begin{equation}
\|y-u(t)\|^2 \leq \left(1-\frac{\gamma}{8}\right)^t\|y-u(0)\|^2, \label{eq:iter-function-relu}
\end{equation}
for all $t\geq 1$ with high probability, where $R_1=\frac{100n\log p}{\sqrt{pd}}$ and $R_2=32\sqrt{\frac{n^2\log p}{p}}$.
\end{thm}

Consider the contaminated model $\eta=\wh{\beta}+z$ and $\Theta_j=\wh{W}_j+Z_j$, where each entry of $z$ and $Z_j$ is zero with probability $1-\epsilon$ and follows an arbitrary distribution with the complementary probability $\epsilon$. We apply Algorithm \ref{alg:MR} to repair the neural net model. We study two situations. In the first situation, $\wh{\beta}=\beta(t_{\max})$ and $\wh{W}=W(t_{\max})$ are the direct output of Algorithm \ref{alg:GD}.

\begin{thm}\label{thm:repair-nn-1-relu}
Under the conditions of Theorem \ref{thm:nn-grad-relu}, additionally assume that $\frac{\log p}{d}$ and $\epsilon\sqrt{n}$ are sufficiently small. We then have $\wt{W}=\wh{W}$ and $\frac{1}{p}\|\wt{\beta}-\wh{\beta}\|^2 \lesssim \frac{n^3\log p}{p}$ with high probability.
\end{thm}

In the second situation, we have $\wh{W}=W(t_{\max})$ and then $\wh{\beta}$ is obtained by carrying out gradient descent over $\beta$ using features $\wt{X}=\psi(X\wh{W})$. Since the gradient descent over $\beta$ is initialized at $0$, we shall replace the $\beta(0)$ by $0$ in Algorithm \ref{alg:MR} as well.

\begin{thm}\label{thm:repair-nn-2-relu}
Under the conditions of Theorem \ref{thm:nn-grad-relu}, additionally assume that $\frac{\log p}{d}$ and $\epsilon\sqrt{n}$ are sufficiently small. We then have $\wt{W}=\wh{W}$ and $\wt{\beta}=\wh{\beta}$ with high probability.
\end{thm}

\begin{remark}
When $\epsilon\sqrt{n}$ is sufficiently small, the conditions of Theorem \ref{thm:repair-nn-1-relu} and Theorem \ref{thm:repair-nn-2-relu} can be simplified as $p \gg n^3$ and $d\gg n$ by ignoring the logarithmic factors. The more stringent requirement on $\epsilon$ is due to the fact that the design matrix $\psi(XW)$ does not have approximate zero mean with the ReLU activation. This results in a large $\sigma^2$ in Condition A. In contrast, the hyperbolic tangent activation is an odd function, a property of symmetry that leads to Condition A with a constant $\sigma^2$.
\end{remark}


\subsection{Proofs of Lemma \ref{lem:design-rf-relu} and Corollary \ref{cor:repair-rf-relu}}

We first state the proof of Lemma \ref{lem:design-rf-relu}. 
The conclusion of Condition A is obvious by
$$\sum_{i=1}^n\mathbb{E}\left(\frac{1}{p}\sum_{j=1}^pc_j\psi(W_j^Tx_i)\right)^2\leq \sum_{i=1}^n\frac{1}{p}\sum_{j=1}^p\mathbb{E}|W_j^Tx_i|^2= n,$$
and Markov's inequality. To check Condition B, we prove (\ref{eq:l1-upper-A}) and (\ref{eq:l2-upper-A}) separately.
\begin{proof}[Proof of (\ref{eq:l1-upper-A}) of Lemma \ref{lem:design-rf-relu}]
We adopt a similar strategy to the proof of Lemma \ref{lem:design-rf}. Define
$$f(W,X,\Delta)=\frac{1}{p}\sum_{j=1}^p\left|\sum_{i=1}^n\psi(W_j^Tx_i)\Delta_i\right|,$$
and $g(X,\Delta)=\mathbb{E}(f(W,X,\Delta)|X)$.
We then have
\begin{eqnarray}
\nonumber \inf_{\|\Delta\|=1}f(W,X,\Delta) &\geq& \inf_{\|\Delta\|=1}\mathbb{E}f(W,X,\Delta) - \sup_{\|\Delta\|=1}\left|f(W,X,\Delta)-\mathbb{E}f(W,X,\Delta)\right| \\
\label{eq:exp-f-inf} &\geq& \inf_{\|\Delta\|=1}\mathbb{E}f(W,X,\Delta) \\
\label{eq:ep-f} && - \sup_{\|\Delta\|=1}\left|f(W,X,\Delta)-\mathbb{E}(f(W,X,\Delta)|X)\right| \\
\label{eq:ep-g} && - \sup_{\|\Delta\|=1}\left|g(X,\Delta)-\mathbb{\mathbb{E}}g(X,\Delta)\right|.
\end{eqnarray}
We will analyze the three terms above separately.

\paragraph{Analysis of (\ref{eq:exp-f-inf}).} Define $h(W_j)=\mathbb{E}(\psi(W_j^Tx_i)|W_j)$ and $\bar{\psi}(W_j^Tx_i)=\psi(W_j^Tx_i)-h(W_j)$. We then have
\begin{equation}
\mathbb{E}f(W,X,\Delta)=\mathbb{E}\left|\sum_{i=1}^n\bar{\psi}(W^Tx_i)\Delta_i+h(W)\sum_{i=1}^n\Delta_i\right|. \label{eq:will-be-split}
\end{equation}
A lower bound of (\ref{eq:will-be-split}) is
$$\mathbb{E}f(W,X,\Delta)\geq \left|\sum_{i=1}^n\Delta_i\right|\left|\mathbb{E}h(W)\right|-\mathbb{E}\left|\sum_{i=1}^n\bar{\psi}(W^Tx_i)\Delta_i\right|,$$
where the second term can be bounded by
\begin{eqnarray*}
\mathbb{E}\left|\sum_{i=1}^n\bar{\psi}(W^Tx_i)\Delta_i\right| &\leq& \sqrt{\mathbb{E}\left|\sum_{i=1}^n\bar{\psi}(W^Tx_i)\Delta_i\right|^2} \\
&=& \sqrt{\mathbb{E}\Var\left(\left|\sum_{i=1}^n\psi(W^Tx_i)\Delta_i\right|\Big|W\right)} \\
&=& \sqrt{\mathbb{E}\sum_{i=1}^n\Delta_i^2\Var(\psi(W^Tx_i)|W)} \\
&=& \sqrt{\mathbb{E}\sum_{i=1}^n\Delta_i^2\mathbb{E}(|\psi(W^Tx_i)|^2|W)} \\
&=& \sqrt{\mathbb{E}|\psi(W^Tx)|^2} \leq \sqrt{\mathbb{E}|W^Tx|^2} = 1.
\end{eqnarray*}
Since
$$\mathbb{E}h(W)=\frac{1}{\sqrt{2\pi}}\mathbb{E}\|W\|=\frac{1}{\sqrt{\pi}}\frac{\Gamma((d+1)/2)}{\sqrt{d}\Gamma(d/2)}\geq \frac{1}{\sqrt{2\pi}}\sqrt{\frac{d-1}{d}}.$$
Therefore, as long as $d\geq 3$ and $\left|\sum_{i=1}^n\Delta_i\right|\geq 7$, we have $\mathbb{E}f(W,X,\Delta)\geq 1$, and we thus can conclude that
\begin{equation}
\inf_{\|\Delta\|=1,|\sum_{i=1}^n\Delta_i|\geq 7}\mathbb{E}f(W,X,\Delta) \gtrsim 1.\label{eq:l1-1-1}
\end{equation}

Now we consider the case $\left|\sum_{i=1}^n\Delta_i\right|< 7$. A lower bound for $\left|\sum_{i=1}^n\psi(W^Tx_i)\Delta_i\right|$ is
\begin{equation}
\left|\sum_{i=1}^n\psi(W^Tx_i)\Delta_i\right| \geq \left|\sum_{i=1}^n\bar{\psi}(W^Tx_i)\Delta_i\right| - 7h(W) = \left|\sum_{i=1}^n\bar{\psi}(W^Tx_i)\Delta_i\right| - \frac{7}{\sqrt{2\pi}}\|W\|. \label{eq:seven}
\end{equation}
Thus,
\begin{eqnarray}
\nonumber \mathbb{E}f(W,X,\Delta) &\geq& \mathbb{E}\left(\left|\sum_{i=1}^n\psi(W^Tx_i)\Delta_i\right|\mathbb{I}\left\{\left|\sum_{i=1}^n\bar{\psi}(W^Tx_i)\Delta_i\right|\geq 6, 1/2\leq \|W\|^2\leq 2\right\}\right) \\
\nonumber &\geq& \mathbb{P}\left(\left|\sum_{i=1}^n\bar{\psi}(W^Tx_i)\Delta_i\right|\geq 6, 1/2\leq \|W\|^2\leq 2\right) \\
\nonumber &=& \mathbb{P}\left(\left|\sum_{i=1}^n\bar{\psi}(W^Tx_i)\Delta_i\right|\geq 6\Big|1/2\leq \|W\|^2\leq 2\right)\mathbb{P}\left(1/2\leq \|W\|^2\leq 2\right) \\
\nonumber &\geq& \mathbb{P}\left(\left|\sum_{i=1}^n\bar{\psi}(W^Tx_i)\Delta_i\right|\geq 6\Big|1/2\leq \|W\|^2\leq 2\right)\left(1-2\exp(-d/16)\right),
\end{eqnarray}
where the last inequality is by Lemma \ref{lem:chi-squared}. By direct calculation, we have
\begin{equation}
\Var\left(\bar{\psi}(W^Tx)|W\right)=\|W\|^2\Var(\max(0,W^Tx/\|W\|)|W)=\|W\|^2\frac{1-\pi^{-1}}{2}, \label{eq:cond-var-X-W}
\end{equation}
and
$$\mathbb{E}\left(|\bar{\psi}(W^Tx)|^3|W\right) \leq 3\mathbb{E}\left(|\psi(W^Tx)|^3|W\right)+3|h(W)|^3 \leq \frac{3}{2}\|W\|^3.$$
Therefore, by Lemma \ref{lem:stein}, we have
\begin{eqnarray}
\nonumber && \mathbb{P}\left(\left|\sum_{i=1}^n\bar{\psi}(W^Tx_i)\Delta_i\right|\geq 6\Big|1/2\leq \|W\|^2\leq 2\right) \\
\nonumber &\geq& \mathbb{P}\left(\frac{\left|\sum_{i=1}^n\bar{\psi}(W^Tx_i)\Delta_i\right|}{\|W\|\sqrt{\frac{1-\pi^{-1}}{2}}}\geq 21\Bigg|1/2\leq \|W\|^2\leq 2\right) \\
\nonumber &\geq& \mathbb{P}\left(N(0,1)>21\right) - \sup_{1/2\leq \|W\|^2\leq 2} 2\sqrt{3\sum_{i=1}^n|\Delta_i|^3\frac{\mathbb{E}\left(|\bar{\psi}(W^Tx_i)|^3|W\right)}{\|W\|^3\left(\frac{1-\pi^{-1}}{2}\right)^{3/2}}} \\
\nonumber &\geq& \mathbb{P}\left(N(0,1)>21\right) - 10\sqrt{\sum_{i=1}^n|\Delta_i|^3} \\
\nonumber &\geq& \mathbb{P}\left(N(0,1)>21\right) - 10\max_{1\leq i\leq n}|\Delta_i|^{3/2}.
\end{eqnarray}
Hence, when $\max_{1\leq i\leq n}|\Delta_i|^{3/2}\leq \delta_0^{3/2}:=\mathbb{P}\left(N(0,1)>21\right)/20$ and $\left|\sum_{i=1}^n\Delta_i\right|< 7$, we can lower bound $\mathbb{E}f(W,X,\Delta)$ by an absolute constant, and we conclude that
\begin{equation}
\inf_{\|\Delta\|=1,|\sum_{i=1}^n\Delta_i|\leq 7, \max_{1\leq i\leq n}|\Delta_i|\leq\delta_0}\mathbb{E}f(W,X,\Delta) \gtrsim 1.\label{eq:l1-1-2-relu}
\end{equation}

Finally, we consider the case when $\max_{1\leq i\leq n}|\Delta_i|> \delta_0$ and $\left|\sum_{i=1}^n\Delta_i\right|< 7$. Without loss of generality, we can assume $\Delta_1>\delta_0$. Note that the lower bound (\ref{eq:seven}) still holds, and thus we have
$$\left|\sum_{i=1}^n\psi(W^Tx_i)\Delta_i\right|\geq  \bar{\psi}(W^Tx_1)\Delta_1 - \left|\sum_{i=2}^n\bar{\psi}(W^Tx_i)\Delta_i\right| - \frac{7}{\sqrt{2\pi}}\|W\|.$$
We then lower bound $\mathbb{E}f(W,X,\Delta)$ by
\begin{eqnarray*}
&& \mathbb{E}\left(\left|\sum_{i=1}^n\psi(W^Tx_i)\Delta_i\right|\mathbb{I}\left\{ \bar{\psi}(W^Tx_1)\Delta_1 \geq 8, \left|\sum_{i=2}^n\bar{\psi}(W^Tx_i)\Delta_i\right|\leq 2, 1/2\leq \|W\|^2\leq 2\right\}\right) \\
&\geq& \mathbb{P}\left(\bar{\psi}(W^Tx_1)\Delta_1 \geq 8, \left|\sum_{i=2}^n\bar{\psi}(W^Tx_i)\Delta_i\right|\leq 2\Big|1/2\leq \|W\|^2\leq 2\right)\mathbb{P}\left(1/2\leq \|W\|^2\leq 2\right) \\
&\geq& \mathbb{P}\left(\bar{\psi}(W^Tx_1)\Delta_1 \geq 8\Big|1/2\leq \|W\|^2\leq 2\right) \\
&& \times \mathbb{P}\left(\left|\sum_{i=2}^n\bar{\psi}(W^Tx_i)\Delta_i\right|\leq 2\Big|1/2\leq \|W\|^2\leq 2\right)\left(1-2\exp(-d/16)\right).
\end{eqnarray*}
For any $W$ that satisfies $1/2\leq \|W\|^2\leq 2$, we have
\begin{eqnarray*}
\mathbb{P}\left(\bar{\psi}(W^Tx_1)\Delta_1 \geq 8\Big|W\right) &\geq& \mathbb{P}\left(\bar{\psi}(W^Tx_1)\geq 8/\delta_0\Big|W\right) \\
&\geq&  \mathbb{P}\left(\psi(W^Tx_1)\geq 8/\delta_0+1/\sqrt{\pi}\Big|W\right) \\
&\geq& \mathbb{P}\left(W^Tx_1\geq 8/\delta_0+1/\sqrt{\pi}\Big|W\right) \\
&\geq& \mathbb{P}\left(N(0,1)\geq \sqrt{2}8/\delta_0 + \sqrt{2/\pi}\right),
\end{eqnarray*}
which is a constant. We also have
\begin{eqnarray*}
&& \mathbb{P}\left(\left|\sum_{i=2}^n\bar{\psi}(W^Tx_i)\Delta_i\right|\leq 2\Big|1/2\leq \|W\|^2\leq 2\right) \\
&\geq& 1 - \frac{1}{4}\Var\left(\sum_{i=2}^n\bar{\psi}(W^Tx_i)\Delta_i\Big|W\right) \\
&\geq& \frac{1}{2},
\end{eqnarray*}
where the last inequality is by (\ref{eq:cond-var-X-W}). Therefore, we have
$$\mathbb{E}f(W,X,\Delta)\geq \frac{1}{2}\left(1-2\exp(-d/16)\right)\mathbb{P}\left(N(0,1)\geq \sqrt{2}8/\delta_0 + \sqrt{2/\pi}\right)\gtrsim 1,$$
and we can conclude that
\begin{equation}
\inf_{\|\Delta\|=1,|\sum_{i=1}^n\Delta_i|\leq 7, \max_{1\leq i\leq n}|\Delta_i|\geq\delta_0}\mathbb{E}f(W,X,\Delta) \gtrsim 1.\label{eq:l1-1-3-relu}
\end{equation}

In the end, we combine the three cases (\ref{eq:l1-1-1}), (\ref{eq:l1-1-2-relu}), and (\ref{eq:l1-1-3-relu}),  and we obtain the conclusion that $\inf_{\|\Delta\|=1}\mathbb{E}f(W,X,\Delta)\gtrsim 1$.


\paragraph{Analysis of (\ref{eq:ep-f}).} This step follows the same analysis of (\ref{eq:ep-f-relu}) in the proof of Lemma \ref{lem:design-rf}, and we have
$$\sup_{\|\Delta\|=1}\left|f(W,X,\Delta)-\mathbb{E}(f(W,X,\Delta)|X)\right|\lesssim \sqrt{\frac{n^2}{p}},$$
with high probability.

\paragraph{Analysis of (\ref{eq:ep-g}).} This step follows a similar analysis of (\ref{eq:ep-g-relu}) in the proof of Lemma \ref{lem:design-rf}. The only difference is that the bound $\mathbb{E}g(X,\Delta)\leq 1$ there can be replaced by $\mathbb{E}g(X,\Delta)\leq \sqrt{n}$, because
$$\mathbb{E}g(X,\Delta)\leq \mathbb{E}\sqrt{\sum_{i=1}^n|\psi(W^Tx_i)|^2}\leq \sqrt{\sum_{i=1}^n\mathbb{E}|\psi(W^Tx_i)|^2}\leq \sqrt{n}.$$
Therefore,
$$\sup_{\|\Delta\|=1}|g(X,\Delta) - \mathbb{E}g(X,\Delta)|\lesssim \sqrt{\frac{n\log(1+2/\zeta)}{d}} + \sqrt{n}\zeta,$$
with high probability as long as $\zeta\leq 1/2$. We choose $\zeta=\frac{c}{\sqrt{n}}$ with a sufficiently small constant $c>0$, and thus the bound is sufficiently small as long as $\frac{n\log n}{d}$ is sufficiently small.

Finally, combine results for (\ref{eq:exp-f-inf}), (\ref{eq:ep-f}) and (\ref{eq:ep-g}), and we obtain the desired conclusion as long as $n^2/p$ and $n\log n/d$ are sufficiently small.
\end{proof}

To prove (\ref{eq:l2-upper-A}) of Lemma \ref{lem:design-rf-relu}, we establish the following stronger result.
\begin{lemma}\label{lem:lim-G-relu}
Consider independent $W_1,...,W_p\sim N(0,d^{-1}I_d)$ and $x_1,...,x_n\sim N(0,I_d)$. We define the matrices $G,\bar{G}\in\mathbb{R}^{n\times n}$ by
$$
G_{il}=\frac{1}{p}\sum_{j=1}^p\psi(W^T_jx_i)\psi(W_j^Tx_l),
$$
and
$$\bar{G}_{il}=\begin{cases}
\frac{1}{2}, & i=l, \\
\frac{1}{2\pi}+\frac{1}{4}\frac{\bar{x}_i^T\bar{x}_l}{d} + \frac{1}{2\pi}\left(\frac{\|x_i\|}{\sqrt{d}}-1+\frac{\|x_l\|}{\sqrt{d}}-1\right), & i\neq l.
\end{cases}$$
Assume $d/\log n$ is sufficiently large, and then
$$\opnorm{G-\bar{G}}^2\lesssim \frac{n^2}{p} + \frac{\log n}{d} + \frac{n^2}{d^2},$$
with high probability. Moreover, we also have $\opnorm{G}\lesssim n$ with high probability.
\end{lemma}
\begin{proof}
Define $\wt{G}\in\mathbb{R}^{n\times n}$ with entries $\wt{G}_{il}=\mathbb{E}\left(\psi(W^Tx_i)\psi(W^Tx_l)|X\right)$, and we first bound the difference between $G$ and $\wt{G}$. Note that
$$\mathbb{E}(G_{il}-\wt{G}_{il})^2 = \mathbb{E}\Var(G_{il}|X) \leq \frac{1}{p}\mathbb{E}|\psi(W^Tx_i)\psi(W^Tx_l)|^2=\frac{3}{2p}\mathbb{E}\|W\|^4\leq 5p^{-1}.$$
We then have
$$
\mathbb{E}\opnorm{G-\wt{G}}^2 \leq \mathbb{E}\fnorm{G-\wt{G}}^2 \leq \frac{5n^2}{p}.
$$
By Markov's inequality,
\begin{equation}
\opnorm{G-\wt{G}}^2 \lesssim \frac{n^2}{p}, \label{eq:G-G-tilde-relu}
\end{equation}
with high probability.

Next, we study the diagonal entries of $\wt{G}$. For any $i\in[n]$, $\wt{G}_{ii}=\mathbb{E}(|\psi(W^Tx_i)|^2|X)=\frac{\|x_i\|^2}{2d}$. By Lemma \ref{lem:chi-squared} and a union bound argument, we have
\begin{equation}
\max_{1\leq i\leq n}|\wt{G}_{ii}-\bar{G}_{ii}|\lesssim \sqrt{\frac{\log n}{d}}, \label{eq:G-diag-relu}
\end{equation}
with high probability.

Now we analyze the off-diagonal entries. We use the notation $\bar{x}_i=\frac{\sqrt{d}}{\|x_i\|}x_i$. For any $i\neq l$, we have
\begin{eqnarray}
\label{eq:G-tilde-1-relu} \wt{G}_{il} &=& \mathbb{E}\left(\psi(W^T\bar{x}_i)\psi(W^T\bar{x}_l)|X\right) \\
\label{eq:G-tilde-2-relu} && + \mathbb{E}\left((\psi(W^Tx_i)-\psi(W^T\bar{x}_i))\psi(W^T\bar{x}_l)|X\right) \\
\label{eq:G-tilde-3-relu} && + \mathbb{E}\left(\psi(W^T\bar{x}_i)(\psi(W^Tx_l)-\psi(W^T\bar{x}_l))|X\right) \\
\label{eq:G-tilde-4-relu} && + \mathbb{E}\left((\psi(W^Tx_i)-\psi(W^T\bar{x}_i))(\psi(W^Tx_l)-\psi(W^T\bar{x}_l))|X\right).
\end{eqnarray}
For the first term on the right hand side of (\ref{eq:G-tilde-1-relu}), we observe that $\mathbb{E}\left(\psi(W^T\bar{x}_i)\psi(W^T\bar{x}_l)|X\right)$ is a function of $\frac{\bar{x}_i^T\bar{x}_l}{d}$, and thus we can write
$$\mathbb{E}\left(\psi(W^T\bar{x}_i)\psi(W^T\bar{x}_l)|X\right)=f\left(\frac{\bar{x}_i^T\bar{x}_l}{d}\right),$$
where
$$f(\rho) = \begin{cases}
\mathbb{E}\psi(\sqrt{1-\rho}U+\sqrt{\rho}Z)\psi(\sqrt{1-\rho}V+\sqrt{\rho}Z), & \rho \geq 0, \\
\mathbb{E}\psi(\sqrt{1+\rho}U-\sqrt{-\rho}Z)\psi(\sqrt{1+\rho}V+\sqrt{-\rho}Z), & \rho < 0,
\end{cases}$$
with $U,V,Z\stackrel{iid}{\sim} N(0,1)$. By some direct calculations, we have $f(0)=\frac{1}{2\pi}$, $f'(0)=\frac{1}{4}$, and $\sup_{|\rho|\leq 0.2}\frac{|f'(\rho)-f'(0)|}{|\rho|}\lesssim 1$. Therefore, as long as $|\bar{x}_i^T\bar{x}_l|/d\leq 1/5$,
$$\left|f\left(\frac{\bar{x}_i^T\bar{x}_l}{d}\right)-\frac{1}{2\pi}-\frac{1}{4}\frac{\bar{x}_i^T\bar{x}_l}{d}\right|\leq C_1\left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|^2,$$
for some constant $C_1>0$. By Lemma \ref{lem:inner-prod}, we know that $\max_{i\neq l}|\bar{x}_i^T\bar{x}_l|/d\lesssim \sqrt{\frac{\log n}{d}}\leq 1/5$ with high probability, which then implies
\begin{equation}
\sum_{i\neq l}\left(\mathbb{E}\left(\psi(W^T\bar{x}_i)\psi(W^T\bar{x}_l)|X\right)-\bar{G}_{il}\right)^2 \leq C_1\sum_{i\neq l}\left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|^4.\label{eq:4th-bd-later}
\end{equation}
The term on the right hand side has been analyzed in (\ref{eq:G-H}), and we have $\sum_{i\neq l}\left|\frac{x_i^Tx_l}{d}\right|^4\lesssim \frac{n^2}{d^2}$ with high probability.

We also need to analyze the contributions of (\ref{eq:G-tilde-2-relu}) and (\ref{eq:G-tilde-3-relu}). Observe the fact that $\mathbb{I}\{W^Tx_i\geq 0\}=\mathbb{I}\{W^T\bar{x}_i\geq 0\}$, which implies
\begin{eqnarray}
\nonumber \psi(W^Tx_i)-\psi(W^T\bar{x}_i) &=& W^T(x_i-\bar{x}_i)\mathbb{I}\{W^T\bar{x}_i\geq 0\}\psi(W^T\bar{x}_l) \\
\label{eq:interesting-rep} &=& \left(\frac{\|x_i\|}{\sqrt{d}}-1\right)\psi(W^T\bar{x}_i)\psi(W^T\bar{x}_l).
\end{eqnarray}
Then, the sum of (\ref{eq:G-tilde-2-relu}) and (\ref{eq:G-tilde-3-relu}) can be written as
$$\left(\frac{\|x_i\|}{\sqrt{d}}-1+\frac{\|x_l\|}{\sqrt{d}}-1\right)f\left(\frac{\bar{x}_i^T\bar{x}_l}{d}\right).$$
Note that
\begin{eqnarray*}
&& \sum_{i\neq l} \left(\frac{\|x_i\|}{\sqrt{d}}-1+\frac{\|x_l\|}{\sqrt{d}}-1\right)^2\left[f\left(\frac{\bar{x}_i^T\bar{x}_l}{d}\right)-\frac{1}{2\pi}\right]^2 \\
&\lesssim& \sum_{i\neq l} \left(\frac{\|x_i\|}{\sqrt{d}}-1+\frac{\|x_l\|}{\sqrt{d}}-1\right)^4 + \sum_{i\neq l}\left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|^4.
\end{eqnarray*}
We have already shown that $\sum_{i\neq l}\left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|^4\lesssim \frac{n^2}{d^2}$ with high probability. By integrating out the probability tail bound of Lemma \ref{lem:chi-squared}, we have $\mathbb{E}\left(\frac{\|x_i\|}{\sqrt{d}}-1\right)^4\lesssim d^{-2}$, which then implies
$$\sum_{i\neq l} \mathbb{E}\left(\frac{\|x_i\|}{\sqrt{d}}-1+\frac{\|x_l\|}{\sqrt{d}}-1\right)^4\lesssim \frac{n^2}{d^2}$$
and the corresponding high-probability bound by Markov's inequality.

Finally, we show that the contribution of (\ref{eq:G-tilde-4-relu}) is negligible. By (\ref{eq:interesting-rep}), we can write (\ref{eq:G-tilde-4-relu}) as
$$\left(\frac{\|x_i\|}{\sqrt{d}}-1\right)\left(\frac{\|x_l\|}{\sqrt{d}}-1\right)\mathbb{E}\left(\psi(W^T\bar{x}_i)^2\psi(W^T\bar{x}_l)^2\Big|X\right),$$
whose absolute value can be bounded by $\frac{3}{2}\left|\frac{\|x_i\|}{\sqrt{d}}-1\right|\left|\frac{\|x_l\|}{\sqrt{d}}-1\right|$. Since
$$\sum_{i\neq l}\mathbb{E}\left(\frac{\|x_i\|}{\sqrt{d}}-1\right)^2\mathbb{E}\left(\frac{\|x_l\|}{\sqrt{d}}-1\right)^2\lesssim \frac{n^2}{d^2},$$
we can conclude that (\ref{eq:G-tilde-4-relu}) is bounded by $O\left(\frac{n^2}{d^2}\right)$ with high probability by Markov's inequality.

Combining the analyses of (\ref{eq:G-tilde-1-relu}), (\ref{eq:G-tilde-2-relu}), (\ref{eq:G-tilde-3-relu}) and (\ref{eq:G-tilde-4-relu}), we conclude that $\sum_{i\neq l}(\wt{G}_{il}-\bar{G}_{il})^2\lesssim \frac{n^2}{d^2}$ with high probability. Together with (\ref{eq:G-G-tilde-relu}) and (\ref{eq:G-diag-relu}), we obtain the desired bound for $\opnorm{G-\bar{G}}$.

To prove the last conclusion $\opnorm{\bar{G}}\lesssim n$, it suffices to analyze $\lambda_{\max}(\bar{G})$. We bound this quantity by $\mathbb{E}\lambda_{\max}(\bar{G})^2\leq \mathbb{E}\fnorm{\bar{G}}^2\lesssim n^2$, which leads to the desired conclusion.
\end{proof}


\begin{proof}[Proof of Corollary \ref{cor:repair-rf-relu}]
Since $\wh{\theta}$ belongs to the row space of $\wt{X}$, there exists some $u^*\in\mathbb{R}^n$ such that $\wh{\theta}=\wt{X}^Tu^*$.
By Theorem \ref{thm:main-improved} and Lemma \ref{lem:design-rf-relu}, we know that $\wt{u}=u^*$ with high probability, and therefore $\wt{\theta}=\wt{X}^T\wt{u}=\wt{X}^Tu^*=\wh{\theta}$.
\end{proof}





\subsection{Proof of Theorem \ref{thm:nn-grad-relu}}


To prove Theorem \ref{thm:nn-grad-relu}, we need the following kernel random matrix result.
\begin{lemma}\label{lem:lim-H-relu}
Consider independent $W_1,\ldots,W_p\sim N(0,d^{-1}I_d)$, $x_1,\ldots,x_n\sim N(0,I_d)$, and parameters $\beta_1,\ldots,\beta_p\sim N(0,1)$. We define the matrices $H, \bar{H}\in\mathbb{R}^{n\times n}$ by
\begin{eqnarray*}
H_{il} &=& \frac{x_i^Tx_l}{d}\frac{1}{p}\sum_{j=1}^p\beta_j^2\mathbb{I}\{W_j^Tx_i\geq 0, W_j^Tx_l\geq 0\}, \\
\bar{H}_{il} &=& \frac{1}{4}\frac{x_i^Tx_l}{\|x_i\|\|x_l\|} + \frac{1}{4}\mathbb{I}\{i=l\}.
\end{eqnarray*}
Assume $d/\log n$ is sufficiently large, and then
$$\opnorm{H-\bar{H}}^2 \lesssim \frac{n^2}{pd} + \frac{n}{p} + \frac{\log n}{d} + \frac{n^2}{d^2},$$
with high probability. If we additionally assume that $d/n$ and $p/n$ are sufficiently large, we will also have
\begin{equation}
\frac{1}{5}\leq\lambda_{\min}(H)\leq\lambda_{\max}(H)\lesssim 1,\label{eq:last-added-ref}
\end{equation}
with high probability.
%If we assume that $d/n$ and $p/n$ are sufficiently large, we will also have
%$$\lambda_{\min}(H)\geq \frac{1}{8},$$
%with high probability.
\end{lemma}
\begin{proof}
Define $\wt{H}\in\mathbb{R}^{n\times n}$ with entries $\wt{H}_{il}=\frac{x_i^Tx_l}{d}\mathbb{E}\left(\beta^2\mathbb{I}\{W^Tx_i\geq 0, W^Tx_l\geq 0\}\big|X\right)$, and we first bound the difference between $H$ and $\wt{H}$. Note that
$$\mathbb{E}(H_{il}-\wt{H}_{il})^2=\mathbb{E}\Var(H_{il}|X)\leq \frac{1}{p}\mathbb{E}\left(\frac{|x_i^Tx_l|^2}{d^2}\beta^4\right)\leq\begin{cases}
\frac{3}{pd}, & i\neq l, \\
9p^{-1}, & i=l.
\end{cases}$$
We then have
$$\mathbb{E}\opnorm{H-\wt{H}}^2 \leq \mathbb{E}\fnorm{H-\wt{H}}^2 \leq \frac{3n^2}{pd} + \frac{9n}{p}.$$
By Markov's inequality,
\begin{equation}
\opnorm{H-\wt{H}}^2 \lesssim \frac{n^2}{pd} + \frac{n}{p}, \label{eq:H-H-tilde-relu}
\end{equation}
with high probability.

Next, we study the diagonal entries of $\wt{H}$. For any $i\in[n]$, $\wt{H}_{ii}=\frac{\|x_i\|^2}{d}\mathbb{E}(\beta^2\mathbb{I}\{W^Tx_i\geq 0\}|X)=\frac{\|x_i\|^2}{2d}$. The same analysis that leads to the bound (\ref{eq:G-diag-relu}) also implies that
\begin{equation}
\max_{1\leq i\leq n}|\wt{H}_{ii}-\bar{H}_{ii}|\lesssim \sqrt{\frac{\log n}{d}}, \label{eq:H-diag-relu}
\end{equation}
with high probability.

Now we analyze the off-diagonal entries. Recall the notation $\bar{x}_i=\frac{\sqrt{d}}{\|x_i\|}x_i$. For any $i\neq l$, we have
\begin{eqnarray}
\nonumber \wt{H}_{il} &=& \frac{\|x_i\|\|x_l\|}{d}\frac{\bar{x}_i^T\bar{x}_l}{d}\mathbb{P}\left(W^T\bar{x}_i\geq 0, W^T\bar{x}_l\geq 0|X\right) \\
\label{eq:H-tilde-il-relu} &=& \frac{\bar{x}_i^T\bar{x}_l}{d}\mathbb{P}\left(W^T\bar{x}_i\geq 0, W^T\bar{x}_l\geq 0|X\right) \\
\nonumber && + \left(\frac{\|x_i\|\|x_l\|}{d}-1\right)\frac{\bar{x}_i^T\bar{x}_l}{d}\mathbb{P}\left(W^T\bar{x}_i\geq 0, W^T\bar{x}_l\geq 0|X\right).
\end{eqnarray}
Since $\mathbb{P}\left(W^T\bar{x}_i\geq 0, W^T\bar{x}_l\geq 0|X\right)$ is a function of $\frac{\bar{x}_i^T\bar{x}_l}{d}$, we can write
\begin{equation}
\frac{\bar{x}_i^T\bar{x}_l}{d}\mathbb{P}\left(W^T\bar{x}_i\geq 0, W^T\bar{x}_l\geq 0|X\right)=f\left(\frac{\bar{x}_i^T\bar{x}_l}{d}\right), \label{eq:f-H-tilde-relu}
\end{equation}
where for $\rho>0$,
\begin{eqnarray*}
f(\rho) &=& \rho\mathbb{P}\left(\sqrt{1-\rho}U+\sqrt{\rho}Z\geq 0, \sqrt{1-\rho}V+\sqrt{\rho}Z\geq0\right) \\
&=& \rho\mathbb{E}\mathbb{P}\left(\sqrt{1-\rho}U+\sqrt{\rho}Z\geq 0, \sqrt{1-\rho}V+\sqrt{\rho}Z\geq 0|Z\right) \\
&=& \rho\mathbb{E}\Phi\left(\sqrt{\frac{\rho}{1-\rho}}Z\right)^2,
\end{eqnarray*}
with $U,V,Z\stackrel{iid}{\sim} N(0,1)$ and $\Phi(\cdot)$ being the cumulative distribution function of $N(0,1)$. Similarly, for $\rho<0$,
$$f(\rho) = \rho\mathbb{E}\left[\Phi\left(\sqrt{\frac{-\rho}{1+\rho}}Z\right)\left(1-\Phi\left(\sqrt{\frac{-\rho}{1+\rho}}Z\right)\right)\right].$$
By some direct calculations, we have $f(0)=0$, $f'(0)=\frac{1}{4}$, and
$$\sup_{|\rho|\leq 1/5}|f''(\rho)|\lesssim \sup_{|t|\leq 1/2}\left|\mathbb{E}\phi(tZ)\Phi(tZ)Z/t\right| + \sup_{|t|\leq 1/2}\left|\mathbb{E}\phi(tZ)Z/t\right|,$$
where $\phi(x)=(2\pi)^{-1/2}e^{-x^2/2}$. For any $|t|\leq 1/2$,
$$
\left|\mathbb{E}\phi(tZ)Z/t\right| = \left|\mathbb{E}\frac{\phi(tZ)-\phi(0)}{tZ}Z^2\right| = \left|\mathbb{E}\xi\phi(\xi)Z^2\right| \leq \frac{|t|}{\sqrt{2\pi}}\mathbb{E}|Z|^3\lesssim 1,
$$
where $\xi$ is a scalar between $0$ and $tZ$ so that $|\xi|\leq |tZ|$. By a similar argument, we also have $\sup_{|t|\leq 1/2}\left|\mathbb{E}\phi(tZ)\Phi(tZ)Z/t\right|\lesssim 1$ so that $\sup_{|\rho|\leq 1/5}|f''(\rho)|\lesssim 1$. Therefore, as long as $|\bar{x}_i^T\bar{x}_l|/d\leq 1/5$,
$$\left|f\left(\frac{\bar{x}_i^T\bar{x}_l}{d}\right)-\frac{1}{4}\frac{\bar{x}_i^T\bar{x}_l}{d}\right|\leq C_1\left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|^2,$$
for some constant $C_1>0$. By Lemma \ref{lem:inner-prod}, we know that $\max_{i\neq l}|\bar{x}_i^T\bar{x}_l|/d\lesssim \sqrt{\frac{\log n}{d}}\leq 1/5$ with high probability. In view of the identities (\ref{eq:H-tilde-il-relu}) and (\ref{eq:f-H-tilde-relu}), we then have the high probability bound,
\begin{eqnarray}
\nonumber \sum_{i\neq l}\left(\wt{H}_{il}-\frac{1}{4}\frac{\bar{x}_i^T\bar{x}_l}{d}\right)^2 &\leq& 2\sum_{i\neq l}\left(\frac{\|x_i\|\|x_l\|}{d}-1\right)^2\left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|^2  + 2C_1\sum_{i\neq l}\left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|^4 \\
 \label{eq:high-prob-off-diag} &\leq& \sum_{i\neq l}\left(\frac{\|x_i\|\|x_l\|}{d}-1\right)^4 + (2C_1+1)\sum_{i\neq l}\left|\frac{\bar{x}_i^T\bar{x}_l}{d}\right|^4.
\end{eqnarray}
For the first term on the right hand side of (\ref{eq:high-prob-off-diag}), we use Lemma \ref{lem:inner-prod} and obtain a probability tail bound for $|\|x_i\|\|x_l\|-d|$. By integrating out this tail bound, we have
$$\sum_{i\neq l}\mathbb{E}\left(\frac{\|x_i\|\|x_l\|}{d}-1\right)^4\lesssim \frac{n^2}{d^2},$$
which, by Markov's inequality, implies $\sum_{i\neq l}\left(\frac{\|x_i\|\|x_l\|}{d}-1\right)^4\lesssim \frac{n^2}{d^2}$ with high probability.
Using the same argument in the proof of Lemma \ref{lem:lim-G-relu}, we have $\sum_{i\neq l}\left|\frac{x_i^Tx_l}{d}\right|^4\lesssim \frac{n^2}{d^2}$ with high probability.
Finally, combining (\ref{eq:H-H-tilde-relu}), (\ref{eq:H-diag-relu}), and the bound for (\ref{eq:high-prob-off-diag}), we obtain the desired bound for $\opnorm{H-\bar{H}}$.
%The lower bound for $\lambda_{\min}(H)$ is a direct application of Weyl's inequality.
The last conclusion (\ref{eq:last-added-ref}) follows a similar argument in the proof of Lemma \ref{lem:lim-G}. The proof is complete.
\end{proof}






Now we are ready to prove Theorem \ref{thm:nn-grad-relu}.
\begin{proof}[Proof of Theorem \ref{thm:nn-grad-relu}]
The proof is similar to that of Theorem \ref{thm:nn-grad}, and we will omit repeated arguments. We will use the high-probability inequalities (\ref{eq:r1e1})-(\ref{eq:r1e9}). Then, it suffices to establish Claims A, B, C and D in the proof of Theorem \ref{thm:nn-grad}. Since Claims A and C follow the same argument, we only need to check Claims B and D. Given the similarity of Claims B and D, we only present the proof of Claim D. 
We have
\begin{equation}
u(k+1)-u(k)=\gamma(H(k)+G(k))(y-u(k))+r(k), \label{eq:iter-u-relu}
\end{equation}
where
\begin{eqnarray*}
G_{il}(k) &=& \frac{1}{p}\sum_{j=1}^p\psi(W_j(k)^Tx_l)\psi(W_j(k)^Tx_i), \\
H_{il}(k) &=& \frac{x_i^Tx_l}{d}\frac{1}{p}\sum_{j=1}^p\beta_j(k+1)^2\psi'(W_j(k)^Tx_i)\psi'(W_j(k)^Tx_l),
\end{eqnarray*}
and
\begin{eqnarray*}
r_i(k) &=& \frac{1}{\sqrt{p}}\sum_{j=1}^p\beta_j(k+1)\left(\psi(W_j(k+1)^Tx_i)-\psi(W_j(k)^Tx_i)\right) \\
&& - \frac{1}{\sqrt{p}}\sum_{j=1}^p\beta_j(k+1)(W_j(k+1)-W_j(k))^Tx_i\psi'(W_j(k)^Tx_i).
\end{eqnarray*}


With the same argument, the bound (\ref{eq:x-japan}) still holds. 
By Lemma \ref{lem:lim-G-relu} and the fact that $G(k)$ is positive semi-definite, we have
\begin{equation}
0 \leq \lambda_{\min}(G(k)) \leq \lambda_{\max}(G(k)) \lesssim n. \label{eq:Gk-spec}
\end{equation}

We also need to control the difference between $H(k)$ and $H(0)$. By the definition, we have
\begin{eqnarray}
\label{eq:r-H-d-1-relu} |H_{il}(k)-H_{il}(0)| &\leq& \left|\frac{x_i^Tx_l}{d}\right|\frac{1}{p}\sum_{j=1}^p|\beta_j(k+1)^2-\beta_j^2(0)| \\
\label{eq:r-H-d-2-relu} && + \left|\frac{x_i^Tx_l}{d}\right|\frac{1}{p}\sum_{j=1}^p\beta_j^2(0)|\psi'(W_j(k)^Tx_i) - \psi'(W_j(0)^Tx_i)| \\
\label{eq:r-H-d-3-relu} && + \left|\frac{x_i^Tx_l}{d}\right|\frac{1}{p}\sum_{j=1}^p\beta_j^2(0)|\psi'(W_j(k)^Tx_l) - \psi'(W_j(0)^Tx_l)|.
\end{eqnarray}
We can bound (\ref{eq:r-H-d-1-relu}) by $\left|\frac{x_i^Tx_l}{d}\right|\frac{1}{p}\sum_{j=1}^pR_2(R_2+2|\beta_j(0)|)$. To bound (\ref{eq:r-H-d-2-relu}), we note that
\begin{eqnarray}
\nonumber |\psi'(W_j(k)^Tx_i) - \psi'(W_j(0)^Tx_i)| &\leq& \mathbb{I}\{|W_j(0)^Tx_i|\leq |(W_j(k)-W_j(0))^Tx_i|\} \\
\label{eq:simon-bound} &\leq& \mathbb{I}\{|W_j(0)^Tx_i|\leq R_1\|x_i\|\},
\end{eqnarray}
which implies
\begin{eqnarray*}
&& \left|\frac{x_i^Tx_l}{d}\right|\frac{1}{p}\sum_{j=1}^p\beta_j^2(0)|\psi'(W_j(k)^Tx_i) - \psi'(W_j(0)^Tx_i)| \\
&\leq& \left|\frac{x_i^Tx_l}{d}\right|\frac{1}{p}\sum_{j=1}^p\beta_j^2(0)\mathbb{I}\{|W_j(0)^Tx_i|\leq R_1\|x_i\|\},
\end{eqnarray*}
and a similar bound holds for (\ref{eq:r-H-d-3-relu}). Then,
\begin{eqnarray*}
\opnorm{H(k)-H(0)} &\leq& \max_{1\leq i\leq n}|H_{ii}(k)-H_{ii}(0)| + \max_{1\leq l\leq n}\sum_{i\in[n]\backslash\{l\}}|H_{il}(k)-H_{il}(0)| \\
&\lesssim& \max_{1\leq i\leq n} \frac{1}{p}\sum_{j=1}^p\beta_j^2(0)\mathbb{I}\{|W_j^T(0)^Tx_i|\leq R_1\|x_i\|\} \\
&& + d^{-1/2}n\max_{1\leq i\leq n}\frac{1}{p}\sum_{j=1}^p\beta_j^2(0)\mathbb{I}\{|W_j^T(0)^Tx_i|\leq R_1\|x_i\|\} \\
&& + \max_{1\leq l\leq n}\sum_{i=1}^n\left|\frac{x_i^Tx_l}{d}\right|R_2\frac{1}{p}\sum_{j=1}^p(R_2+2|\beta_j(0)|) \\
&\lesssim& \left(1+\frac{n}{\sqrt{d}}\right)\left(\sqrt{d}R_1\log p+\frac{\sqrt{\log n}\log p}{\sqrt{p}} + R_2^2 + R_2\sqrt{\log p}\right) \\
&\lesssim& \left(1+\frac{n}{\sqrt{d}}\right)\frac{n(\log p)^2}{\sqrt{p}},
\end{eqnarray*}
where we have used (\ref{eq:r1e1}), (\ref{eq:r1e4}), (\ref{eq:r1e5}), (\ref{eq:r1e6}) and (\ref{eq:r1e9}). In view of Lemma \ref{lem:lim-H-relu}, we then have
\begin{equation}
\frac{1}{6} \leq \lambda_{\min}(H(k)) \leq \lambda_{\max}(H(k)) \lesssim 1, \label{eq:Hk-spec}
\end{equation}
under the conditions of $d,p$ and $n$.

Next, we give a bound for $r_i(k)$. Observe that
$$\psi(W_j(k+1)^Tx_i)-\psi(W_j(k)^Tx_i)=(W_j(k+1)-W_j(k))^Tx_i\psi'(W_j(k)^Tx_i),$$
when $\mathbb{I}\{W_j(k+1)^Tx_i>0\}=\mathbb{I}\{W_j(k)^Tx_i>0\}$. Thus, we only need to sum over those $j\in[p]$ that $\mathbb{I}\{W_j(k+1)^Tx_i>0\}\neq \mathbb{I}\{W_j(k)^Tx_i>0\}$. By (\ref{eq:simon-bound}), we have
\begin{eqnarray*}
&& \left|\mathbb{I}\{W_j(k+1)^Tx_i>0\}-\mathbb{I}\{W_j(k)^Tx_i>0\}\right| \\
&\leq& \left|\mathbb{I}\{W_j(k+1)^Tx_i>0\}-\mathbb{I}\{W_j(0)^Tx_i>0\}\right|+ \left|\mathbb{I}\{W_j(k)^Tx_i>0\}-\mathbb{I}\{W_j(0)^Tx_i>0\}\right| \\
&\leq& 2\mathbb{I}\{|W_j^T(0)^Tx_i|\leq R_1\|x_i\|\}.
\end{eqnarray*}
Therefore,
\begin{eqnarray*}
&& \left|\psi(W_j(k+1)^Tx_i)-\psi(W_j(k)^Tx_i)-(W_j(k+1)-W_j(k))^Tx_i\psi'(W_j(k)^Tx_i)\right| \\
&\leq& 4|(W_j(k+1)-W_j(k))^Tx_i|\mathbb{I}\{|W_j^T(0)^Tx_i|\leq R_1\|x_i\|\} \\
&\leq& \frac{4\gamma}{d\sqrt{p}}|\beta_j(k+1)|\|y-u(k)\|\|x_i\|\sqrt{\sum_{l=1}^n\|x_l\|^2}\mathbb{I}\{|W_j^T(0)^Tx_i|\leq R_1\|x_i\|\},
\end{eqnarray*}
which implies
\begin{eqnarray*}
|r_i(k)| &\leq& \frac{4\gamma}{dp}\sum_{j=1}^p|\beta_j(k+1)|^2\|y-u(k)\|\|x_i\|\sqrt{\sum_{l=1}^n\|x_l\|^2}\mathbb{I}\{|W_j^T(0)^Tx_i|\leq R_1\|x_i\|\} \\
&\lesssim& \sqrt{n}\|y-u(k)\|\gamma\frac{1}{p}\sum_{j=1}^p(\beta_j(0)^2+R_2^2)\mathbb{I}\{|W_j^T(0)^Tx_i|\leq R_1\|x_i\|\} \\
&\lesssim& \gamma\sqrt{n}\log p\left(R_1+\sqrt{\frac{\log n}{p}}\right)\|y-u(k)\|.
\end{eqnarray*}
This leads to the bound
\begin{equation}
\|r(k)\|=\sqrt{\sum_{i=1}^n|r_i(k)|^2}\lesssim \gamma n\log p\left(R_1+\sqrt{\frac{\log n}{p}}\right)\|y-u(k)\|.\label{eq:bound-res-k-relu}
\end{equation}

Now we are ready to analyze $\|y-u(k+1)\|^2$. Given the relation (\ref{eq:iter-u-relu}), we have
\begin{eqnarray*}
\|y-u(k+1)\|^2 &=& \|y-u(k)\|^2 - 2\iprod{y-u(k)}{u(k+1)-u(k)} + \|u(k)-u(k+1)\|^2 \\
&=& \|y-u(k)\|^2 - 2\gamma(y-u(k))^T(H(k)+G(k))(y-u(k)) \\
&& - 2\iprod{y-u(k)}{r(k)} + \|u(k)-u(k+1)\|^2.
\end{eqnarray*}
By (\ref{eq:Gk-spec}) and (\ref{eq:Hk-spec}), we have
\begin{equation}
- 2\gamma(y-u(k))^T(H(k)+G(k))(y-u(k)) \leq -\frac{\gamma}{6}\|y-u(k)\|^2. \label{eq:main-inner-relu}
\end{equation}
The bound (\ref{eq:bound-res-k-relu}) implies
$$- 2\iprod{y-u(k)}{r(k)}\leq 2\|y-u(k)\|\|r(k)\|\lesssim \gamma n\log p\left(R_1+\sqrt{\frac{\log n}{p}}\right)\|y-u(k)\|^2.$$
By (\ref{eq:Gk-spec}), (\ref{eq:Hk-spec}) and (\ref{eq:bound-res-k-relu}), we also have
\begin{eqnarray*}
\|u(k)-u(k+1)\|^2 &\leq& 2\gamma^2\|(H(k)+G(k))(y-u(k))\|^2 + 2\|r(k)\|^2 \\
&\lesssim& \gamma^2n\|y-u(k)\|^2 + (\gamma n\log p)^2\left(R_1+\sqrt{\frac{\log n}{p}}\right)^2\|y-u(k)\|^2 .
\end{eqnarray*}
Therefore, as long as $\frac{n\log n}{d}$, $\frac{n^3(\log p)^4}{p}$ and $\gamma n$ are all sufficiently small, we have
$$- 2\iprod{y-u(k)}{r(k)} + \|u(k)-u(k+1)\|^2 \leq \frac{\gamma}{24}\|y-u(k)\|^2.$$
Together with the bound (\ref{eq:main-inner-relu}), we have
$$\|y-u(k+1)\|^2 \leq \left(1-\frac{\gamma}{8}\right)\|y-u(k)\|^2\leq \left(1-\frac{\gamma}{8}\right)^{k+1}\|y-u(0)\|^2,$$
and thus Claim D is true. The proof is complete.
\end{proof}


\subsection{Proofs of Theorem \ref{thm:repair-nn-1-relu} and Theorem \ref{thm:repair-nn-2-relu}}

\begin{proof}[Proof of Theorem \ref{thm:repair-nn-1-relu}]
The proof is the same as that of Theorem \ref{thm:repair-nn-1}. The only exception here is that we apply Lemma \ref{lem:design-rf-relu} and Lemma \ref{lem:lim-G-relu} instead of Lemma \ref{lem:design-rf} and Lemma \ref{lem:lim-G}.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:repair-nn-2-relu}]
The analysis of $\wh{v}_1,...,\wh{v}_p$ is the same as that in the proof of Theorem \ref{thm:repair-nn-1}, and we have $\wt{W}_j=\wh{W}_j$ for all $j\in[p]$ with high probability.

To analyze $\wh{u}$, we apply Theorem \ref{thm:main-improved}. It suffices to check Condition A and Condition B for the design matrix $\psi(X^T\wt{W}^T)=\psi(X^T\wh{W}^T)$. Since
$$\sum_{i=1}^n\mathbb{E}\left(\frac{1}{p}\sum_{j=1}^pc_j\psi(\wh{W}_j^Tx_i)\right)^2\leq \sum_{i=1}^n\frac{1}{p}\sum_{j=1}^p\mathbb{E}\psi(\wh{W}^Tx_i)^2,$$
and $\mathbb{E}\psi(\wh{W}^Tx_i)^2\leq \mathbb{E}|\wh{W}_j^Tx_i|^2\lesssim 1 + R_1d\lesssim 1$, Condition A holds with $\sigma^2\asymp p$.
We also need to check Condition B. By Theorem \ref{thm:nn-grad-relu}, we have
\begin{eqnarray*}
&& \left|\frac{1}{p}\sum_{j=1}^p\left|\sum_{i=1}^n\psi(\wh{W}_j^Tx_i)\Delta_i\right| - \frac{1}{p}\sum_{j=1}^p\left|\sum_{i=1}^n\psi(W_j(0)^Tx_i)\Delta_i\right|\right| \\
&\leq& \frac{1}{p}\sum_{j=1}^p\sum_{i=1}^n|\wh{W}_j^Tx_i-W_j(0)^Tx_i||\Delta_i| \\
&\leq& R_1\sum_{i=1}^n\|x_i\||\Delta_i| \\
&\leq& R_1\sqrt{\sum_{i=1}^n\|x_i\|^2} \\
&\lesssim& \frac{n^{3/2}\log p}{\sqrt{p}},
\end{eqnarray*}
where $\sum_{i=1}^n\|x_i\|^2\lesssim nd$ is by Lemma \ref{lem:chi-squared}. By Lemma \ref{lem:design-rf-relu}, we can deduce that
$$\inf_{\|\Delta\|=1}\frac{1}{p}\sum_{j=1}^p\left|\sum_{i=1}^n\psi(\wh{W}_j^Tx_i)\Delta_i\right|\gtrsim 1,$$
as long as $\frac{n^{3/2}\log p}{\sqrt{p}}$ is sufficiently small. By (\ref{eq:Gk-spec}), we also have
$$\sup_{\|\Delta\|=1}\frac{1}{p}\sum_{j=1}^p\left|\sum_{i=1}^n\psi(\wh{W}_j^Tx_i)\Delta_i\right|^2\lesssim n.$$
Therefore, Condition B holds with $\overline{\lambda}^2\asymp n$ and $\underline{\lambda}\asymp 1$. Apply Theorem \ref{thm:main-improved}, and we have $\wt{\beta}=\wh{\beta}$ with high probability as desired.
\end{proof}
