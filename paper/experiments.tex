% !TEX root = ./repair.tex

\section{Simulation studies}
\label{sec:experiments}
%\let\beta\theta

In this section we discuss experimental results for over-paramaterized linear models, random feature models, and neural networks, illustrating and confirming the theoretical results presented in the previous sections.
In all of our experiments, the \texttt{quantreg} package in $R$ is used to carry out the median regression (quantile regression for quantile level $\tau = \frac{1}{2}$) of \eqref{eq:keylp} using the Frisch-Newton interior point algorithm to solve the linear program (method \texttt{fn} in this package).


\subsection{Over-parameterized linear models}


We begin by giving further details of the simulation briefly discussed
in Section~\ref{sec:overview}. In this experiment we simulate underdetermined linear models where $p > n$.
We generate $n$ data points $(X_i, y_i)$ where
$y_i = X_i^T \theta^* + w_i$ with $w_i$ an additive noise term. We then compute the minimum norm estimator
\begin{equation}
  \hat\theta = X^T (X X^T)^{-1} y
\end{equation}
The estimated model is corrupted to
\begin{equation}
  \eta = \hat\theta + z
\end{equation}
where $z_j \sim (1-\epsilon) \delta_0 +\epsilon Q$. The corrupted estimator is then repaired by performing median regression:
\begin{align}
  \tilde u &= \argmin \|\eta - X^T u\|_1 \\
  \tilde \theta &= X^T \tilde u.
\end{align}

The design is sampled as $X_{ij} \sim N(0,1)$ and we take $\theta_j^* \sim N(0,1)$ and $Q = N(1,1)$. In the plots shown
in Figure~\ref{fig:exp1} the sample size is fixed at $n=100$ and the dimension $p$ is varied according to $p/n=200/j^2$
for a range of values of $j$. The plots show the empirical probability of exact repair $\tilde\theta = \hat\theta$ as a function of $\epsilon$. Each point on the curves is the average repair success over $500$ random trials.
The roughly equal spacing of the curves agrees with the theory, which indicates that $\sqrt{n/p}/(1-\epsilon)$ should be sufficiently small for successful repair.

\begin{figure}[ht]
  \begin{center}
    \begin{tabular}{cc}
      \includegraphics[width=.47\textwidth]{fig/plot-linear-100} &
      \includegraphics[width=.47\textwidth]{fig/plot-linear-100-adj}\\[-10pt]
    \end{tabular}
  \end{center}
\caption{Model repair for underdetermined linear models $y=X^T\theta + w$ with $p>n$. The left plot shows the empirical probability of successful model repair for $n=100$ with the model dimension $p$
varying as $p/n = 200 /j^2$, for $j=1,\ldots, 7$. Each point is an average over 500 random trials. The covariates are sampled as $N(0,1)$ and the corruption distribution is $Q=N(1,1)$. The right plot shows the repair probablity as a function
of the adjusted corruption probability $\tilde\epsilon_j = \epsilon + c'\cdot j - \frac{1}{2}$ for $c'=0.085$.}
\label{fig:exp1}
\end{figure}

\subsection{Varying the mean}

In this experiment we simulate over-parameterized linear models
with nonzero mean. The data are generated as $X_{ij} \sim N(\mu,1)$
independently, where we vary the mean $\mu$ and fix the dimension $p=500$ and sample size $n=50$.
The probability of successful repair is shown in Figure~\ref{fig:mean}.

\begin{figure}[ht]
  \begin{center}
    \begin{tabular}{c}
      $n=50$ and $p=500$ fixed, varying mean $\mu$\\
      \includegraphics[width=.55\textwidth]{fig/plot-linear-mean-shift-50}
    \end{tabular}
  \end{center}
\caption{Model repair for linear models with design entries $X_{i,j}\sim N(\mu, 1)$,
where the mean $\mu$ is varied and the sample size and dimension are fixed at $n=50$ and $p=500$. Consistent with Theorem~\ref{thm:main-improved}, a lower level of noise is tolerated as the mean $\mu$ increases. In the plot above, the means are chosen as $\mu_j = c_j/\sqrt{n}$ for $c_j$ varying between zero and two.}
\label{fig:mean}
\end{figure}

As expected, the fraction $\epsilon$ that allows successful repair decreases; it appears to saturate at some fixed value $\epsilon_{\min}$. The mean $\mu$ cannot be made too large because it causes the design $X$ to become ill-conditioned, and the median regression fails.

The inequality in Condition $A$ in this case takes the form
\begin{align*}
  \left\|\frac{1}{m}\sum_{i=1}^m c_i a_i\right\|^2 &\leq \frac{(1+\mu^2)k}{m} + \mu^2 k
   \equiv \frac{(1+\mu^2)n}{p} + \mu^2 n.
   %  = \frac{(1+\mu^2)n}{p} + c_j.
\end{align*}
The mean $\mu$ in the above plot is taken to be $\mu_j = c_j / \sqrt{n}$ as $c_j$ varies between zero and two.

\subsection{Random features models trained with gradient descent}

In this experiment we simulate over-parameterized random features models.
We generate $n$ data points $(X_i, y_i)$ where
$y_i = X_i^T \theta^* + w_i$ with $w_i$ an additive noise term. The covariates are generated
as a layer of a random neural network, with $X_i = \tanh(WZ_i)$ where $Z_i \in\reals^d$ with $Z_{ij} \sim N(0,1)$
and $W\in\reals^{p\times d}$ with $W_{ij} \sim N(0, 1/d)$. We then approximate the least squares
solution using gradient descent intialized at zero, with updates
\begin{equation}
  \hat\theta^{(t)} = \hat\theta^{(t-1)} + \frac{\eta}{n} X^T R^{(t-1)}
\end{equation}
where the residual vector $R^{(t-1)}\in\reals^n$ is given by $R_i = (y_i - X_i^ T\hat\theta^{(t-1)})$.
The step size $\eta$ is selected empirically to insure convergence in under $1{,}000$ iterations.
Figure~\ref{fig:rf} shows two sets of results, for $n=50$ and $n=100$. For each value of the final dimension $p$,
three values of the original data dimension $d$ are selected: $d=p$, $d=\lceil 2p/3\rceil$,
and $d=\lceil p/2\rceil$. The recovery success curves for gradient descent are similar to those obtained for the minimal norm solution. The results are also similar if the ReLU activation function is used, as long as the population mean of the features is subtracted.

\begin{figure}[ht]
  \begin{center}
    \begin{tabular}{cc}
      {\scriptsize $n=50$} & {\scriptsize $n=100$} \\
      \includegraphics[width=.47\textwidth]{fig/plot-rf-50} &
      \includegraphics[width=.47\textwidth]{fig/plot-rf-100}\\[-10pt]
    \end{tabular}
  \end{center}
\caption{Model repair for random feature models $y=\psi(XW)\theta + w$ with $p>n$, where $\psi(\cdot) = \tanh(\cdot)$
for $n=50$ (left) and $n=100$ (right). For each value of $p$, three values of $d$ are evaluated, $d=p$, $d=\lceil 2p/3\rceil$,
and $d=\lceil p/2\rceil$; the results are effectively the same for each $d$.}
\label{fig:rf}
\end{figure}
